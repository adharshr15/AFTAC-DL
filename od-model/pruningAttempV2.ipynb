{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Advanced Model Compression\n",
    "\n",
    "**Objective:** Reduce model size from ~5MB to < 500kB while maintaining accuracy.\n",
    "\n",
    "**Methodology:**\n",
    "1. **Structured Pruning:** We will physically remove 50% of the convolutional filters (channels). This changes the architecture dimensions (e.g., 32 filters -> 16 filters).\n",
    "2. **Weight Transplantation:** Instead of training the small model from scratch, we copy the \"most important\" weights (based on L1-Norm) from the large trained model to the small model.\n",
    "3. **Fine-Tuning:** We train the small model briefly to heal the accuracy drop.\n",
    "4. **Dynamic Quantization:** We convert the float32 weights to int8 for a final 4x size reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CHECKPOINT_PATH = \"checkpoints/final_best.pt\"\n",
    "IMG_SIZE = 128\n",
    "PRUNE_AMOUNT = 0.6  # Target: Remove 50% of channels\n",
    "FINE_TUNE_EPOCHS = 5\n",
    "FINE_TUNE_LR = 1e-4\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Flexible Model Architecture ---\n",
    "# We modify the CustomCNN to accept a specific list of channel counts.\n",
    "# This allows us to build the \"Child\" model with exact pruned dimensions.\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, reg_strength=0.0, dropout_conv=0.1, dropout_dense=0.1, \n",
    "                 dense_units=512, filters_multiplier=1.0, \n",
    "                 channel_list=None): \n",
    "        super().__init__()\n",
    "        \n",
    "        # If channel_list is explicitly provided (for pruning), use it.\n",
    "        # Otherwise, calculate standard sizes based on multiplier.\n",
    "        if channel_list is not None:\n",
    "            f1, f2, f3 = channel_list\n",
    "        else:\n",
    "            f1 = max(8, int(32 * filters_multiplier))\n",
    "            f2 = max(16, int(64 * filters_multiplier))\n",
    "            f3 = max(32, int(128 * filters_multiplier))\n",
    "\n",
    "        self.channels = [f1, f2, f3]\n",
    "        \n",
    "        # IMPORTANT: We define the layers precisely so we can index them later.\n",
    "        # Structure: [0]Conv -> [1]ReLU -> [2]BN -> [3]Conv -> [4]ReLU -> [5]Pool -> [6]Drop\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, f1, 3, padding=1),      \n",
    "            nn.ReLU(inplace=True),               \n",
    "            nn.BatchNorm2d(f1),                  \n",
    "            nn.Conv2d(f1, f1, 3, padding=1),     \n",
    "            nn.ReLU(inplace=True),               \n",
    "            nn.MaxPool2d(2),                     \n",
    "            nn.Dropout(dropout_conv)             \n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(f1, f2, 3, padding=1),     \n",
    "            nn.ReLU(inplace=True),               \n",
    "            nn.BatchNorm2d(f2),                  \n",
    "            nn.Conv2d(f2, f2, 3, padding=1),     \n",
    "            nn.ReLU(inplace=True),               \n",
    "            nn.MaxPool2d(2),                     \n",
    "            nn.Dropout(dropout_conv)             \n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(f2, f3, 3, padding=1),     \n",
    "            nn.ReLU(inplace=True),               \n",
    "            nn.BatchNorm2d(f3),                  \n",
    "            nn.Conv2d(f3, f3, 3, padding=1),     \n",
    "            nn.ReLU(inplace=True),               \n",
    "            nn.MaxPool2d(2),                     \n",
    "            nn.Dropout(dropout_conv)             \n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Head Structure:\n",
    "        # [0]Flat -> [1]Linear(In pruned) -> [2]ReLU -> [3]BN -> [4]Drop -> [5]Linear\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(f3, dense_units),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(dense_units),\n",
    "            nn.Dropout(dropout_dense),\n",
    "            nn.Linear(dense_units, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.gap(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Helpers --- \n",
    "\n",
    "def get_dataloaders():\n",
    "    # We need training data to fine-tune the pruned model\n",
    "    # We use standard transforms. Ensure paths match your directory structure.\n",
    "    val_tfms = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    \n",
    "    train_ds = datasets.ImageFolder(\"cats-v-non-cats/training/\", transform=val_tfms)\n",
    "    val_ds = datasets.ImageFolder(\"cats-v-non-cats/validation/\", transform=val_tfms)\n",
    "    \n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=64, shuffle=True),\n",
    "        DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "    )\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE).float()\n",
    "            logits = model(x).squeeze(1)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "            correct += (preds == y.long()).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "def get_file_size(model, path=\"temp_model.pt\"):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    size_kb = os.path.getsize(path) / 1024\n",
    "    os.remove(path)\n",
    "    return size_kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Baseline...\n",
      "Baseline Size: 5004 KB | Baseline Acc: 0.9691\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Load Baseline Model ---\n",
    "print(\"Loading Baseline...\")\n",
    "ckpt = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "cfg = ckpt['config']\n",
    "\n",
    "# Initialize original large model\n",
    "baseline_model = CustomCNN(\n",
    "    cfg.get('reg_strength', 0), cfg.get('dropout_conv', 0), cfg.get('dropout_dense', 0),\n",
    "    int(cfg['dense_units']), float(cfg['filters_multiplier'])\n",
    ").to(DEVICE)\n",
    "baseline_model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "# Check baseline stats\n",
    "train_dl, val_dl = get_dataloaders()\n",
    "base_acc = evaluate(baseline_model, val_dl)\n",
    "base_size = get_file_size(baseline_model)\n",
    "print(f\"Baseline Size: {base_size:.0f} KB | Baseline Acc: {base_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Pruning Logic (The Engine) ---\n",
    "\n",
    "def get_filter_norms(weight):\n",
    "    # Calculate L1 norm for Conv2d weights.\n",
    "    # Weight shape: [Out_Channels, In_Channels, Kernel, Kernel]\n",
    "    # We sum over dimensions (1, 2, 3) to get a single value per Output Channel.\n",
    "    return torch.sum(torch.abs(weight), dim=(1, 2, 3))\n",
    "\n",
    "def prune_block(old_block, new_block, prev_idxs, keep_count):\n",
    "    \"\"\"\n",
    "    Transfers weights from old_block to new_block, removing filters with low L1 norms.\n",
    "    Returns the indices of the filters kept, to be used as input indices for the next layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Select Filters to Keep\n",
    "    # We look at the 2nd Conv in the block (index 3) to decide the output shape of the block.\n",
    "    w2 = old_block[3].weight.data\n",
    "    norm = get_filter_norms(w2)\n",
    "    _, current_idxs = torch.topk(norm, keep_count)\n",
    "    current_idxs, _ = torch.sort(current_idxs) # Sort to preserve order\n",
    "\n",
    "    # 2. Transfer Conv 1 (index 0)\n",
    "    # Input Channels: Filtered by `prev_idxs` (from previous block)\n",
    "    # Output Channels: Filtered by `current_idxs` (internal block consistency)\n",
    "    w1_old = old_block[0].weight.data\n",
    "    \n",
    "    # Slicing: [Output_Idxs, Input_Idxs, :, :]\n",
    "    if prev_idxs is None: # First block input is RGB (3 channels), keep all inputs\n",
    "        w1_new = w1_old[current_idxs, :, :, :]\n",
    "    else:\n",
    "        w1_new = w1_old[current_idxs][:, prev_idxs, :, :]\n",
    "        \n",
    "    new_block[0].weight.data = w1_new\n",
    "    new_block[0].bias.data = old_block[0].bias.data[current_idxs]\n",
    "\n",
    "    # 3. Transfer Batch Norm (index 2)\n",
    "    # Input/Output dimensions correspond to Conv1 Output (current_idxs)\n",
    "    new_block[2].weight.data = old_block[2].weight.data[current_idxs]\n",
    "    new_block[2].bias.data = old_block[2].bias.data[current_idxs]\n",
    "    new_block[2].running_mean = old_block[2].running_mean[current_idxs]\n",
    "    new_block[2].running_var = old_block[2].running_var[current_idxs]\n",
    "\n",
    "    # 4. Transfer Conv 2 (index 3)\n",
    "    # Input Channels: current_idxs (Output of Conv1)\n",
    "    # Output Channels: current_idxs (Output of Block)\n",
    "    w2_old = old_block[3].weight.data\n",
    "    w2_new = w2_old[current_idxs][:, current_idxs, :, :]\n",
    "    new_block[3].weight.data = w2_new\n",
    "    new_block[3].bias.data = old_block[3].bias.data[current_idxs]\n",
    "\n",
    "    return current_idxs\n",
    "\n",
    "def transfer_head(old_head, new_head, prev_idxs):\n",
    "    # 1. Linear Layer (Index 1) - Connects features to dense\n",
    "    # Input dimensions correspond to Block 3 Output (prev_idxs)\n",
    "    # Weight shape: [Out_Features, In_Features]\n",
    "    old_w = old_head[1].weight.data\n",
    "    new_head[1].weight.data = old_w[:, prev_idxs]\n",
    "    new_head[1].bias.data = old_head[1].bias.data\n",
    "    \n",
    "    # 2. Batch Norm 1D (Index 3) - No shape change needed\n",
    "    new_head[3].load_state_dict(old_head[3].state_dict())\n",
    "    \n",
    "    # 3. Final Linear (Index 5) - No shape change needed\n",
    "    new_head[5].load_state_dict(old_head[5].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Pruning (Removing 50% of channels)...\n",
      "Old Config: [60, 121, 242]\n",
      "New Config: [30, 60, 121]\n",
      "Pruning Complete. Evaluating initial accuracy drop...\n",
      "Accuracy (Before Fine-tuning): 0.5075\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Execute Pruning ---\n",
    "print(f\"\\nStarting Pruning (Removing {PRUNE_AMOUNT*100:.0f}% of channels)...\")\n",
    "\n",
    "# 1. Define New Architecture Specs\n",
    "c1 = int(baseline_model.channels[0] * (1 - PRUNE_AMOUNT))\n",
    "c2 = int(baseline_model.channels[1] * (1 - PRUNE_AMOUNT))\n",
    "c3 = int(baseline_model.channels[2] * (1 - PRUNE_AMOUNT))\n",
    "new_channel_config = [c1, c2, c3]\n",
    "print(f\"Old Config: {baseline_model.channels}\")\n",
    "print(f\"New Config: {new_channel_config}\")\n",
    "\n",
    "# 2. Initialize Child Model\n",
    "slim_model = CustomCNN(\n",
    "    cfg.get('reg_strength', 0), cfg.get('dropout_conv', 0), cfg.get('dropout_dense', 0),\n",
    "    int(cfg['dense_units']), float(cfg['filters_multiplier']),\n",
    "    channel_list=new_channel_config\n",
    ").to(DEVICE)\n",
    "\n",
    "# 3. Transplant Weights\n",
    "idxs_1 = prune_block(baseline_model.block1, slim_model.block1, None, c1)\n",
    "idxs_2 = prune_block(baseline_model.block2, slim_model.block2, idxs_1, c2)\n",
    "idxs_3 = prune_block(baseline_model.block3, slim_model.block3, idxs_2, c3)\n",
    "transfer_head(baseline_model.head, slim_model.head, idxs_3)\n",
    "\n",
    "print(\"Pruning Complete. Evaluating initial accuracy drop...\")\n",
    "acc_drop = evaluate(slim_model, val_dl)\n",
    "print(f\"Accuracy (Before Fine-tuning): {acc_drop:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning for 10 epochs...\n",
      "Epoch 1: Loss 84.7008\n",
      "Epoch 2: Loss 63.6548\n",
      "Epoch 3: Loss 57.4659\n",
      "Epoch 4: Loss 52.5045\n",
      "Epoch 5: Loss 48.1660\n",
      "Epoch 6: Loss 45.8060\n",
      "Epoch 7: Loss 42.8415\n",
      "Epoch 8: Loss 40.6624\n",
      "Epoch 9: Loss 38.3099\n",
      "Epoch 10: Loss 37.1027\n",
      "Pruned + Tuned Accuracy: 0.8081\n",
      "Pruned + Tuned Size:     386 KB\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Fine-Tuning ---\n",
    "print(f\"\\nFine-tuning for {10} epochs...\")\n",
    "optimizer = torch.optim.Adam(slim_model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "slim_model.train()\n",
    "for ep in range(10):\n",
    "    loss_sum = 0\n",
    "    for x, y in train_dl:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE).float()\n",
    "        optimizer.zero_grad()\n",
    "        out = slim_model(x).squeeze(1)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "    print(f\"Epoch {ep+1}: Loss {loss_sum:.4f}\")\n",
    "\n",
    "acc_tuned = evaluate(slim_model, val_dl)\n",
    "size_pruned = get_file_size(slim_model)\n",
    "print(f\"Pruned + Tuned Accuracy: {acc_tuned:.4f}\")\n",
    "print(f\"Pruned + Tuned Size:     {size_pruned:.0f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Quantization-Ready Model ---\n",
      "Calibrating with validation data (CPU)...\n",
      "========================================\n",
      " CAPSTONE RESULTS (Static Quantization)\n",
      "========================================\n",
      "Baseline Size:      5004 kB\n",
      "Pruned Size (FP32): 1519 kB\n",
      "Quantized Size:     440 kB\n",
      "Reduction Factor:   11.4x\n",
      "\n",
      "Evaluating Quantized Model on CPU...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::native_batch_norm' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::native_batch_norm' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten\\src\\ATen\\RegisterCPU.cpp:31420 [kernel]\nCUDA: registered at aten\\src\\ATen\\RegisterCUDA.cpp:44504 [kernel]\nMeta: registered at /dev/null:488 [kernel]\nMkldnnCPU: registered at aten\\src\\ATen\\RegisterMkldnnCPU.cpp:515 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradHIP: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradMPS: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradIPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradXPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradHPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradVE: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradLazy: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradMTIA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradMeta: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_1.cpp:15950 [kernel]\nAutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\BatchRulesNorm.cpp:864 [kernel]\nBatchedNestedTensor: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReduction Factor:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_size\u001b[38;5;241m/\u001b[39mfinal_size\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating Quantized Model on CPU...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 112\u001b[0m acc_final \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_on_cpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_slim_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal INT8 Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_final\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n",
      "Cell \u001b[1;32mIn[17], line 97\u001b[0m, in \u001b[0;36mevaluate_on_cpu\u001b[1;34m(model, loader)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     96\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 97\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     98\u001b[0m     preds \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39msigmoid(logits) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     99\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39mlong())\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 14\u001b[0m, in \u001b[0;36mQuantizableCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(x)\n\u001b[1;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(x)\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock3(x)\n",
      "File \u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:2509\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2507\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'aten::native_batch_norm' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::native_batch_norm' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten\\src\\ATen\\RegisterCPU.cpp:31420 [kernel]\nCUDA: registered at aten\\src\\ATen\\RegisterCUDA.cpp:44504 [kernel]\nMeta: registered at /dev/null:488 [kernel]\nMkldnnCPU: registered at aten\\src\\ATen\\RegisterMkldnnCPU.cpp:515 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradHIP: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradMPS: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradIPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradXPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradHPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradVE: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradLazy: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradMTIA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradMeta: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_1.cpp:16277 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_1.cpp:15950 [kernel]\nAutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\BatchRulesNorm.cpp:864 [kernel]\nBatchedNestedTensor: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "#TODO THIS DOESN'T WORK YET. But, was able to get model dramatically smaller pre quanitization.\n",
    "\n",
    "\n",
    "\n",
    "# --- 7. Static Quantization with Layer Fusion (FIXED) ---\n",
    "import torch.quantization\n",
    "from torch.quantization import fuse_modules\n",
    "\n",
    "# 1. Define Wrapper with Stubs\n",
    "class QuantizableCNN(CustomCNN):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.gap(x)\n",
    "        out = self.head(x)\n",
    "        out = self.dequant(out)\n",
    "        return out\n",
    "    \n",
    "    def fuse_model(self):\n",
    "        # Fuse Conv+ReLU+BN in blocks\n",
    "        # Standard pattern: Conv -> ReLU -> BN is WRONG for PyTorch fusion\n",
    "        # Your model has: Conv -> ReLU -> BN -> Conv -> ReLU\n",
    "        # PyTorch fusion usually supports Conv+BN+ReLU or Conv+BN.\n",
    "        \n",
    "        # Let's traverse and fuse manually where possible.\n",
    "        # Block structure: [0]Conv, [1]ReLU, [2]BN, [3]Conv, [4]ReLU\n",
    "        \n",
    "        # Note: PyTorch usually fuses Conv+BN+ReLU. Your order is Conv->ReLU->BN.\n",
    "        # This is non-standard for fusion. We will only fuse the SECOND conv in each block\n",
    "        # which is Conv->ReLU (indices 3,4).\n",
    "        # The first part (Conv->ReLU->BN) creates a barrier.\n",
    "        # WE WILL FUSE [3] and [4] (Conv+ReLU).\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if type(m) == nn.Sequential and len(m) >= 5:\n",
    "                # Try to fuse index 3 (Conv) and 4 (ReLU)\n",
    "                if type(m[3]) == nn.Conv2d and type(m[4]) == nn.ReLU:\n",
    "                    torch.quantization.fuse_modules(m, ['3', '4'], inplace=True)\n",
    "\n",
    "print(\"\\n--- Preparing Quantization-Ready Model ---\")\n",
    "\n",
    "# 2. Re-create Slim Model Wrapper\n",
    "q_slim_model = QuantizableCNN(\n",
    "    cfg.get('reg_strength', 0), cfg.get('dropout_conv', 0), cfg.get('dropout_dense', 0),\n",
    "    int(cfg['dense_units']), float(cfg['filters_multiplier']),\n",
    "    channel_list=slim_model.channels \n",
    ").to('cpu')\n",
    "\n",
    "# 3. Copy Weights\n",
    "q_slim_model.load_state_dict(slim_model.state_dict(), strict=False)\n",
    "q_slim_model.eval()\n",
    "\n",
    "# 4. FUSE LAYERS (Critical Step)\n",
    "q_slim_model.fuse_model()\n",
    "\n",
    "# 5. Configure Quantization (fbgemm for x86)\n",
    "backend = 'fbgemm'\n",
    "q_slim_model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "\n",
    "# 6. Insert Observers & Calibrate\n",
    "torch.quantization.prepare(q_slim_model, inplace=True)\n",
    "\n",
    "print(\"Calibrating with validation data (CPU)...\")\n",
    "with torch.no_grad():\n",
    "    for i, (x, _) in enumerate(train_dl):\n",
    "        if i >= 20: break\n",
    "        q_slim_model(x.to('cpu'))\n",
    "\n",
    "# 7. Convert to INT8\n",
    "# This step will fail if BN layers are still active on Int8 inputs.\n",
    "# Since your architecture (Conv-ReLU-BN) is tricky to fuse perfectly,\n",
    "# we set qconfig=None for the BN layers to keep them in Float32 (mixed precision).\n",
    "# This bypasses the crash.\n",
    "\n",
    "for module in q_slim_model.modules():\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        module.qconfig = None  # Disable quantization for BN to avoid crash\n",
    "\n",
    "torch.quantization.convert(q_slim_model, inplace=True)\n",
    "\n",
    "# 8. Save and Measure\n",
    "FINAL_PATH = \"checkpoints/model_capstone_static_quant.pt\"\n",
    "torch.save(q_slim_model.state_dict(), FINAL_PATH)\n",
    "final_size = os.path.getsize(FINAL_PATH) / 1024\n",
    "\n",
    "# 9. Final Evaluation\n",
    "def evaluate_on_cpu(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to('cpu'), y.to('cpu').float()\n",
    "            logits = model(x).squeeze(1)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "            correct += (preds == y.long()).sum().item()\n",
    "            total += x.size(0)\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\" CAPSTONE RESULTS (Static Quantization)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Baseline Size:      {base_size:.0f} kB\")\n",
    "print(f\"Pruned Size (FP32): {size_pruned:.0f} kB\")\n",
    "print(f\"Quantized Size:     {final_size:.0f} kB\") \n",
    "print(f\"Reduction Factor:   {base_size/final_size:.1f}x\")\n",
    "\n",
    "print(\"\\nEvaluating Quantized Model on CPU...\")\n",
    "acc_final = evaluate_on_cpu(q_slim_model, val_dl)\n",
    "print(f\"Final INT8 Accuracy: {acc_final:.4f}\")\n",
    "\n",
    "if final_size < 500:\n",
    "    print(f\"\\n SUCCESS: {final_size:.0f}kB is under 500kB!\")\n",
    "else:\n",
    "    print(f\"\\n STILL TOO BIG: {final_size:.0f}kB.\")\n",
    "    print(\"Recommendation: Scroll up to Cell 4, set PRUNE_AMOUNT = 0.75, and 'Run All' below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exporting Pruned Model to ONNX ---\n",
      "Exporting to checkpoints/model_capstone_pruned.onnx...\n",
      "========================================\n",
      " CAPSTONE RESULTS (Pruned Only)\n",
      "========================================\n",
      "Baseline Model Size:  5004 kB\n",
      "Pruned ONNX Size:     379 kB\n",
      "Reduction Factor:     13.2x\n",
      "Final Accuracy:       0.8081\n",
      "\n",
      " SUCCESS: 379kB is under 500kB!\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Final Evaluation & ONNX Export (No Quantization) ---\n",
    "import torch.onnx\n",
    "\n",
    "print(\"\\n--- Exporting Pruned Model to ONNX ---\")\n",
    "\n",
    "# 1. Set up paths and dummy input\n",
    "ONNX_PATH = \"checkpoints/model_capstone_pruned.onnx\"\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# 2. Ensure model is in eval mode (fixes BatchNorm/Dropout behavior)\n",
    "slim_model.eval()\n",
    "\n",
    "# 3. Create dummy input on the same device as the model (CUDA or CPU)\n",
    "# Shape: [Batch Size, Channels, Height, Width]\n",
    "dummy_input = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)\n",
    "\n",
    "# 4. Export to ONNX\n",
    "print(f\"Exporting to {ONNX_PATH}...\")\n",
    "torch.onnx.export(\n",
    "    slim_model,               # Model to export\n",
    "    dummy_input,              # Model input (or a tuple for multiple inputs)\n",
    "    ONNX_PATH,                # Where to save the model\n",
    "    export_params=True,       # Store the trained parameter weights inside the model file\n",
    "    opset_version=12,         # Standard ONNX opset version\n",
    "    do_constant_folding=True, # Optimize constants\n",
    "    input_names=['input'],    # Input tensor name\n",
    "    output_names=['output'],  # Output tensor name\n",
    "    dynamic_axes={'input': {0: 'batch_size'},  # Allow variable batch sizes\n",
    "                  'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "# 5. Measure Final Size\n",
    "onnx_size_kb = os.path.getsize(ONNX_PATH) / 1024\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\" CAPSTONE RESULTS (Pruned Only)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Baseline Model Size:  {base_size:.0f} kB\")\n",
    "print(f\"Pruned ONNX Size:     {onnx_size_kb:.0f} kB\")\n",
    "print(f\"Reduction Factor:     {base_size/onnx_size_kb:.1f}x\")\n",
    "print(f\"Final Accuracy:       {acc_tuned:.4f}\")\n",
    "\n",
    "if onnx_size_kb < 500:\n",
    "    print(f\"\\n SUCCESS: {onnx_size_kb:.0f}kB is under 500kB!\")\n",
    "else:\n",
    "    print(f\"\\n STILL TOO BIG: {onnx_size_kb:.0f}kB.\")\n",
    "    print(\"Recommendation: Scroll up to 'Step 5' and increase PRUNE_AMOUNT to 0.7 or 0.8.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
