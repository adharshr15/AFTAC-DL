{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c94cd05",
   "metadata": {},
   "source": [
    "\n",
    "# Cats vs. Non-Cats ‚Äî PyTorch (with Bayesian Optimization)\n",
    "\n",
    "This notebook is a drop-in replacement for the TensorFlow version:\n",
    "- Baseline custom CNN\n",
    "- MobileNetV2 transfer-learning baseline\n",
    "- Bayesian optimization via Optuna (maximize **val accuracy**)\n",
    "- Early stopping + ReduceLROnPlateau\n",
    "- Mixed precision on GPU\n",
    "- Optional test metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d5c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optional) If you need to install packages in this environment, uncomment:\n",
    "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124\n",
    "# %pip install optuna scikit-learn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ed74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, time, math, shutil, zipfile\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AMP    = torch.cuda.is_available()\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07887c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extracted to: cats-v-non-cats/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths\n",
    "zip_file_path = \"cats-v-non-cats.zip\"\n",
    "extract_dir   = \"cats-v-non-cats/\"\n",
    "TRAINING_DIR   = \"cats-v-non-cats/training/\"\n",
    "VALIDATION_DIR = \"cats-v-non-cats/validation/\"\n",
    "TESTING_DIR    = \"cats-v-non-cats/test/\"\n",
    "INCLUDE_TEST   = True\n",
    "\n",
    "# Extract (if zip present)\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "try:\n",
    "    if os.path.exists(zip_file_path):\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            for info in zip_ref.infolist():\n",
    "                if not info.filename.startswith('__MACOSX/'):\n",
    "                    arcname = info.filename\n",
    "                    if arcname.startswith('cats-v-non-cats/'):\n",
    "                        arcname = arcname[len('cats-v-non-cats/'):]\n",
    "                    if arcname:\n",
    "                        info.filename = arcname\n",
    "                        zip_ref.extract(info, extract_dir)\n",
    "        macosx_folder = os.path.join(extract_dir, '__MACOSX')\n",
    "        if os.path.exists(macosx_folder):\n",
    "            shutil.rmtree(macosx_folder)\n",
    "        print(f\"File extracted to: {extract_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during unzipping: {e}\")\n",
    "\n",
    "# Baseline config (mirrors your TF keys)\n",
    "BASELINE_CONFIG = {\n",
    "    'learning_rate':      0.001,\n",
    "    'reg_strength':       0.00001,   # weight decay\n",
    "    'dropout_conv':       0.15,\n",
    "    'dropout_dense':      0.4,\n",
    "    'dense_units':        512,\n",
    "    'filters_multiplier': 0.75,\n",
    "    'batch_size':         128,\n",
    "    'beta_1':             0.8,\n",
    "    'beta_2':             0.99\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "009514a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE = 128\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomAffine(0, translate=(0.2, 0.2), shear=0.2),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def build_loaders(batch_size: int):\n",
    "    train_ds = datasets.ImageFolder(TRAINING_DIR, transform=train_tfms)\n",
    "    val_ds   = datasets.ImageFolder(VALIDATION_DIR, transform=val_tfms)\n",
    "    kwargs = dict(num_workers=6, pin_memory=True, persistent_workers=True) if torch.cuda.is_available() else {}\n",
    "    train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    val_ld   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    if INCLUDE_TEST and os.path.isdir(TESTING_DIR):\n",
    "        test_ds = datasets.ImageFolder(TESTING_DIR, transform=val_tfms)\n",
    "        test_ld = DataLoader(test_ds, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "    else:\n",
    "        test_ld = None\n",
    "    return train_ld, val_ld, test_ld, train_ds.class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f67d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, reg_strength: float, dropout_conv: float, dropout_dense: float,\n",
    "                 dense_units: int, filters_multiplier: float):\n",
    "        super().__init__()\n",
    "        f1 = max(8, int(32 * filters_multiplier))\n",
    "        f2 = max(16, int(64 * filters_multiplier))\n",
    "        f3 = max(32, int(128 * filters_multiplier))\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, f1, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            nn.Conv2d(f1, f1, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(dropout_conv)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(f1, f2, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.Conv2d(f2, f2, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(dropout_conv)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(f2, f3, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(f3),\n",
    "            nn.Conv2d(f3, f3, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(dropout_conv)\n",
    "        )\n",
    "        self.gap   = nn.AdaptiveAvgPool2d(1)\n",
    "        self.head  = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(f3, dense_units), nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(dense_units),\n",
    "            nn.Dropout(dropout_dense),\n",
    "            nn.Linear(dense_units, 1)  # binary logit\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x); x = self.block2(x); x = self.block3(x)\n",
    "        x = self.gap(x)\n",
    "        return self.head(x)\n",
    "\n",
    "def build_mobilenet_v2(num_classes=1, dropout_dense=0.4, reg_strength=1e-4, train_base=False):\n",
    "    m = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "    m.features.requires_grad_(train_base)\n",
    "    in_features = m.classifier[-1].in_features\n",
    "    m.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_dense),\n",
    "        nn.Linear(in_features, 1)\n",
    "    )\n",
    "    return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba01d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device, amp=False):\n",
    "    model.eval()\n",
    "    loss_sum, n, correct = 0.0, 0, 0\n",
    "    all_probs, all_labels = [], []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        yf = y.float().to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=amp and device.type=='cuda'):\n",
    "            logits = model(x).squeeze(1)\n",
    "            loss = criterion(logits, yf)\n",
    "        loss_sum += float(loss.item()) * x.size(0)\n",
    "        n += x.size(0)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= 0.5).long().cpu()\n",
    "        correct += int((preds == y.cpu()).sum().item())\n",
    "        all_probs.extend(probs.detach().cpu().tolist())\n",
    "        all_labels.extend(y.cpu().tolist())\n",
    "\n",
    "    val_loss = loss_sum / max(1, n)\n",
    "    val_acc  = correct / max(1, n)\n",
    "    try:\n",
    "        if len(set(all_labels)) == 2:\n",
    "            val_auc = roc_auc_score(all_labels, np.array(all_probs))\n",
    "        else:\n",
    "            val_auc = float(\"nan\")\n",
    "    except Exception:\n",
    "        val_auc = float(\"nan\")\n",
    "    return val_loss, val_acc, val_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81b2542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_model(config: Dict, epochs: int = 5,\n",
    "                             param_name: Optional[str] = None,\n",
    "                             param_value: Optional[str] = None) -> Dict:\n",
    "    batch_size     = int(config['batch_size'])\n",
    "    lr             = float(config['learning_rate'])\n",
    "    weight_decay   = float(config['reg_strength'])\n",
    "    dropout_conv   = float(config['dropout_conv'])\n",
    "    dropout_dense  = float(config['dropout_dense'])\n",
    "    dense_units    = int(config['dense_units'])\n",
    "    filt_mult      = float(config['filters_multiplier'])\n",
    "    beta1          = float(config['beta_1'])\n",
    "    beta2          = float(config['beta_2'])\n",
    "\n",
    "    train_ld, val_ld, test_ld, class_to_idx = build_loaders(batch_size)\n",
    "    model = CustomCNN(weight_decay, dropout_conv, dropout_dense, dense_units, filt_mult).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=5, verbose=True)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
    "\n",
    "    patience = 15\n",
    "    best_acc, best_state, wait = -1.0, None, 0\n",
    "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': [], 'val_auc': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss, epoch_correct, n = 0.0, 0, 0\n",
    "        for x, y in train_ld:\n",
    "            x = x.to(DEVICE, non_blocking=True)\n",
    "            y = y.float().to(DEVICE, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=AMP):\n",
    "                logits = model(x).squeeze(1)\n",
    "                loss   = criterion(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            epoch_loss += float(loss.item()) * x.size(0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            epoch_correct += int((preds.cpu() == y.cpu().long()).sum().item())\n",
    "            n += x.size(0)\n",
    "\n",
    "        train_loss = epoch_loss / max(1, n)\n",
    "        train_acc  = epoch_correct / max(1, n)\n",
    "        val_loss, val_acc, val_auc = eval_epoch(model, val_ld, DEVICE, amp=AMP)\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}; best val_acc={best_acc:.4f}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}  \"\n",
    "              f\"loss={train_loss:.4f} acc={train_acc:.4f}  \"\n",
    "              f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} val_auc={val_auc:.4f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
    "\n",
    "    val_loss, val_acc, val_auc = eval_epoch(model, val_ld, DEVICE, amp=AMP)\n",
    "    results = {\n",
    "        'val_accuracy': float(val_acc),\n",
    "        'val_auc': float(val_auc),\n",
    "        'val_loss': float(val_loss),\n",
    "        'history': history,\n",
    "        'batch_size_used': batch_size\n",
    "    }\n",
    "    if INCLUDE_TEST and (test_ld is not None):\n",
    "        test_loss, test_acc, test_auc = eval_epoch(model, test_ld, DEVICE, amp=AMP)\n",
    "        results.update({'test_accuracy': float(test_acc), 'test_auc': float(test_auc), 'test_loss': float(test_loss)})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95900637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_mobilenet(config: Dict, epochs: int = 5) -> Dict:\n",
    "    batch_size    = int(config['batch_size'])\n",
    "    lr            = float(config['learning_rate'])\n",
    "    weight_decay  = float(config.get('reg_strength', 1e-4))\n",
    "    dropout_dense = float(config.get('dropout_dense', 0.4))\n",
    "    beta1         = float(config['beta_1'])\n",
    "    beta2         = float(config['beta_2'])\n",
    "\n",
    "    train_ld, val_ld, test_ld, _ = build_loaders(batch_size)\n",
    "    model = build_mobilenet_v2(dropout_dense=dropout_dense, reg_strength=weight_decay, train_base=False).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(beta1,beta2), weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=5, verbose=True)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
    "\n",
    "    patience = 15\n",
    "    best_acc, best_state, wait = -1.0, None, 0\n",
    "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': [], 'val_auc': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss, epoch_correct, n = 0.0, 0, 0\n",
    "        for x, y in train_ld:\n",
    "            x = x.to(DEVICE, non_blocking=True); y = y.float().to(DEVICE, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=AMP):\n",
    "                logits = model(x).squeeze(1)\n",
    "                loss   = criterion(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "\n",
    "            epoch_loss += float(loss.item()) * x.size(0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            epoch_correct += int((preds.cpu() == y.cpu().long()).sum().item())\n",
    "            n += x.size(0)\n",
    "\n",
    "        train_loss = epoch_loss / max(1, n)\n",
    "        train_acc  = epoch_correct / max(1, n)\n",
    "        val_loss, val_acc, val_auc = eval_epoch(model, val_ld, DEVICE, amp=AMP)\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}; best val_acc={best_acc:.4f}\")\n",
    "                break\n",
    "\n",
    "        print(f\"[MobileNetV2] Epoch {epoch+1}/{epochs}  \"\n",
    "              f\"loss={train_loss:.4f} acc={train_acc:.4f}  \"\n",
    "              f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} val_auc={val_auc:.4f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(DEVICE) for k, v in best_state.items()})\n",
    "\n",
    "    val_loss, val_acc, val_auc = eval_epoch(model, val_ld, DEVICE, amp=AMP)\n",
    "    results = {'val_accuracy': float(val_acc), 'val_auc': float(val_auc), 'val_loss': float(val_loss),\n",
    "               'history': history, 'batch_size_used': batch_size}\n",
    "    if INCLUDE_TEST and (test_ld is not None):\n",
    "        test_loss, test_acc, test_auc = eval_epoch(model, test_ld, DEVICE, amp=AMP)\n",
    "        results.update({'test_accuracy': float(test_acc), 'test_auc': float(test_auc), 'test_loss': float(test_loss)})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b6875ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    config = {\n",
    "        'learning_rate':      trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True),\n",
    "        'reg_strength':       trial.suggest_float('reg_strength', 1e-6, 1e-2, log=True),\n",
    "        'dropout_conv':       trial.suggest_float('dropout_conv', 0.1, 0.4),\n",
    "        'dropout_dense':      trial.suggest_float('dropout_dense', 0.2, 0.7),\n",
    "        'dense_units':        trial.suggest_categorical('dense_units', [128, 256, 512, 1024, 2048]),\n",
    "        'filters_multiplier': trial.suggest_float('filters_multiplier', 0.5, 2.0),\n",
    "        'batch_size':         trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "        'beta_1':             trial.suggest_float('beta_1', 0.7, 0.99),\n",
    "        'beta_2':             trial.suggest_float('beta_2', 0.9, 0.9999)\n",
    "    }\n",
    "    result = train_and_evaluate_model(config, epochs=30, param_name=f\"trial_{trial.number}\", param_value=\"bayesian\")\n",
    "    trial.set_user_attr('val_auc',  result['val_auc'])\n",
    "    trial.set_user_attr('val_loss', result['val_loss'])\n",
    "    acc = float(result['val_accuracy'])\n",
    "    print(f\"Trial {trial.number}: Accuracy = {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "def run_bayesian_optimization(n_trials=50, timeout=None):\n",
    "    print(f\"Starting Bayesian Optimization with {n_trials} trials...\")\n",
    "    study = optuna.create_study(direction='maximize',\n",
    "                                sampler=TPESampler(seed=42),\n",
    "                                study_name='cnn_hyperparameter_optimization_pytorch')\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BAYESIAN OPTIMIZATION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "    print(f\"Best trial number: {study.best_trial.number}\")\n",
    "    print(f\"Best validation accuracy: {study.best_value:.4f}\")\n",
    "    print(\"\\nüèÜ Best hyperparameters:\")\n",
    "    for k, v in study.best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    bt = study.best_trial\n",
    "    if 'val_auc' in bt.user_attrs:\n",
    "        print(\"\\nBest trial metrics:\")\n",
    "        print(f\"  Validation AUC:  {bt.user_attrs['val_auc']:.4f}\")\n",
    "        print(f\"  Validation Loss: {bt.user_attrs['val_loss']:.4f}\")\n",
    "    return study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d6c2547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50  loss=0.5459 acc=0.7251  val_loss=0.7081 val_acc=0.6798 val_auc=0.7462\n",
      "Epoch 2/50  loss=0.4633 acc=0.7824  val_loss=0.9065 val_acc=0.6507 val_auc=0.8620\n",
      "Epoch 3/50  loss=0.4156 acc=0.8136  val_loss=0.9453 val_acc=0.6582 val_auc=0.8683\n",
      "Epoch 4/50  loss=0.3876 acc=0.8308  val_loss=0.3361 val_acc=0.8605 val_auc=0.9338\n",
      "Epoch 5/50  loss=0.3543 acc=0.8469  val_loss=0.3692 val_acc=0.8511 val_auc=0.9292\n",
      "Epoch 6/50  loss=0.3319 acc=0.8570  val_loss=0.5004 val_acc=0.7875 val_auc=0.9453\n",
      "Epoch 7/50  loss=0.3149 acc=0.8674  val_loss=0.3484 val_acc=0.8708 val_auc=0.9536\n",
      "Epoch 8/50  loss=0.2999 acc=0.8745  val_loss=0.2662 val_acc=0.8923 val_auc=0.9578\n",
      "Epoch 9/50  loss=0.2805 acc=0.8841  val_loss=0.3090 val_acc=0.8764 val_auc=0.9671\n",
      "Epoch 10/50  loss=0.2772 acc=0.8853  val_loss=0.2964 val_acc=0.8830 val_auc=0.9702\n",
      "Epoch 11/50  loss=0.2751 acc=0.8864  val_loss=0.2257 val_acc=0.9036 val_auc=0.9739\n",
      "Epoch 12/50  loss=0.2609 acc=0.8930  val_loss=0.7482 val_acc=0.7247 val_auc=0.9624\n",
      "Epoch 13/50  loss=0.2438 acc=0.9004  val_loss=0.3765 val_acc=0.8642 val_auc=0.9692\n",
      "Epoch 14/50  loss=0.2477 acc=0.9031  val_loss=0.3969 val_acc=0.8399 val_auc=0.9659\n",
      "Epoch 15/50  loss=0.2334 acc=0.9070  val_loss=0.2169 val_acc=0.9007 val_auc=0.9742\n",
      "Epoch 16/50  loss=0.2343 acc=0.9036  val_loss=0.7525 val_acc=0.7500 val_auc=0.9627\n",
      "Epoch 17/50  loss=0.2320 acc=0.9050  val_loss=0.2055 val_acc=0.9213 val_auc=0.9791\n",
      "Epoch 18/50  loss=0.2163 acc=0.9133  val_loss=0.2118 val_acc=0.9223 val_auc=0.9789\n",
      "Epoch 19/50  loss=0.2124 acc=0.9155  val_loss=0.4593 val_acc=0.8371 val_auc=0.9700\n",
      "Epoch 20/50  loss=0.2106 acc=0.9194  val_loss=0.1758 val_acc=0.9335 val_auc=0.9827\n",
      "Epoch 21/50  loss=0.2025 acc=0.9199  val_loss=0.2695 val_acc=0.8961 val_auc=0.9728\n",
      "Epoch 22/50  loss=0.1930 acc=0.9221  val_loss=0.3454 val_acc=0.8727 val_auc=0.9792\n",
      "Epoch 23/50  loss=0.2040 acc=0.9185  val_loss=0.2519 val_acc=0.9054 val_auc=0.9801\n",
      "Epoch 24/50  loss=0.2013 acc=0.9215  val_loss=0.1943 val_acc=0.9073 val_auc=0.9836\n",
      "Epoch 25/50  loss=0.1941 acc=0.9187  val_loss=0.2277 val_acc=0.9167 val_auc=0.9785\n",
      "Epoch 26/50  loss=0.1913 acc=0.9238  val_loss=0.1714 val_acc=0.9363 val_auc=0.9847\n",
      "Epoch 27/50  loss=0.1882 acc=0.9254  val_loss=0.3343 val_acc=0.8736 val_auc=0.9802\n",
      "Epoch 28/50  loss=0.1768 acc=0.9295  val_loss=0.5195 val_acc=0.8174 val_auc=0.9644\n",
      "Epoch 29/50  loss=0.1763 acc=0.9302  val_loss=0.2241 val_acc=0.9298 val_auc=0.9771\n",
      "Epoch 30/50  loss=0.1760 acc=0.9290  val_loss=0.2199 val_acc=0.9213 val_auc=0.9867\n",
      "Epoch 31/50  loss=0.1732 acc=0.9335  val_loss=0.1938 val_acc=0.9316 val_auc=0.9847\n",
      "Epoch 32/50  loss=0.1708 acc=0.9300  val_loss=0.2093 val_acc=0.9242 val_auc=0.9849\n",
      "Epoch 33/50  loss=0.1433 acc=0.9425  val_loss=0.1304 val_acc=0.9466 val_auc=0.9917\n",
      "Epoch 34/50  loss=0.1345 acc=0.9485  val_loss=0.2346 val_acc=0.9204 val_auc=0.9889\n",
      "Epoch 35/50  loss=0.1338 acc=0.9459  val_loss=0.1883 val_acc=0.9335 val_auc=0.9907\n",
      "Epoch 36/50  loss=0.1338 acc=0.9470  val_loss=0.2559 val_acc=0.9120 val_auc=0.9889\n",
      "Epoch 37/50  loss=0.1329 acc=0.9477  val_loss=0.2261 val_acc=0.9185 val_auc=0.9893\n",
      "Epoch 38/50  loss=0.1270 acc=0.9489  val_loss=0.1613 val_acc=0.9391 val_auc=0.9913\n",
      "Epoch 39/50  loss=0.1218 acc=0.9532  val_loss=0.2289 val_acc=0.9157 val_auc=0.9904\n",
      "Epoch 40/50  loss=0.1272 acc=0.9485  val_loss=0.1708 val_acc=0.9335 val_auc=0.9916\n",
      "Epoch 41/50  loss=0.1190 acc=0.9534  val_loss=0.2025 val_acc=0.9279 val_auc=0.9910\n",
      "Epoch 42/50  loss=0.1165 acc=0.9533  val_loss=0.1916 val_acc=0.9288 val_auc=0.9912\n",
      "Epoch 43/50  loss=0.1188 acc=0.9519  val_loss=0.2009 val_acc=0.9270 val_auc=0.9912\n",
      "Epoch 44/50  loss=0.1204 acc=0.9533  val_loss=0.1943 val_acc=0.9307 val_auc=0.9920\n",
      "Epoch 45/50  loss=0.1142 acc=0.9572  val_loss=0.1775 val_acc=0.9335 val_auc=0.9919\n",
      "Epoch 46/50  loss=0.1139 acc=0.9563  val_loss=0.1898 val_acc=0.9307 val_auc=0.9914\n",
      "Epoch 47/50  loss=0.1159 acc=0.9551  val_loss=0.1843 val_acc=0.9335 val_auc=0.9918\n",
      "Early stopping at epoch 48; best val_acc=0.9466\n",
      "\n",
      "Baseline Results:\n",
      "Accuracy: 0.9466\n",
      "AUC:      0.9917\n",
      "Loss:     0.1304\n",
      "\n",
      "Training MobileNetV2 baseline (transfer learning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MobileNetV2] Epoch 1/10  loss=0.3584 acc=0.8574  val_loss=0.1770 val_acc=0.9401 val_auc=0.9909\n",
      "[MobileNetV2] Epoch 2/10  loss=0.2287 acc=0.9133  val_loss=0.1452 val_acc=0.9448 val_auc=0.9922\n",
      "[MobileNetV2] Epoch 3/10  loss=0.2128 acc=0.9187  val_loss=0.1289 val_acc=0.9494 val_auc=0.9927\n",
      "[MobileNetV2] Epoch 4/10  loss=0.2075 acc=0.9163  val_loss=0.1218 val_acc=0.9569 val_auc=0.9929\n",
      "[MobileNetV2] Epoch 5/10  loss=0.1958 acc=0.9225  val_loss=0.1319 val_acc=0.9438 val_auc=0.9932\n",
      "[MobileNetV2] Epoch 6/10  loss=0.1878 acc=0.9279  val_loss=0.1156 val_acc=0.9579 val_auc=0.9935\n",
      "[MobileNetV2] Epoch 7/10  loss=0.1888 acc=0.9272  val_loss=0.1111 val_acc=0.9625 val_auc=0.9933\n",
      "[MobileNetV2] Epoch 8/10  loss=0.1795 acc=0.9299  val_loss=0.1032 val_acc=0.9644 val_auc=0.9937\n",
      "[MobileNetV2] Epoch 9/10  loss=0.1915 acc=0.9236  val_loss=0.1161 val_acc=0.9569 val_auc=0.9937\n",
      "[MobileNetV2] Epoch 10/10  loss=0.1824 acc=0.9290  val_loss=0.1330 val_acc=0.9438 val_auc=0.9941\n",
      "\n",
      "MobileNetV2 Baseline Results:\n",
      "Accuracy: 0.9644\n",
      "AUC:      0.9937\n",
      "Loss:     0.1032\n",
      "\n",
      "=== QUICK COMPARISON ===\n",
      "Custom CNN  - Accuracy: 0.9466, AUC: 0.9917\n",
      "MobileNetV2 - Accuracy: 0.9644, AUC: 0.9937\n",
      "Accuracy diff (MobileNet - Custom): +0.0178\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Training baseline model...\")\n",
    "baseline_result = train_and_evaluate_model(BASELINE_CONFIG, epochs=50)\n",
    "print(\"\\nBaseline Results:\")\n",
    "print(f\"Accuracy: {baseline_result['val_accuracy']:.4f}\")\n",
    "print(f\"AUC:      {baseline_result['val_auc']:.4f}\")\n",
    "print(f\"Loss:     {baseline_result['val_loss']:.4f}\")\n",
    "\n",
    "print(\"\\nTraining MobileNetV2 baseline (transfer learning)...\")\n",
    "mobilenet_config = BASELINE_CONFIG.copy()\n",
    "mobilenet_result = train_and_evaluate_mobilenet(mobilenet_config, epochs=10)\n",
    "print(\"\\nMobileNetV2 Baseline Results:\")\n",
    "print(f\"Accuracy: {mobilenet_result['val_accuracy']:.4f}\")\n",
    "print(f\"AUC:      {mobilenet_result['val_auc']:.4f}\")\n",
    "print(f\"Loss:     {mobilenet_result['val_loss']:.4f}\")\n",
    "\n",
    "print('\\n=== QUICK COMPARISON ===')\n",
    "try:\n",
    "    print(f\"Custom CNN  - Accuracy: {baseline_result['val_accuracy']:.4f}, AUC: {baseline_result['val_auc']:.4f}\")\n",
    "    print(f\"MobileNetV2 - Accuracy: {mobilenet_result['val_accuracy']:.4f}, AUC: {mobilenet_result['val_auc']:.4f}\")\n",
    "    acc_diff = mobilenet_result['val_accuracy'] - baseline_result['val_accuracy']\n",
    "    print(f\"Accuracy diff (MobileNet - Custom): {acc_diff:+.4f}\")\n",
    "except Exception as e:\n",
    "    print('Comparison failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd621192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30  loss=0.2169 acc=0.9176  val_loss=0.2675 val_acc=0.8979 val_auc=0.9795\n",
      "Epoch 26/30  loss=0.2141 acc=0.9167  val_loss=0.2592 val_acc=0.9007 val_auc=0.9801\n",
      "Epoch 27/30  loss=0.2105 acc=0.9139  val_loss=0.3241 val_acc=0.8764 val_auc=0.9734\n",
      "Epoch 28/30  loss=0.2049 acc=0.9194  val_loss=0.2897 val_acc=0.8839 val_auc=0.9774\n",
      "Epoch 29/30  loss=0.2039 acc=0.9222  val_loss=0.2799 val_acc=0.8914 val_auc=0.9797\n",
      "Epoch 30/30  loss=0.1997 acc=0.9201  val_loss=0.2718 val_acc=0.8923 val_auc=0.9800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 22:08:50,701] Trial 3 finished with value: 0.903558052434457 and parameters: {'learning_rate': 0.0015375920235481753, 'reg_strength': 5.4880470007660465e-06, 'dropout_conv': 0.39087538832936763, 'dropout_dense': 0.5875664116805572, 'dense_units': 128, 'filters_multiplier': 0.7939742936287177, 'batch_size': 64, 'beta_1': 0.9403338776540595, 'beta_2': 0.9356396573366896}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3: Accuracy = 0.9036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5470 acc=0.7258  val_loss=0.4722 val_acc=0.7818 val_auc=0.8555\n",
      "Epoch 2/30  loss=0.4657 acc=0.7844  val_loss=0.5406 val_acc=0.7603 val_auc=0.8589\n",
      "Epoch 3/30  loss=0.4229 acc=0.8103  val_loss=0.3763 val_acc=0.8380 val_auc=0.9264\n",
      "Epoch 4/30  loss=0.3887 acc=0.8294  val_loss=0.4682 val_acc=0.8099 val_auc=0.9081\n",
      "Epoch 5/30  loss=0.3539 acc=0.8467  val_loss=0.3504 val_acc=0.8586 val_auc=0.9271\n",
      "Epoch 6/30  loss=0.3303 acc=0.8634  val_loss=0.3624 val_acc=0.8614 val_auc=0.9398\n",
      "Epoch 7/30  loss=0.3164 acc=0.8634  val_loss=0.4088 val_acc=0.8390 val_auc=0.9219\n",
      "Epoch 8/30  loss=0.3054 acc=0.8702  val_loss=0.3122 val_acc=0.8745 val_auc=0.9452\n",
      "Epoch 9/30  loss=0.2945 acc=0.8769  val_loss=0.2586 val_acc=0.8876 val_auc=0.9632\n",
      "Epoch 10/30  loss=0.2702 acc=0.8902  val_loss=0.2824 val_acc=0.8830 val_auc=0.9573\n",
      "Epoch 11/30  loss=0.2582 acc=0.8962  val_loss=0.3086 val_acc=0.8839 val_auc=0.9641\n",
      "Epoch 12/30  loss=0.2448 acc=0.9023  val_loss=0.3341 val_acc=0.8839 val_auc=0.9631\n",
      "Epoch 13/30  loss=0.2432 acc=0.8997  val_loss=0.2635 val_acc=0.8876 val_auc=0.9748\n",
      "Epoch 14/30  loss=0.2423 acc=0.9032  val_loss=0.2748 val_acc=0.8876 val_auc=0.9660\n",
      "Epoch 15/30  loss=0.2332 acc=0.9093  val_loss=0.4565 val_acc=0.8361 val_auc=0.9713\n",
      "Epoch 16/30  loss=0.2008 acc=0.9233  val_loss=0.1793 val_acc=0.9260 val_auc=0.9807\n",
      "Epoch 17/30  loss=0.1917 acc=0.9232  val_loss=0.2480 val_acc=0.9054 val_auc=0.9745\n",
      "Epoch 18/30  loss=0.1839 acc=0.9274  val_loss=0.1900 val_acc=0.9326 val_auc=0.9824\n",
      "Epoch 19/30  loss=0.1779 acc=0.9291  val_loss=0.2120 val_acc=0.9157 val_auc=0.9778\n",
      "Epoch 20/30  loss=0.1693 acc=0.9347  val_loss=0.2214 val_acc=0.9139 val_auc=0.9770\n",
      "Epoch 21/30  loss=0.1763 acc=0.9315  val_loss=0.1703 val_acc=0.9335 val_auc=0.9839\n",
      "Epoch 22/30  loss=0.1670 acc=0.9341  val_loss=0.2073 val_acc=0.9223 val_auc=0.9779\n",
      "Epoch 23/30  loss=0.1639 acc=0.9345  val_loss=0.1958 val_acc=0.9260 val_auc=0.9798\n",
      "Epoch 24/30  loss=0.1637 acc=0.9386  val_loss=0.1839 val_acc=0.9335 val_auc=0.9827\n",
      "Epoch 25/30  loss=0.1678 acc=0.9346  val_loss=0.1980 val_acc=0.9195 val_auc=0.9811\n",
      "Epoch 26/30  loss=0.1554 acc=0.9393  val_loss=0.1830 val_acc=0.9307 val_auc=0.9832\n",
      "Epoch 27/30  loss=0.1544 acc=0.9421  val_loss=0.1958 val_acc=0.9298 val_auc=0.9818\n",
      "Epoch 28/30  loss=0.1478 acc=0.9436  val_loss=0.1784 val_acc=0.9298 val_auc=0.9837\n",
      "Epoch 29/30  loss=0.1458 acc=0.9459  val_loss=0.1883 val_acc=0.9298 val_auc=0.9830\n",
      "Epoch 30/30  loss=0.1473 acc=0.9448  val_loss=0.1759 val_acc=0.9345 val_auc=0.9846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 22:14:36,576] Trial 4 finished with value: 0.9344569288389513 and parameters: {'learning_rate': 0.000132965214572995, 'reg_strength': 0.00014817820606039095, 'dropout_conv': 0.1422772674924288, 'dropout_dense': 0.6010984903770198, 'dense_units': 256, 'filters_multiplier': 1.7231921426822512, 'batch_size': 64, 'beta_1': 0.803955061277839, 'beta_2': 0.9115753190465605}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4: Accuracy = 0.9345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=1.0887 acc=0.5534  val_loss=14.7417 val_acc=0.5000 val_auc=0.5000\n",
      "Epoch 2/30  loss=0.6587 acc=0.6390  val_loss=0.6478 val_acc=0.6227 val_auc=0.6770\n",
      "Epoch 3/30  loss=0.6369 acc=0.6367  val_loss=0.6899 val_acc=0.5047 val_auc=0.7546\n",
      "Epoch 4/30  loss=0.6178 acc=0.6684  val_loss=0.6961 val_acc=0.5197 val_auc=0.7575\n",
      "Epoch 5/30  loss=0.6264 acc=0.6465  val_loss=0.8967 val_acc=0.5000 val_auc=0.7326\n",
      "Epoch 6/30  loss=0.6257 acc=0.6582  val_loss=0.6207 val_acc=0.5674 val_auc=0.7875\n",
      "Epoch 7/30  loss=0.5925 acc=0.6784  val_loss=0.7633 val_acc=0.6049 val_auc=0.7949\n",
      "Epoch 8/30  loss=0.5971 acc=0.6801  val_loss=1.2446 val_acc=0.6404 val_auc=0.7831\n",
      "Epoch 9/30  loss=0.5862 acc=0.6915  val_loss=0.6655 val_acc=0.7060 val_auc=0.7731\n",
      "Epoch 10/30  loss=0.5732 acc=0.7056  val_loss=0.6615 val_acc=0.7350 val_auc=0.7999\n",
      "Epoch 11/30  loss=0.5662 acc=0.7186  val_loss=1.3096 val_acc=0.5974 val_auc=0.7783\n",
      "Epoch 12/30  loss=0.5492 acc=0.7289  val_loss=0.5273 val_acc=0.7556 val_auc=0.8380\n",
      "Epoch 13/30  loss=0.5354 acc=0.7375  val_loss=0.5213 val_acc=0.7687 val_auc=0.8488\n",
      "Epoch 14/30  loss=0.5332 acc=0.7468  val_loss=0.6413 val_acc=0.5787 val_auc=0.8562\n",
      "Epoch 15/30  loss=0.5378 acc=0.7409  val_loss=2.2234 val_acc=0.5000 val_auc=0.8479\n",
      "Epoch 16/30  loss=0.5205 acc=0.7542  val_loss=0.8605 val_acc=0.5609 val_auc=0.8440\n",
      "Epoch 17/30  loss=0.5095 acc=0.7604  val_loss=0.8949 val_acc=0.5122 val_auc=0.8442\n",
      "Epoch 18/30  loss=0.5110 acc=0.7576  val_loss=1.0361 val_acc=0.5103 val_auc=0.8330\n",
      "Epoch 19/30  loss=0.5256 acc=0.7534  val_loss=0.5074 val_acc=0.7444 val_auc=0.8651\n",
      "Epoch 20/30  loss=0.4666 acc=0.7886  val_loss=0.4820 val_acc=0.7715 val_auc=0.8741\n",
      "Epoch 21/30  loss=0.4495 acc=0.7944  val_loss=0.5198 val_acc=0.7481 val_auc=0.8752\n",
      "Epoch 22/30  loss=0.4445 acc=0.7999  val_loss=0.5935 val_acc=0.7060 val_auc=0.8868\n",
      "Epoch 23/30  loss=0.4290 acc=0.8061  val_loss=0.4778 val_acc=0.7781 val_auc=0.8950\n",
      "Epoch 24/30  loss=0.4173 acc=0.8173  val_loss=0.4357 val_acc=0.7865 val_auc=0.9013\n",
      "Epoch 25/30  loss=0.4139 acc=0.8133  val_loss=0.4013 val_acc=0.8249 val_auc=0.9019\n",
      "Epoch 26/30  loss=0.4028 acc=0.8184  val_loss=0.9757 val_acc=0.5468 val_auc=0.9167\n",
      "Epoch 27/30  loss=0.4023 acc=0.8223  val_loss=0.6330 val_acc=0.7163 val_auc=0.9236\n",
      "Epoch 28/30  loss=0.3853 acc=0.8320  val_loss=1.0534 val_acc=0.6077 val_auc=0.9162\n",
      "Epoch 29/30  loss=0.3859 acc=0.8327  val_loss=0.3799 val_acc=0.8333 val_auc=0.9267\n",
      "Epoch 30/30  loss=0.3815 acc=0.8303  val_loss=0.4319 val_acc=0.8099 val_auc=0.9254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 22:20:17,611] Trial 5 finished with value: 0.8333333333333334 and parameters: {'learning_rate': 0.028340904295147733, 'reg_strength': 0.0003113095956122125, 'dropout_conv': 0.19926940745579477, 'dropout_dense': 0.23177917514301183, 'dense_units': 2048, 'filters_multiplier': 1.2083223877429239, 'batch_size': 64, 'beta_1': 0.9235804821868226, 'beta_2': 0.9493301800768027}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5: Accuracy = 0.8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5800 acc=0.7121  val_loss=0.5524 val_acc=0.7818 val_auc=0.8571\n",
      "Epoch 2/30  loss=0.5001 acc=0.7635  val_loss=0.5714 val_acc=0.7884 val_auc=0.8859\n",
      "Epoch 3/30  loss=0.4451 acc=0.7975  val_loss=0.7016 val_acc=0.6957 val_auc=0.8606\n",
      "Epoch 4/30  loss=0.4081 acc=0.8258  val_loss=0.3967 val_acc=0.8493 val_auc=0.9241\n",
      "Epoch 5/30  loss=0.3853 acc=0.8425  val_loss=0.3327 val_acc=0.8633 val_auc=0.9336\n",
      "Epoch 6/30  loss=0.3545 acc=0.8531  val_loss=0.3114 val_acc=0.8783 val_auc=0.9484\n",
      "Epoch 7/30  loss=0.3472 acc=0.8556  val_loss=0.3068 val_acc=0.8680 val_auc=0.9458\n",
      "Epoch 8/30  loss=0.3203 acc=0.8700  val_loss=0.3420 val_acc=0.8652 val_auc=0.9441\n",
      "Epoch 9/30  loss=0.3233 acc=0.8693  val_loss=0.3120 val_acc=0.8736 val_auc=0.9553\n",
      "Epoch 10/30  loss=0.3148 acc=0.8725  val_loss=0.3361 val_acc=0.8493 val_auc=0.9621\n",
      "Epoch 11/30  loss=0.2955 acc=0.8790  val_loss=0.2623 val_acc=0.8970 val_auc=0.9643\n",
      "Epoch 12/30  loss=0.2949 acc=0.8772  val_loss=0.2414 val_acc=0.9045 val_auc=0.9648\n",
      "Epoch 13/30  loss=0.2786 acc=0.8885  val_loss=0.2194 val_acc=0.9139 val_auc=0.9731\n",
      "Epoch 14/30  loss=0.2790 acc=0.8848  val_loss=0.2756 val_acc=0.8895 val_auc=0.9628\n",
      "Epoch 15/30  loss=0.2727 acc=0.8881  val_loss=0.3276 val_acc=0.8605 val_auc=0.9716\n",
      "Epoch 16/30  loss=0.2632 acc=0.8895  val_loss=0.2059 val_acc=0.9242 val_auc=0.9775\n",
      "Epoch 17/30  loss=0.2592 acc=0.8943  val_loss=0.2765 val_acc=0.8830 val_auc=0.9771\n",
      "Epoch 18/30  loss=0.2577 acc=0.8962  val_loss=0.2449 val_acc=0.8989 val_auc=0.9694\n",
      "Epoch 19/30  loss=0.2484 acc=0.9018  val_loss=0.2307 val_acc=0.9026 val_auc=0.9780\n",
      "Epoch 20/30  loss=0.2443 acc=0.9007  val_loss=0.2377 val_acc=0.9054 val_auc=0.9677\n",
      "Epoch 21/30  loss=0.2381 acc=0.9055  val_loss=0.2417 val_acc=0.9064 val_auc=0.9783\n",
      "Epoch 22/30  loss=0.2314 acc=0.9054  val_loss=0.1834 val_acc=0.9213 val_auc=0.9817\n",
      "Epoch 23/30  loss=0.2004 acc=0.9215  val_loss=0.1745 val_acc=0.9298 val_auc=0.9861\n",
      "Epoch 24/30  loss=0.1849 acc=0.9258  val_loss=0.1603 val_acc=0.9401 val_auc=0.9875\n",
      "Epoch 25/30  loss=0.1865 acc=0.9261  val_loss=0.1631 val_acc=0.9363 val_auc=0.9871\n",
      "Epoch 26/30  loss=0.1826 acc=0.9321  val_loss=0.1980 val_acc=0.9242 val_auc=0.9853\n",
      "Epoch 27/30  loss=0.1840 acc=0.9270  val_loss=0.1415 val_acc=0.9410 val_auc=0.9881\n",
      "Epoch 28/30  loss=0.1718 acc=0.9326  val_loss=0.1676 val_acc=0.9335 val_auc=0.9866\n",
      "Epoch 29/30  loss=0.1732 acc=0.9311  val_loss=0.1547 val_acc=0.9438 val_auc=0.9881\n",
      "Epoch 30/30  loss=0.1794 acc=0.9276  val_loss=0.1734 val_acc=0.9270 val_auc=0.9856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 22:25:52,999] Trial 6 finished with value: 0.9438202247191011 and parameters: {'learning_rate': 0.0012329098365270509, 'reg_strength': 5.1305517605898387e-05, 'dropout_conv': 0.10762573802322856, 'dropout_dense': 0.25394571349665224, 'dense_units': 2048, 'filters_multiplier': 0.8739383437233124, 'batch_size': 32, 'beta_1': 0.7840279213449927, 'beta_2': 0.9161060065966751}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6: Accuracy = 0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.9241 acc=0.5610  val_loss=0.7396 val_acc=0.5000 val_auc=0.5771\n",
      "Epoch 2/30  loss=0.6432 acc=0.6319  val_loss=0.6942 val_acc=0.5000 val_auc=0.7426\n",
      "Epoch 3/30  loss=0.6231 acc=0.6530  val_loss=0.8476 val_acc=0.5000 val_auc=0.6994\n",
      "Epoch 4/30  loss=0.6111 acc=0.6726  val_loss=1.1818 val_acc=0.5000 val_auc=0.5937\n",
      "Epoch 5/30  loss=0.6218 acc=0.6550  val_loss=0.6101 val_acc=0.6236 val_auc=0.7704\n",
      "Epoch 6/30  loss=0.6291 acc=0.6485  val_loss=0.6648 val_acc=0.4850 val_auc=0.6470\n",
      "Epoch 7/30  loss=0.6345 acc=0.6429  val_loss=2.2101 val_acc=0.5000 val_auc=0.7360\n",
      "Epoch 8/30  loss=0.6317 acc=0.6510  val_loss=0.5860 val_acc=0.6845 val_auc=0.7323\n",
      "Epoch 9/30  loss=0.6238 acc=0.6533  val_loss=0.5855 val_acc=0.6704 val_auc=0.7656\n",
      "Epoch 10/30  loss=0.6279 acc=0.6516  val_loss=0.5962 val_acc=0.7144 val_auc=0.7850\n",
      "Epoch 11/30  loss=0.6238 acc=0.6596  val_loss=0.5837 val_acc=0.7116 val_auc=0.7489\n",
      "Epoch 12/30  loss=0.6263 acc=0.6490  val_loss=0.5889 val_acc=0.6873 val_auc=0.7387\n",
      "Epoch 13/30  loss=0.6281 acc=0.6452  val_loss=0.7022 val_acc=0.5702 val_auc=0.6796\n",
      "Epoch 14/30  loss=0.6179 acc=0.6654  val_loss=0.6464 val_acc=0.6976 val_auc=0.7172\n",
      "Epoch 15/30  loss=0.6373 acc=0.6388  val_loss=0.6240 val_acc=0.6330 val_auc=0.7909\n",
      "Epoch 16/30  loss=0.6144 acc=0.6596  val_loss=0.5796 val_acc=0.6882 val_auc=0.7394\n",
      "Epoch 17/30  loss=0.5755 acc=0.6986  val_loss=0.5642 val_acc=0.7032 val_auc=0.8160\n",
      "Epoch 18/30  loss=0.5563 acc=0.7193  val_loss=0.9057 val_acc=0.5459 val_auc=0.7162\n",
      "Epoch 19/30  loss=0.5492 acc=0.7213  val_loss=0.5189 val_acc=0.7378 val_auc=0.8243\n",
      "Epoch 20/30  loss=0.5525 acc=0.7255  val_loss=0.5028 val_acc=0.7453 val_auc=0.8349\n",
      "Epoch 21/30  loss=0.5407 acc=0.7273  val_loss=0.6142 val_acc=0.6283 val_auc=0.8252\n",
      "Epoch 22/30  loss=0.5438 acc=0.7305  val_loss=0.5044 val_acc=0.7500 val_auc=0.8427\n",
      "Epoch 23/30  loss=0.5402 acc=0.7338  val_loss=0.5478 val_acc=0.7331 val_auc=0.8358\n",
      "Epoch 24/30  loss=0.5361 acc=0.7355  val_loss=0.5570 val_acc=0.7406 val_auc=0.8485\n",
      "Epoch 25/30  loss=0.5436 acc=0.7288  val_loss=0.5386 val_acc=0.7238 val_auc=0.8272\n",
      "Epoch 26/30  loss=0.5241 acc=0.7412  val_loss=0.4668 val_acc=0.7884 val_auc=0.8700\n",
      "Epoch 27/30  loss=0.5065 acc=0.7508  val_loss=0.4487 val_acc=0.7912 val_auc=0.8698\n",
      "Epoch 28/30  loss=0.5108 acc=0.7541  val_loss=0.5478 val_acc=0.7603 val_auc=0.8339\n",
      "Epoch 29/30  loss=0.5027 acc=0.7646  val_loss=0.4585 val_acc=0.7846 val_auc=0.8715\n",
      "Epoch 30/30  loss=0.4892 acc=0.7670  val_loss=0.8701 val_acc=0.5552 val_auc=0.8663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 22:31:41,524] Trial 7 finished with value: 0.7911985018726592 and parameters: {'learning_rate': 0.05233480488540085, 'reg_strength': 0.0017079750342958238, 'dropout_conv': 0.2900211269531271, 'dropout_dense': 0.6357302950938588, 'dense_units': 512, 'filters_multiplier': 1.8441369498852398, 'batch_size': 128, 'beta_1': 0.937224282117523, 'beta_2': 0.9859869852673088}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7: Accuracy = 0.7912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5671 acc=0.7042  val_loss=0.8086 val_acc=0.6433 val_auc=0.7874\n",
      "Epoch 2/30  loss=0.5247 acc=0.7331  val_loss=0.8172 val_acc=0.6564 val_auc=0.7682\n",
      "Epoch 3/30  loss=0.5008 acc=0.7523  val_loss=1.1390 val_acc=0.6292 val_auc=0.7568\n",
      "Epoch 4/30  loss=0.4718 acc=0.7817  val_loss=0.9815 val_acc=0.6489 val_auc=0.7965\n",
      "Epoch 5/30  loss=0.4622 acc=0.7856  val_loss=1.1375 val_acc=0.6348 val_auc=0.7780\n",
      "Epoch 6/30  loss=0.4500 acc=0.7864  val_loss=1.4510 val_acc=0.6217 val_auc=0.7801\n",
      "Epoch 7/30  loss=0.4440 acc=0.7934  val_loss=1.3405 val_acc=0.6348 val_auc=0.7664\n",
      "Epoch 8/30  loss=0.4350 acc=0.8024  val_loss=1.4969 val_acc=0.6283 val_auc=0.7274\n",
      "Epoch 9/30  loss=0.4274 acc=0.8038  val_loss=1.4730 val_acc=0.6442 val_auc=0.7416\n",
      "Epoch 10/30  loss=0.4153 acc=0.8184  val_loss=1.6079 val_acc=0.6283 val_auc=0.7451\n",
      "Epoch 11/30  loss=0.4164 acc=0.8114  val_loss=1.5976 val_acc=0.6348 val_auc=0.7456\n",
      "Epoch 12/30  loss=0.4152 acc=0.8140  val_loss=1.5114 val_acc=0.6404 val_auc=0.7542\n",
      "Epoch 13/30  loss=0.4162 acc=0.8152  val_loss=1.5341 val_acc=0.6414 val_auc=0.7419\n",
      "Epoch 14/30  loss=0.4150 acc=0.8125  val_loss=1.5297 val_acc=0.6358 val_auc=0.7550\n",
      "Epoch 15/30  loss=0.4136 acc=0.8166  val_loss=1.5548 val_acc=0.6358 val_auc=0.7506\n",
      "Epoch 16/30  loss=0.4117 acc=0.8116  val_loss=1.5408 val_acc=0.6395 val_auc=0.7501\n",
      "Early stopping at epoch 17; best val_acc=0.6564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 22:35:05,070] Trial 8 finished with value: 0.6563670411985019 and parameters: {'learning_rate': 1.0661259689433896e-05, 'reg_strength': 0.00011040511903162256, 'dropout_conv': 0.22522330094463372, 'dropout_dense': 0.31105390523536514, 'dense_units': 512, 'filters_multiplier': 1.5545284383427669, 'batch_size': 32, 'beta_1': 0.8442020667087917, 'beta_2': 0.9300577431506953}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8: Accuracy = 0.6564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5511 acc=0.7179  val_loss=0.6147 val_acc=0.6826 val_auc=0.8432\n",
      "Epoch 2/30  loss=0.4801 acc=0.7768  val_loss=0.4038 val_acc=0.8268 val_auc=0.9052\n",
      "Epoch 3/30  loss=0.4193 acc=0.8090  val_loss=0.3622 val_acc=0.8483 val_auc=0.9279\n",
      "Epoch 4/30  loss=0.3971 acc=0.8269  val_loss=0.3724 val_acc=0.8474 val_auc=0.9173\n",
      "Epoch 5/30  loss=0.3700 acc=0.8393  val_loss=0.2686 val_acc=0.8979 val_auc=0.9564\n",
      "Epoch 6/30  loss=0.3478 acc=0.8476  val_loss=0.3555 val_acc=0.8511 val_auc=0.9272\n",
      "Epoch 7/30  loss=0.3293 acc=0.8625  val_loss=0.3028 val_acc=0.8783 val_auc=0.9554\n",
      "Epoch 8/30  loss=0.3258 acc=0.8638  val_loss=0.3167 val_acc=0.8792 val_auc=0.9585\n",
      "Epoch 9/30  loss=0.3077 acc=0.8768  val_loss=0.2410 val_acc=0.8970 val_auc=0.9663\n",
      "Epoch 10/30  loss=0.2969 acc=0.8745  val_loss=0.2527 val_acc=0.9045 val_auc=0.9618\n",
      "Epoch 11/30  loss=0.2852 acc=0.8846  val_loss=0.2495 val_acc=0.9026 val_auc=0.9648\n",
      "Epoch 12/30  loss=0.2821 acc=0.8835  val_loss=0.2291 val_acc=0.9064 val_auc=0.9684\n",
      "Epoch 13/30  loss=0.2763 acc=0.8875  val_loss=0.3168 val_acc=0.8727 val_auc=0.9501\n",
      "Epoch 14/30  loss=0.2850 acc=0.8840  val_loss=0.2150 val_acc=0.9176 val_auc=0.9730\n",
      "Epoch 15/30  loss=0.2637 acc=0.8949  val_loss=0.2399 val_acc=0.9082 val_auc=0.9733\n",
      "Epoch 16/30  loss=0.2581 acc=0.8960  val_loss=0.2229 val_acc=0.9195 val_auc=0.9753\n",
      "Epoch 17/30  loss=0.2558 acc=0.8943  val_loss=0.2285 val_acc=0.9139 val_auc=0.9785\n",
      "Epoch 18/30  loss=0.2498 acc=0.8996  val_loss=0.2441 val_acc=0.9092 val_auc=0.9714\n",
      "Epoch 19/30  loss=0.2481 acc=0.9004  val_loss=0.1847 val_acc=0.9251 val_auc=0.9799\n",
      "Epoch 20/30  loss=0.2394 acc=0.9064  val_loss=0.2074 val_acc=0.9176 val_auc=0.9769\n",
      "Epoch 21/30  loss=0.2345 acc=0.9051  val_loss=0.2060 val_acc=0.9270 val_auc=0.9770\n",
      "Epoch 22/30  loss=0.2227 acc=0.9127  val_loss=0.1507 val_acc=0.9326 val_auc=0.9860\n",
      "Epoch 23/30  loss=0.2332 acc=0.9052  val_loss=0.1999 val_acc=0.9298 val_auc=0.9781\n",
      "Epoch 24/30  loss=0.2274 acc=0.9099  val_loss=0.2096 val_acc=0.9185 val_auc=0.9838\n",
      "Epoch 25/30  loss=0.2251 acc=0.9114  val_loss=0.1545 val_acc=0.9410 val_auc=0.9879\n",
      "Epoch 26/30  loss=0.2134 acc=0.9143  val_loss=0.1554 val_acc=0.9335 val_auc=0.9855\n",
      "Epoch 27/30  loss=0.2255 acc=0.9088  val_loss=0.1679 val_acc=0.9373 val_auc=0.9883\n",
      "Epoch 28/30  loss=0.2070 acc=0.9211  val_loss=0.1871 val_acc=0.9279 val_auc=0.9862\n",
      "Epoch 29/30  loss=0.2077 acc=0.9150  val_loss=0.1793 val_acc=0.9298 val_auc=0.9885\n",
      "Epoch 30/30  loss=0.2055 acc=0.9194  val_loss=0.1905 val_acc=0.9242 val_auc=0.9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 22:41:05,869] Trial 9 finished with value: 0.9410112359550562 and parameters: {'learning_rate': 0.00013783578294796384, 'reg_strength': 1.4045842344024705e-06, 'dropout_conv': 0.28286930019396905, 'dropout_dense': 0.4513395116144307, 'dense_units': 512, 'filters_multiplier': 1.2341791404163445, 'batch_size': 16, 'beta_1': 0.7689148877577958, 'beta_2': 0.9727488132263248}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9: Accuracy = 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.6342 acc=0.6492  val_loss=0.5898 val_acc=0.7388 val_auc=0.7993\n",
      "Epoch 2/30  loss=0.5706 acc=0.7082  val_loss=0.6700 val_acc=0.6423 val_auc=0.8553\n",
      "Epoch 3/30  loss=0.5257 acc=0.7454  val_loss=0.5790 val_acc=0.7041 val_auc=0.8648\n",
      "Epoch 4/30  loss=0.5145 acc=0.7549  val_loss=0.5984 val_acc=0.7378 val_auc=0.8436\n",
      "Epoch 5/30  loss=0.5043 acc=0.7681  val_loss=0.4754 val_acc=0.7846 val_auc=0.8578\n",
      "Epoch 6/30  loss=0.4776 acc=0.7749  val_loss=0.6915 val_acc=0.6536 val_auc=0.8783\n",
      "Epoch 7/30  loss=0.4694 acc=0.7824  val_loss=0.3982 val_acc=0.8455 val_auc=0.9098\n",
      "Epoch 8/30  loss=0.4551 acc=0.7913  val_loss=0.6621 val_acc=0.7313 val_auc=0.8893\n",
      "Epoch 9/30  loss=0.4460 acc=0.7993  val_loss=0.5538 val_acc=0.7556 val_auc=0.8956\n",
      "Epoch 10/30  loss=0.4328 acc=0.8116  val_loss=0.4386 val_acc=0.8090 val_auc=0.9061\n",
      "Epoch 11/30  loss=0.4313 acc=0.8041  val_loss=0.4997 val_acc=0.7650 val_auc=0.9129\n",
      "Epoch 12/30  loss=0.4196 acc=0.8150  val_loss=0.4473 val_acc=0.7818 val_auc=0.9263\n",
      "Epoch 13/30  loss=0.4132 acc=0.8194  val_loss=0.4743 val_acc=0.8268 val_auc=0.9117\n",
      "Epoch 14/30  loss=0.3711 acc=0.8400  val_loss=0.3911 val_acc=0.8361 val_auc=0.9420\n",
      "Epoch 15/30  loss=0.3534 acc=0.8529  val_loss=0.4817 val_acc=0.7903 val_auc=0.9425\n",
      "Epoch 16/30  loss=0.3486 acc=0.8540  val_loss=0.3286 val_acc=0.8670 val_auc=0.9511\n",
      "Epoch 17/30  loss=0.3506 acc=0.8488  val_loss=0.5300 val_acc=0.7566 val_auc=0.9450\n",
      "Epoch 18/30  loss=0.3421 acc=0.8562  val_loss=0.3534 val_acc=0.8577 val_auc=0.9504\n",
      "Epoch 19/30  loss=0.3316 acc=0.8579  val_loss=0.5875 val_acc=0.7509 val_auc=0.9492\n",
      "Epoch 20/30  loss=0.3322 acc=0.8614  val_loss=0.3344 val_acc=0.8596 val_auc=0.9580\n",
      "Epoch 21/30  loss=0.3329 acc=0.8581  val_loss=0.4188 val_acc=0.8099 val_auc=0.9540\n",
      "Epoch 22/30  loss=0.3199 acc=0.8663  val_loss=0.3141 val_acc=0.8642 val_auc=0.9573\n",
      "Epoch 23/30  loss=0.3148 acc=0.8723  val_loss=0.3133 val_acc=0.8652 val_auc=0.9615\n",
      "Epoch 24/30  loss=0.3131 acc=0.8689  val_loss=0.3347 val_acc=0.8689 val_auc=0.9601\n",
      "Epoch 25/30  loss=0.3054 acc=0.8727  val_loss=0.3331 val_acc=0.8642 val_auc=0.9582\n",
      "Epoch 26/30  loss=0.3132 acc=0.8728  val_loss=0.3541 val_acc=0.8483 val_auc=0.9603\n",
      "Epoch 27/30  loss=0.3105 acc=0.8708  val_loss=0.2736 val_acc=0.8933 val_auc=0.9623\n",
      "Epoch 28/30  loss=0.2981 acc=0.8746  val_loss=0.3799 val_acc=0.8436 val_auc=0.9603\n",
      "Epoch 29/30  loss=0.3002 acc=0.8780  val_loss=0.4436 val_acc=0.8109 val_auc=0.9634\n",
      "Epoch 30/30  loss=0.2953 acc=0.8784  val_loss=0.3532 val_acc=0.8614 val_auc=0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 22:46:59,736] Trial 10 finished with value: 0.8932584269662921 and parameters: {'learning_rate': 0.009908765478430503, 'reg_strength': 2.8504320627871546e-05, 'dropout_conv': 0.18102285590352837, 'dropout_dense': 0.34456342667644324, 'dense_units': 128, 'filters_multiplier': 0.5580877999417748, 'batch_size': 16, 'beta_1': 0.8809842228799859, 'beta_2': 0.9036756716752411}. Best is trial 1 with value: 0.9456928838951311.\n",
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10: Accuracy = 0.8933\n",
      "Epoch 1/30  loss=0.5492 acc=0.7306  val_loss=0.4889 val_acc=0.7687 val_auc=0.8635\n",
      "Epoch 2/30  loss=0.4804 acc=0.7784  val_loss=0.5212 val_acc=0.8034 val_auc=0.8927\n",
      "Epoch 3/30  loss=0.4224 acc=0.8110  val_loss=0.5787 val_acc=0.7715 val_auc=0.8889\n",
      "Epoch 4/30  loss=0.3753 acc=0.8380  val_loss=0.4210 val_acc=0.8324 val_auc=0.9004\n",
      "Epoch 5/30  loss=0.3622 acc=0.8443  val_loss=0.3639 val_acc=0.8558 val_auc=0.9337\n",
      "Epoch 6/30  loss=0.3410 acc=0.8586  val_loss=0.3420 val_acc=0.8652 val_auc=0.9322\n",
      "Epoch 7/30  loss=0.3146 acc=0.8674  val_loss=0.5974 val_acc=0.7481 val_auc=0.9089\n",
      "Epoch 8/30  loss=0.3021 acc=0.8780  val_loss=0.6489 val_acc=0.7341 val_auc=0.9455\n",
      "Epoch 9/30  loss=0.2970 acc=0.8804  val_loss=0.4936 val_acc=0.7884 val_auc=0.9456\n",
      "Epoch 10/30  loss=0.2800 acc=0.8883  val_loss=0.3765 val_acc=0.8399 val_auc=0.9435\n",
      "Epoch 11/30  loss=0.2623 acc=0.8982  val_loss=0.2580 val_acc=0.8951 val_auc=0.9695\n",
      "Epoch 12/30  loss=0.2576 acc=0.9002  val_loss=0.7755 val_acc=0.7257 val_auc=0.9577\n",
      "Epoch 13/30  loss=0.2563 acc=0.8981  val_loss=0.6123 val_acc=0.7640 val_auc=0.9576\n",
      "Epoch 14/30  loss=0.2409 acc=0.9020  val_loss=0.5958 val_acc=0.7603 val_auc=0.9701\n",
      "Epoch 15/30  loss=0.2395 acc=0.9029  val_loss=0.5088 val_acc=0.8296 val_auc=0.9534\n",
      "Epoch 16/30  loss=0.2364 acc=0.9091  val_loss=0.5824 val_acc=0.8240 val_auc=0.9443\n",
      "Epoch 17/30  loss=0.2226 acc=0.9100  val_loss=0.4167 val_acc=0.8418 val_auc=0.9646\n",
      "Epoch 18/30  loss=0.1916 acc=0.9270  val_loss=0.2571 val_acc=0.9036 val_auc=0.9824\n",
      "Epoch 19/30  loss=0.1741 acc=0.9321  val_loss=0.1636 val_acc=0.9354 val_auc=0.9861\n",
      "Epoch 20/30  loss=0.1697 acc=0.9345  val_loss=0.2106 val_acc=0.9185 val_auc=0.9852\n",
      "Epoch 21/30  loss=0.1670 acc=0.9359  val_loss=0.2016 val_acc=0.9223 val_auc=0.9865\n",
      "Epoch 22/30  loss=0.1622 acc=0.9374  val_loss=0.2333 val_acc=0.9082 val_auc=0.9871\n",
      "Epoch 23/30  loss=0.1621 acc=0.9355  val_loss=0.2572 val_acc=0.9036 val_auc=0.9858\n",
      "Epoch 24/30  loss=0.1598 acc=0.9388  val_loss=0.2161 val_acc=0.9148 val_auc=0.9870\n",
      "Epoch 25/30  loss=0.1555 acc=0.9388  val_loss=0.2873 val_acc=0.8961 val_auc=0.9872\n",
      "Epoch 26/30  loss=0.1461 acc=0.9444  val_loss=0.1905 val_acc=0.9288 val_auc=0.9880\n",
      "Epoch 27/30  loss=0.1506 acc=0.9411  val_loss=0.2009 val_acc=0.9167 val_auc=0.9892\n",
      "Epoch 28/30  loss=0.1386 acc=0.9462  val_loss=0.1795 val_acc=0.9335 val_auc=0.9882\n",
      "Epoch 29/30  loss=0.1434 acc=0.9446  val_loss=0.1692 val_acc=0.9410 val_auc=0.9891\n",
      "Epoch 30/30  loss=0.1358 acc=0.9497  val_loss=0.1768 val_acc=0.9335 val_auc=0.9893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 22:52:40,723] Trial 11 finished with value: 0.9410112359550562 and parameters: {'learning_rate': 0.001610128003061735, 'reg_strength': 1.3275283883341516e-05, 'dropout_conv': 0.10455080337440882, 'dropout_dense': 0.21947082195120038, 'dense_units': 2048, 'filters_multiplier': 0.8934070682200308, 'batch_size': 128, 'beta_1': 0.8613371767693089, 'beta_2': 0.9007399284997474}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11: Accuracy = 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5340 acc=0.7341  val_loss=0.5417 val_acc=0.7463 val_auc=0.8622\n",
      "Epoch 2/30  loss=0.4459 acc=0.7996  val_loss=0.3658 val_acc=0.8343 val_auc=0.9184\n",
      "Epoch 3/30  loss=0.4146 acc=0.8125  val_loss=0.4013 val_acc=0.8380 val_auc=0.9276\n",
      "Epoch 4/30  loss=0.3761 acc=0.8382  val_loss=0.4519 val_acc=0.8090 val_auc=0.9366\n",
      "Epoch 5/30  loss=0.3501 acc=0.8490  val_loss=0.3873 val_acc=0.8493 val_auc=0.9465\n",
      "Epoch 6/30  loss=0.3284 acc=0.8651  val_loss=0.3115 val_acc=0.8633 val_auc=0.9419\n",
      "Epoch 7/30  loss=0.3093 acc=0.8697  val_loss=0.3437 val_acc=0.8596 val_auc=0.9492\n",
      "Epoch 8/30  loss=0.3010 acc=0.8739  val_loss=0.3015 val_acc=0.8717 val_auc=0.9576\n",
      "Epoch 9/30  loss=0.2866 acc=0.8811  val_loss=0.2423 val_acc=0.9017 val_auc=0.9670\n",
      "Epoch 10/30  loss=0.2771 acc=0.8844  val_loss=0.2800 val_acc=0.8895 val_auc=0.9649\n",
      "Epoch 11/30  loss=0.2694 acc=0.8906  val_loss=0.7779 val_acc=0.7135 val_auc=0.9358\n",
      "Epoch 12/30  loss=0.2585 acc=0.8954  val_loss=0.3047 val_acc=0.8848 val_auc=0.9670\n",
      "Epoch 13/30  loss=0.2518 acc=0.8986  val_loss=0.2272 val_acc=0.9073 val_auc=0.9748\n",
      "Epoch 14/30  loss=0.2425 acc=0.9048  val_loss=0.3092 val_acc=0.8755 val_auc=0.9492\n",
      "Epoch 15/30  loss=0.2405 acc=0.9053  val_loss=0.4554 val_acc=0.8062 val_auc=0.9578\n",
      "Epoch 16/30  loss=0.2241 acc=0.9068  val_loss=0.2628 val_acc=0.8979 val_auc=0.9657\n",
      "Epoch 17/30  loss=0.2157 acc=0.9108  val_loss=0.2700 val_acc=0.8970 val_auc=0.9691\n",
      "Epoch 18/30  loss=0.2171 acc=0.9155  val_loss=0.3401 val_acc=0.8783 val_auc=0.9594\n",
      "Epoch 19/30  loss=0.2138 acc=0.9136  val_loss=0.2243 val_acc=0.9110 val_auc=0.9694\n",
      "Epoch 20/30  loss=0.2088 acc=0.9171  val_loss=0.1857 val_acc=0.9354 val_auc=0.9814\n",
      "Epoch 21/30  loss=0.2053 acc=0.9153  val_loss=0.2477 val_acc=0.9017 val_auc=0.9757\n",
      "Epoch 22/30  loss=0.1904 acc=0.9266  val_loss=0.1971 val_acc=0.9148 val_auc=0.9847\n",
      "Epoch 23/30  loss=0.1966 acc=0.9247  val_loss=0.2223 val_acc=0.9064 val_auc=0.9721\n",
      "Epoch 24/30  loss=0.1829 acc=0.9267  val_loss=0.3346 val_acc=0.8970 val_auc=0.9738\n",
      "Epoch 25/30  loss=0.1780 acc=0.9301  val_loss=0.2074 val_acc=0.9270 val_auc=0.9784\n",
      "Epoch 26/30  loss=0.1802 acc=0.9312  val_loss=0.1552 val_acc=0.9410 val_auc=0.9859\n",
      "Epoch 27/30  loss=0.1881 acc=0.9252  val_loss=0.2471 val_acc=0.9026 val_auc=0.9776\n",
      "Epoch 28/30  loss=0.1744 acc=0.9318  val_loss=0.2347 val_acc=0.9242 val_auc=0.9857\n",
      "Epoch 29/30  loss=0.1725 acc=0.9342  val_loss=0.2109 val_acc=0.9242 val_auc=0.9746\n",
      "Epoch 30/30  loss=0.1713 acc=0.9327  val_loss=0.2474 val_acc=0.9139 val_auc=0.9809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 22:58:13,375] Trial 12 finished with value: 0.9410112359550562 and parameters: {'learning_rate': 0.00040234933894050715, 'reg_strength': 0.0006846087133163288, 'dropout_conv': 0.1473286551866862, 'dropout_dense': 0.36088358880013244, 'dense_units': 2048, 'filters_multiplier': 0.945303234213279, 'batch_size': 64, 'beta_1': 0.7097399066309968, 'beta_2': 0.927754614093732}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12: Accuracy = 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5959 acc=0.6820  val_loss=0.6663 val_acc=0.6882 val_auc=0.7863\n",
      "Epoch 2/30  loss=0.5366 acc=0.7340  val_loss=0.8732 val_acc=0.5159 val_auc=0.8247\n",
      "Epoch 3/30  loss=0.4965 acc=0.7608  val_loss=0.8926 val_acc=0.6301 val_auc=0.8215\n",
      "Epoch 4/30  loss=0.4575 acc=0.7910  val_loss=0.4214 val_acc=0.8109 val_auc=0.8948\n",
      "Epoch 5/30  loss=0.4413 acc=0.7956  val_loss=0.4795 val_acc=0.7818 val_auc=0.9021\n",
      "Epoch 6/30  loss=0.4165 acc=0.8113  val_loss=0.5212 val_acc=0.7968 val_auc=0.8947\n",
      "Epoch 7/30  loss=0.4013 acc=0.8236  val_loss=0.6790 val_acc=0.6536 val_auc=0.8893\n",
      "Epoch 8/30  loss=0.3893 acc=0.8290  val_loss=0.6246 val_acc=0.7097 val_auc=0.9337\n",
      "Epoch 9/30  loss=0.3697 acc=0.8378  val_loss=0.3563 val_acc=0.8446 val_auc=0.9428\n",
      "Epoch 10/30  loss=0.3690 acc=0.8365  val_loss=0.3315 val_acc=0.8680 val_auc=0.9422\n",
      "Epoch 11/30  loss=0.3555 acc=0.8459  val_loss=0.3696 val_acc=0.8390 val_auc=0.9243\n",
      "Epoch 12/30  loss=0.3374 acc=0.8564  val_loss=0.3836 val_acc=0.8361 val_auc=0.9330\n",
      "Epoch 13/30  loss=0.3365 acc=0.8540  val_loss=0.4588 val_acc=0.7978 val_auc=0.9431\n",
      "Epoch 14/30  loss=0.3367 acc=0.8536  val_loss=0.3605 val_acc=0.8446 val_auc=0.9457\n",
      "Epoch 15/30  loss=0.3271 acc=0.8605  val_loss=0.4458 val_acc=0.7753 val_auc=0.9482\n",
      "Epoch 16/30  loss=0.3193 acc=0.8665  val_loss=0.2818 val_acc=0.8914 val_auc=0.9524\n",
      "Epoch 17/30  loss=0.3160 acc=0.8717  val_loss=0.7016 val_acc=0.6788 val_auc=0.9377\n",
      "Epoch 18/30  loss=0.3143 acc=0.8673  val_loss=0.7110 val_acc=0.7378 val_auc=0.9386\n",
      "Epoch 19/30  loss=0.3042 acc=0.8780  val_loss=0.5059 val_acc=0.7940 val_auc=0.9554\n",
      "Epoch 20/30  loss=0.2983 acc=0.8784  val_loss=0.6326 val_acc=0.7528 val_auc=0.9486\n",
      "Epoch 21/30  loss=0.3034 acc=0.8755  val_loss=0.3728 val_acc=0.8324 val_auc=0.9643\n",
      "Epoch 22/30  loss=0.2945 acc=0.8794  val_loss=0.5437 val_acc=0.7659 val_auc=0.9490\n",
      "Epoch 23/30  loss=0.2461 acc=0.8998  val_loss=0.4985 val_acc=0.7809 val_auc=0.9698\n",
      "Epoch 24/30  loss=0.2453 acc=0.9044  val_loss=0.2552 val_acc=0.9082 val_auc=0.9735\n",
      "Epoch 25/30  loss=0.2356 acc=0.9026  val_loss=0.3945 val_acc=0.8408 val_auc=0.9748\n",
      "Epoch 26/30  loss=0.2306 acc=0.9080  val_loss=0.2976 val_acc=0.8801 val_auc=0.9735\n",
      "Epoch 27/30  loss=0.2281 acc=0.9081  val_loss=0.3263 val_acc=0.8670 val_auc=0.9744\n",
      "Epoch 28/30  loss=0.2147 acc=0.9135  val_loss=0.3961 val_acc=0.8446 val_auc=0.9717\n",
      "Epoch 29/30  loss=0.2245 acc=0.9116  val_loss=0.3645 val_acc=0.8577 val_auc=0.9767\n",
      "Epoch 30/30  loss=0.2224 acc=0.9101  val_loss=0.4380 val_acc=0.8174 val_auc=0.9756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 23:03:51,831] Trial 13 finished with value: 0.9082397003745318 and parameters: {'learning_rate': 0.004959713565737127, 'reg_strength': 4.181181533062139e-05, 'dropout_conv': 0.10203961343333867, 'dropout_dense': 0.28702405058986225, 'dense_units': 128, 'filters_multiplier': 1.4893823304187175, 'batch_size': 32, 'beta_1': 0.8365295469976816, 'beta_2': 0.9460943434647401}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13: Accuracy = 0.9082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5392 acc=0.7317  val_loss=0.6341 val_acc=0.7041 val_auc=0.7617\n",
      "Epoch 2/30  loss=0.4728 acc=0.7756  val_loss=0.5984 val_acc=0.7313 val_auc=0.7978\n",
      "Epoch 3/30  loss=0.4500 acc=0.7887  val_loss=0.6686 val_acc=0.7406 val_auc=0.7968\n",
      "Epoch 4/30  loss=0.4203 acc=0.8138  val_loss=0.7942 val_acc=0.7154 val_auc=0.7900\n",
      "Epoch 5/30  loss=0.4074 acc=0.8150  val_loss=0.4485 val_acc=0.8155 val_auc=0.8914\n",
      "Epoch 6/30  loss=0.3853 acc=0.8313  val_loss=0.5313 val_acc=0.7978 val_auc=0.8758\n",
      "Epoch 7/30  loss=0.3726 acc=0.8339  val_loss=0.5122 val_acc=0.8052 val_auc=0.8680\n",
      "Epoch 8/30  loss=0.3501 acc=0.8522  val_loss=0.5951 val_acc=0.7828 val_auc=0.8481\n",
      "Epoch 9/30  loss=0.3416 acc=0.8545  val_loss=0.4271 val_acc=0.8343 val_auc=0.9139\n",
      "Epoch 10/30  loss=0.3305 acc=0.8597  val_loss=0.5772 val_acc=0.7846 val_auc=0.8928\n",
      "Epoch 11/30  loss=0.3227 acc=0.8618  val_loss=0.4078 val_acc=0.8483 val_auc=0.9165\n",
      "Epoch 12/30  loss=0.3095 acc=0.8741  val_loss=0.3810 val_acc=0.8642 val_auc=0.9276\n",
      "Epoch 13/30  loss=0.3031 acc=0.8762  val_loss=0.4591 val_acc=0.8230 val_auc=0.9022\n",
      "Epoch 14/30  loss=0.2916 acc=0.8775  val_loss=0.3322 val_acc=0.8820 val_auc=0.9430\n",
      "Epoch 15/30  loss=0.2961 acc=0.8797  val_loss=0.4252 val_acc=0.8390 val_auc=0.9187\n",
      "Epoch 16/30  loss=0.2840 acc=0.8861  val_loss=0.5757 val_acc=0.7875 val_auc=0.9017\n",
      "Epoch 17/30  loss=0.2779 acc=0.8868  val_loss=0.4638 val_acc=0.8530 val_auc=0.9170\n",
      "Epoch 18/30  loss=0.2673 acc=0.8890  val_loss=0.3718 val_acc=0.8577 val_auc=0.9372\n",
      "Epoch 19/30  loss=0.2597 acc=0.8926  val_loss=0.3746 val_acc=0.8699 val_auc=0.9443\n",
      "Epoch 20/30  loss=0.2625 acc=0.8950  val_loss=0.4761 val_acc=0.8361 val_auc=0.9244\n",
      "Epoch 21/30  loss=0.2387 acc=0.9010  val_loss=0.4016 val_acc=0.8539 val_auc=0.9408\n",
      "Epoch 22/30  loss=0.2345 acc=0.9065  val_loss=0.3974 val_acc=0.8567 val_auc=0.9390\n",
      "Epoch 23/30  loss=0.2309 acc=0.9080  val_loss=0.4489 val_acc=0.8455 val_auc=0.9337\n",
      "Epoch 24/30  loss=0.2337 acc=0.9059  val_loss=0.3964 val_acc=0.8642 val_auc=0.9391\n",
      "Epoch 25/30  loss=0.2251 acc=0.9114  val_loss=0.5077 val_acc=0.8371 val_auc=0.9255\n",
      "Epoch 26/30  loss=0.2223 acc=0.9116  val_loss=0.4300 val_acc=0.8521 val_auc=0.9343\n",
      "Epoch 27/30  loss=0.2223 acc=0.9098  val_loss=0.4293 val_acc=0.8586 val_auc=0.9367\n",
      "Epoch 28/30  loss=0.2143 acc=0.9148  val_loss=0.4241 val_acc=0.8605 val_auc=0.9363\n",
      "Early stopping at epoch 29; best val_acc=0.8820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 23:09:21,862] Trial 14 finished with value: 0.8820224719101124 and parameters: {'learning_rate': 4.266587910190725e-05, 'reg_strength': 6.249204019970293e-06, 'dropout_conv': 0.2499489620298569, 'dropout_dense': 0.40801573345869896, 'dense_units': 1024, 'filters_multiplier': 1.954149249668864, 'batch_size': 64, 'beta_1': 0.89985518866561, 'beta_2': 0.9176287173402677}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14: Accuracy = 0.8820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5879 acc=0.6895  val_loss=0.5252 val_acc=0.7659 val_auc=0.8289\n",
      "Epoch 2/30  loss=0.5346 acc=0.7372  val_loss=0.7023 val_acc=0.7182 val_auc=0.7906\n",
      "Epoch 3/30  loss=0.5192 acc=0.7540  val_loss=0.5081 val_acc=0.7509 val_auc=0.8564\n",
      "Epoch 4/30  loss=0.4953 acc=0.7637  val_loss=0.5609 val_acc=0.7893 val_auc=0.8544\n",
      "Epoch 5/30  loss=0.4930 acc=0.7657  val_loss=0.5502 val_acc=0.7444 val_auc=0.8266\n",
      "Epoch 6/30  loss=0.4819 acc=0.7713  val_loss=0.4269 val_acc=0.8034 val_auc=0.8861\n",
      "Epoch 7/30  loss=0.4516 acc=0.7910  val_loss=0.5015 val_acc=0.7743 val_auc=0.9024\n",
      "Epoch 8/30  loss=0.4323 acc=0.8048  val_loss=0.5084 val_acc=0.7697 val_auc=0.9014\n",
      "Epoch 9/30  loss=0.4173 acc=0.8130  val_loss=0.4894 val_acc=0.7706 val_auc=0.8986\n",
      "Epoch 10/30  loss=0.4221 acc=0.8130  val_loss=0.4691 val_acc=0.7828 val_auc=0.9053\n",
      "Epoch 11/30  loss=0.4184 acc=0.8103  val_loss=0.4689 val_acc=0.7790 val_auc=0.9108\n",
      "Epoch 12/30  loss=0.4085 acc=0.8174  val_loss=0.4336 val_acc=0.7940 val_auc=0.9289\n",
      "Epoch 13/30  loss=0.3709 acc=0.8399  val_loss=0.3805 val_acc=0.8390 val_auc=0.9325\n",
      "Epoch 14/30  loss=0.3673 acc=0.8401  val_loss=0.3609 val_acc=0.8549 val_auc=0.9382\n",
      "Epoch 15/30  loss=0.3508 acc=0.8468  val_loss=0.3500 val_acc=0.8530 val_auc=0.9402\n",
      "Epoch 16/30  loss=0.3447 acc=0.8539  val_loss=0.3238 val_acc=0.8755 val_auc=0.9474\n",
      "Epoch 17/30  loss=0.3428 acc=0.8539  val_loss=0.3816 val_acc=0.8408 val_auc=0.9421\n",
      "Epoch 18/30  loss=0.3446 acc=0.8546  val_loss=0.3237 val_acc=0.8717 val_auc=0.9440\n",
      "Epoch 19/30  loss=0.3412 acc=0.8542  val_loss=0.3222 val_acc=0.8699 val_auc=0.9439\n",
      "Epoch 20/30  loss=0.3348 acc=0.8600  val_loss=0.3091 val_acc=0.8783 val_auc=0.9484\n",
      "Epoch 21/30  loss=0.3235 acc=0.8634  val_loss=0.2940 val_acc=0.8830 val_auc=0.9509\n",
      "Epoch 22/30  loss=0.3053 acc=0.8720  val_loss=0.2813 val_acc=0.8839 val_auc=0.9529\n",
      "Epoch 23/30  loss=0.3203 acc=0.8659  val_loss=0.2853 val_acc=0.8848 val_auc=0.9527\n",
      "Epoch 24/30  loss=0.3184 acc=0.8649  val_loss=0.2979 val_acc=0.8867 val_auc=0.9497\n",
      "Epoch 25/30  loss=0.3162 acc=0.8663  val_loss=0.3087 val_acc=0.8755 val_auc=0.9524\n",
      "Epoch 26/30  loss=0.3168 acc=0.8684  val_loss=0.2902 val_acc=0.8764 val_auc=0.9536\n",
      "Epoch 27/30  loss=0.3056 acc=0.8729  val_loss=0.2635 val_acc=0.9007 val_auc=0.9608\n",
      "Epoch 28/30  loss=0.3051 acc=0.8773  val_loss=0.2567 val_acc=0.8989 val_auc=0.9594\n",
      "Epoch 29/30  loss=0.2993 acc=0.8748  val_loss=0.2545 val_acc=0.8942 val_auc=0.9605\n",
      "Epoch 30/30  loss=0.2986 acc=0.8759  val_loss=0.2404 val_acc=0.9054 val_auc=0.9649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 23:14:50,769] Trial 15 finished with value: 0.9054307116104869 and parameters: {'learning_rate': 0.004326277961525462, 'reg_strength': 6.338308393517934e-05, 'dropout_conv': 0.33839360923362666, 'dropout_dense': 0.2639121621413201, 'dense_units': 256, 'filters_multiplier': 0.6290012020129809, 'batch_size': 32, 'beta_1': 0.9887485483644981, 'beta_2': 0.9641719694083356}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15: Accuracy = 0.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5568 acc=0.7186  val_loss=0.5862 val_acc=0.6976 val_auc=0.8481\n",
      "Epoch 2/30  loss=0.4911 acc=0.7718  val_loss=0.4378 val_acc=0.8184 val_auc=0.8927\n",
      "Epoch 3/30  loss=0.4404 acc=0.8043  val_loss=0.4664 val_acc=0.8333 val_auc=0.8932\n",
      "Epoch 4/30  loss=0.4109 acc=0.8169  val_loss=0.3236 val_acc=0.8633 val_auc=0.9414\n",
      "Epoch 5/30  loss=0.3828 acc=0.8317  val_loss=0.5579 val_acc=0.7406 val_auc=0.9193\n",
      "Epoch 6/30  loss=0.3719 acc=0.8399  val_loss=0.3627 val_acc=0.8380 val_auc=0.9512\n",
      "Epoch 7/30  loss=0.3603 acc=0.8454  val_loss=0.3382 val_acc=0.8380 val_auc=0.9596\n",
      "Epoch 8/30  loss=0.3477 acc=0.8526  val_loss=0.4453 val_acc=0.8024 val_auc=0.9433\n",
      "Epoch 9/30  loss=0.3394 acc=0.8558  val_loss=0.4203 val_acc=0.8258 val_auc=0.9178\n",
      "Epoch 10/30  loss=0.3190 acc=0.8663  val_loss=0.3140 val_acc=0.8624 val_auc=0.9559\n",
      "Epoch 11/30  loss=0.2737 acc=0.8908  val_loss=0.2725 val_acc=0.8951 val_auc=0.9636\n",
      "Epoch 12/30  loss=0.2755 acc=0.8885  val_loss=0.2925 val_acc=0.8801 val_auc=0.9662\n",
      "Epoch 13/30  loss=0.2671 acc=0.8912  val_loss=0.2423 val_acc=0.9185 val_auc=0.9713\n",
      "Epoch 14/30  loss=0.2642 acc=0.8978  val_loss=0.2328 val_acc=0.9026 val_auc=0.9685\n",
      "Epoch 15/30  loss=0.2582 acc=0.8975  val_loss=0.2480 val_acc=0.9073 val_auc=0.9714\n",
      "Epoch 16/30  loss=0.2553 acc=0.8989  val_loss=0.2495 val_acc=0.8989 val_auc=0.9740\n",
      "Epoch 17/30  loss=0.2465 acc=0.8989  val_loss=0.2320 val_acc=0.9176 val_auc=0.9759\n",
      "Epoch 18/30  loss=0.2388 acc=0.9077  val_loss=0.2325 val_acc=0.9110 val_auc=0.9746\n",
      "Epoch 19/30  loss=0.2477 acc=0.9006  val_loss=0.1971 val_acc=0.9232 val_auc=0.9799\n",
      "Epoch 20/30  loss=0.2355 acc=0.9074  val_loss=0.2122 val_acc=0.9242 val_auc=0.9812\n",
      "Epoch 21/30  loss=0.2312 acc=0.9081  val_loss=0.2473 val_acc=0.9082 val_auc=0.9794\n",
      "Epoch 22/30  loss=0.2304 acc=0.9092  val_loss=0.2319 val_acc=0.9110 val_auc=0.9809\n",
      "Epoch 23/30  loss=0.2313 acc=0.9082  val_loss=0.2188 val_acc=0.9120 val_auc=0.9804\n",
      "Epoch 24/30  loss=0.2210 acc=0.9121  val_loss=0.1975 val_acc=0.9223 val_auc=0.9824\n",
      "Epoch 25/30  loss=0.2282 acc=0.9115  val_loss=0.1979 val_acc=0.9335 val_auc=0.9810\n",
      "Epoch 26/30  loss=0.2308 acc=0.9099  val_loss=0.1730 val_acc=0.9363 val_auc=0.9860\n",
      "Epoch 27/30  loss=0.2221 acc=0.9106  val_loss=0.2283 val_acc=0.9157 val_auc=0.9835\n",
      "Epoch 28/30  loss=0.2218 acc=0.9139  val_loss=0.2837 val_acc=0.8876 val_auc=0.9816\n",
      "Epoch 29/30  loss=0.2245 acc=0.9109  val_loss=0.1855 val_acc=0.9279 val_auc=0.9819\n",
      "Epoch 30/30  loss=0.2148 acc=0.9146  val_loss=0.1818 val_acc=0.9326 val_auc=0.9827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 23:20:34,295] Trial 16 finished with value: 0.9363295880149812 and parameters: {'learning_rate': 0.0006031202991728165, 'reg_strength': 0.000487712398100268, 'dropout_conv': 0.16490723352038325, 'dropout_dense': 0.20179270198537147, 'dense_units': 128, 'filters_multiplier': 1.0165846697969467, 'batch_size': 16, 'beta_1': 0.7078935176534471, 'beta_2': 0.9374189231163035}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16: Accuracy = 0.9363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5544 acc=0.7141  val_loss=0.5326 val_acc=0.7275 val_auc=0.8554\n",
      "Epoch 2/30  loss=0.4789 acc=0.7729  val_loss=0.8136 val_acc=0.6854 val_auc=0.7553\n",
      "Epoch 3/30  loss=0.4434 acc=0.7954  val_loss=0.7610 val_acc=0.7303 val_auc=0.8112\n",
      "Epoch 4/30  loss=0.4291 acc=0.8083  val_loss=0.6764 val_acc=0.7594 val_auc=0.8153\n",
      "Epoch 5/30  loss=0.4111 acc=0.8151  val_loss=0.7002 val_acc=0.7547 val_auc=0.8344\n",
      "Epoch 6/30  loss=0.3923 acc=0.8272  val_loss=0.6239 val_acc=0.7659 val_auc=0.8571\n",
      "Epoch 7/30  loss=0.3767 acc=0.8322  val_loss=0.5428 val_acc=0.7978 val_auc=0.8642\n",
      "Epoch 8/30  loss=0.3665 acc=0.8375  val_loss=0.5375 val_acc=0.7846 val_auc=0.8697\n",
      "Epoch 9/30  loss=0.3479 acc=0.8538  val_loss=0.4987 val_acc=0.7949 val_auc=0.9085\n",
      "Epoch 10/30  loss=0.3339 acc=0.8551  val_loss=0.5313 val_acc=0.8118 val_auc=0.8972\n",
      "Epoch 11/30  loss=0.3248 acc=0.8617  val_loss=0.4285 val_acc=0.8436 val_auc=0.9317\n",
      "Epoch 12/30  loss=0.3125 acc=0.8682  val_loss=0.3846 val_acc=0.8624 val_auc=0.9310\n",
      "Epoch 13/30  loss=0.3138 acc=0.8694  val_loss=0.3232 val_acc=0.8820 val_auc=0.9508\n",
      "Epoch 14/30  loss=0.2939 acc=0.8734  val_loss=0.3945 val_acc=0.8596 val_auc=0.9365\n",
      "Epoch 15/30  loss=0.2943 acc=0.8825  val_loss=0.4001 val_acc=0.8521 val_auc=0.9449\n",
      "Epoch 16/30  loss=0.2881 acc=0.8763  val_loss=0.3703 val_acc=0.8661 val_auc=0.9350\n",
      "Epoch 17/30  loss=0.2741 acc=0.8859  val_loss=0.3672 val_acc=0.8586 val_auc=0.9496\n",
      "Epoch 18/30  loss=0.2722 acc=0.8855  val_loss=0.3451 val_acc=0.8745 val_auc=0.9480\n",
      "Epoch 19/30  loss=0.2729 acc=0.8880  val_loss=0.3279 val_acc=0.8839 val_auc=0.9476\n",
      "Epoch 20/30  loss=0.2649 acc=0.8935  val_loss=0.3926 val_acc=0.8586 val_auc=0.9327\n",
      "Epoch 21/30  loss=0.2623 acc=0.8950  val_loss=0.3390 val_acc=0.8680 val_auc=0.9558\n",
      "Epoch 22/30  loss=0.2524 acc=0.8951  val_loss=0.2950 val_acc=0.8904 val_auc=0.9613\n",
      "Epoch 23/30  loss=0.2468 acc=0.8999  val_loss=0.3565 val_acc=0.8727 val_auc=0.9518\n",
      "Epoch 24/30  loss=0.2472 acc=0.9007  val_loss=0.3180 val_acc=0.8792 val_auc=0.9657\n",
      "Epoch 25/30  loss=0.2405 acc=0.8992  val_loss=0.3015 val_acc=0.8820 val_auc=0.9593\n",
      "Epoch 26/30  loss=0.2316 acc=0.9054  val_loss=0.4134 val_acc=0.8605 val_auc=0.9426\n",
      "Epoch 27/30  loss=0.2395 acc=0.9022  val_loss=0.2538 val_acc=0.9092 val_auc=0.9685\n",
      "Epoch 28/30  loss=0.2280 acc=0.9081  val_loss=0.2798 val_acc=0.8914 val_auc=0.9660\n",
      "Epoch 29/30  loss=0.2336 acc=0.9050  val_loss=0.4043 val_acc=0.8577 val_auc=0.9656\n",
      "Epoch 30/30  loss=0.2290 acc=0.9070  val_loss=0.2399 val_acc=0.9036 val_auc=0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 23:26:12,434] Trial 17 finished with value: 0.9091760299625468 and parameters: {'learning_rate': 7.130719327844118e-05, 'reg_strength': 0.00019454857038557935, 'dropout_conv': 0.21898940087793972, 'dropout_dense': 0.41655175202889805, 'dense_units': 2048, 'filters_multiplier': 1.4199416004961178, 'batch_size': 128, 'beta_1': 0.8233882923025466, 'beta_2': 0.9978586578140648}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17: Accuracy = 0.9092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5682 acc=0.7134  val_loss=0.5184 val_acc=0.7622 val_auc=0.8591\n",
      "Epoch 2/30  loss=0.5108 acc=0.7543  val_loss=0.4436 val_acc=0.8034 val_auc=0.8842\n",
      "Epoch 3/30  loss=0.4784 acc=0.7754  val_loss=0.4895 val_acc=0.7678 val_auc=0.8835\n",
      "Epoch 4/30  loss=0.4450 acc=0.7975  val_loss=0.3893 val_acc=0.8390 val_auc=0.9179\n",
      "Epoch 5/30  loss=0.4357 acc=0.8069  val_loss=0.4863 val_acc=0.7575 val_auc=0.9136\n",
      "Epoch 6/30  loss=0.4314 acc=0.8028  val_loss=0.4939 val_acc=0.7697 val_auc=0.9119\n",
      "Epoch 7/30  loss=0.4185 acc=0.8127  val_loss=0.4216 val_acc=0.8024 val_auc=0.9357\n",
      "Epoch 8/30  loss=0.3922 acc=0.8217  val_loss=0.3408 val_acc=0.8521 val_auc=0.9306\n",
      "Epoch 9/30  loss=0.3934 acc=0.8240  val_loss=0.3501 val_acc=0.8427 val_auc=0.9355\n",
      "Epoch 10/30  loss=0.3817 acc=0.8349  val_loss=0.3808 val_acc=0.8305 val_auc=0.9346\n",
      "Epoch 11/30  loss=0.3851 acc=0.8295  val_loss=0.3904 val_acc=0.8184 val_auc=0.9369\n",
      "Epoch 12/30  loss=0.3774 acc=0.8352  val_loss=0.3738 val_acc=0.8352 val_auc=0.9393\n",
      "Epoch 13/30  loss=0.3732 acc=0.8351  val_loss=0.5634 val_acc=0.7453 val_auc=0.9407\n",
      "Epoch 14/30  loss=0.3637 acc=0.8452  val_loss=0.6675 val_acc=0.6788 val_auc=0.9085\n",
      "Epoch 15/30  loss=0.3248 acc=0.8642  val_loss=0.3632 val_acc=0.8427 val_auc=0.9500\n",
      "Epoch 16/30  loss=0.3145 acc=0.8684  val_loss=0.3565 val_acc=0.8493 val_auc=0.9492\n",
      "Epoch 17/30  loss=0.3062 acc=0.8746  val_loss=0.3248 val_acc=0.8605 val_auc=0.9557\n",
      "Epoch 18/30  loss=0.3010 acc=0.8763  val_loss=0.3378 val_acc=0.8670 val_auc=0.9482\n",
      "Epoch 19/30  loss=0.2931 acc=0.8798  val_loss=0.3555 val_acc=0.8567 val_auc=0.9593\n",
      "Epoch 20/30  loss=0.2929 acc=0.8780  val_loss=0.2893 val_acc=0.8830 val_auc=0.9595\n",
      "Epoch 21/30  loss=0.2910 acc=0.8785  val_loss=0.3089 val_acc=0.8792 val_auc=0.9583\n",
      "Epoch 22/30  loss=0.2965 acc=0.8746  val_loss=0.3503 val_acc=0.8521 val_auc=0.9567\n",
      "Epoch 23/30  loss=0.2841 acc=0.8833  val_loss=0.3807 val_acc=0.8352 val_auc=0.9619\n",
      "Epoch 24/30  loss=0.2974 acc=0.8745  val_loss=0.2918 val_acc=0.8839 val_auc=0.9603\n",
      "Epoch 25/30  loss=0.2818 acc=0.8861  val_loss=0.2699 val_acc=0.8998 val_auc=0.9630\n",
      "Epoch 26/30  loss=0.2784 acc=0.8857  val_loss=0.2539 val_acc=0.9045 val_auc=0.9649\n",
      "Epoch 27/30  loss=0.2814 acc=0.8830  val_loss=0.2633 val_acc=0.9120 val_auc=0.9618\n",
      "Epoch 28/30  loss=0.2779 acc=0.8859  val_loss=0.2451 val_acc=0.9007 val_auc=0.9652\n",
      "Epoch 29/30  loss=0.2764 acc=0.8860  val_loss=0.2700 val_acc=0.8895 val_auc=0.9638\n",
      "Epoch 30/30  loss=0.2716 acc=0.8901  val_loss=0.2690 val_acc=0.8951 val_auc=0.9645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 23:31:41,091] Trial 18 finished with value: 0.9119850187265918 and parameters: {'learning_rate': 0.001901053696105049, 'reg_strength': 0.0015257066795346295, 'dropout_conv': 0.13097292977511915, 'dropout_dense': 0.49530993792314604, 'dense_units': 128, 'filters_multiplier': 0.7369269500900842, 'batch_size': 32, 'beta_1': 0.7634857187121283, 'beta_2': 0.9209942317984111}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18: Accuracy = 0.9120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5667 acc=0.7164  val_loss=0.5005 val_acc=0.7697 val_auc=0.8610\n",
      "Epoch 2/30  loss=0.4975 acc=0.7666  val_loss=0.5123 val_acc=0.7659 val_auc=0.8847\n",
      "Epoch 3/30  loss=0.4516 acc=0.7961  val_loss=0.9502 val_acc=0.6283 val_auc=0.8631\n",
      "Epoch 4/30  loss=0.4218 acc=0.8152  val_loss=0.4057 val_acc=0.8240 val_auc=0.9090\n",
      "Epoch 5/30  loss=0.3887 acc=0.8290  val_loss=0.6982 val_acc=0.7500 val_auc=0.8952\n",
      "Epoch 6/30  loss=0.3618 acc=0.8454  val_loss=0.6627 val_acc=0.7875 val_auc=0.9163\n",
      "Epoch 7/30  loss=0.3529 acc=0.8501  val_loss=0.4784 val_acc=0.8137 val_auc=0.9192\n",
      "Epoch 8/30  loss=0.3312 acc=0.8587  val_loss=0.5688 val_acc=0.7762 val_auc=0.9411\n",
      "Epoch 9/30  loss=0.3133 acc=0.8708  val_loss=0.7490 val_acc=0.7537 val_auc=0.9107\n",
      "Epoch 10/30  loss=0.2976 acc=0.8745  val_loss=1.0524 val_acc=0.7135 val_auc=0.8930\n",
      "Epoch 11/30  loss=0.2681 acc=0.8916  val_loss=0.4722 val_acc=0.8249 val_auc=0.9428\n",
      "Epoch 12/30  loss=0.2602 acc=0.8948  val_loss=0.3695 val_acc=0.8539 val_auc=0.9436\n",
      "Epoch 13/30  loss=0.2492 acc=0.8964  val_loss=0.3946 val_acc=0.8530 val_auc=0.9423\n",
      "Epoch 14/30  loss=0.2378 acc=0.9052  val_loss=0.4649 val_acc=0.8408 val_auc=0.9290\n",
      "Epoch 15/30  loss=0.2408 acc=0.9022  val_loss=0.3778 val_acc=0.8549 val_auc=0.9444\n",
      "Epoch 16/30  loss=0.2337 acc=0.9032  val_loss=0.3476 val_acc=0.8670 val_auc=0.9537\n",
      "Epoch 17/30  loss=0.2334 acc=0.9061  val_loss=0.4182 val_acc=0.8390 val_auc=0.9476\n",
      "Epoch 18/30  loss=0.2357 acc=0.9064  val_loss=0.4403 val_acc=0.8352 val_auc=0.9418\n",
      "Epoch 19/30  loss=0.2313 acc=0.9091  val_loss=0.4042 val_acc=0.8436 val_auc=0.9421\n",
      "Epoch 20/30  loss=0.2248 acc=0.9123  val_loss=0.3666 val_acc=0.8652 val_auc=0.9605\n",
      "Epoch 21/30  loss=0.2235 acc=0.9113  val_loss=0.3828 val_acc=0.8558 val_auc=0.9464\n",
      "Epoch 22/30  loss=0.2156 acc=0.9130  val_loss=0.3353 val_acc=0.8661 val_auc=0.9535\n",
      "Epoch 23/30  loss=0.2211 acc=0.9140  val_loss=0.3466 val_acc=0.8624 val_auc=0.9576\n",
      "Epoch 24/30  loss=0.2089 acc=0.9171  val_loss=0.3647 val_acc=0.8586 val_auc=0.9546\n",
      "Epoch 25/30  loss=0.2069 acc=0.9194  val_loss=0.3236 val_acc=0.8699 val_auc=0.9569\n",
      "Epoch 26/30  loss=0.2080 acc=0.9208  val_loss=0.3236 val_acc=0.8652 val_auc=0.9569\n",
      "Epoch 27/30  loss=0.2115 acc=0.9160  val_loss=0.3205 val_acc=0.8652 val_auc=0.9617\n",
      "Epoch 28/30  loss=0.2122 acc=0.9163  val_loss=0.3389 val_acc=0.8596 val_auc=0.9552\n",
      "Epoch 29/30  loss=0.2054 acc=0.9181  val_loss=0.3158 val_acc=0.8727 val_auc=0.9578\n",
      "Epoch 30/30  loss=0.2056 acc=0.9170  val_loss=0.3159 val_acc=0.8764 val_auc=0.9585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 23:37:13,265] Trial 19 finished with value: 0.8764044943820225 and parameters: {'learning_rate': 0.0002878676714325049, 'reg_strength': 2.322916255793605e-05, 'dropout_conv': 0.27364783967329287, 'dropout_dense': 0.6956385980717997, 'dense_units': 2048, 'filters_multiplier': 1.041861401721342, 'batch_size': 64, 'beta_1': 0.8725882870898124, 'beta_2': 0.908834577667222}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19: Accuracy = 0.8764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.6382 acc=0.6313  val_loss=0.5618 val_acc=0.7210 val_auc=0.7853\n",
      "Epoch 2/30  loss=nan acc=0.6484  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 3/30  loss=0.6794 acc=0.5585  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 4/30  loss=0.6988 acc=0.5176  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 5/30  loss=0.7004 acc=0.5066  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 6/30  loss=0.6985 acc=0.5071  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 7/30  loss=0.6980 acc=0.5128  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 8/30  loss=0.6870 acc=0.5448  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 9/30  loss=0.6626 acc=0.5815  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 10/30  loss=0.6128 acc=0.6578  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 11/30  loss=0.5881 acc=0.6941  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 12/30  loss=0.5844 acc=0.7006  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 13/30  loss=0.5598 acc=0.7168  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 14/30  loss=0.5243 acc=0.7458  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 15/30  loss=0.5165 acc=0.7508  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Early stopping at epoch 16; best val_acc=0.7210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 23:40:23,551] Trial 20 finished with value: 0.7209737827715356 and parameters: {'learning_rate': 0.014701090650799045, 'reg_strength': 1.2106594408061625e-06, 'dropout_conv': 0.33886883914825344, 'dropout_dense': 0.33443700766014484, 'dense_units': 256, 'filters_multiplier': 1.7004171881140318, 'batch_size': 32, 'beta_1': 0.7339823755905006, 'beta_2': 0.9564566193924184}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20: Accuracy = 0.7210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5576 acc=0.7165  val_loss=0.5119 val_acc=0.7481 val_auc=0.8426\n",
      "Epoch 2/30  loss=0.4896 acc=0.7706  val_loss=0.4758 val_acc=0.8155 val_auc=0.8896\n",
      "Epoch 3/30  loss=0.4421 acc=0.7994  val_loss=0.4941 val_acc=0.7949 val_auc=0.8987\n",
      "Epoch 4/30  loss=0.3987 acc=0.8208  val_loss=0.3605 val_acc=0.8558 val_auc=0.9308\n",
      "Epoch 5/30  loss=0.3773 acc=0.8359  val_loss=0.3376 val_acc=0.8764 val_auc=0.9420\n",
      "Epoch 6/30  loss=0.3551 acc=0.8461  val_loss=0.3092 val_acc=0.8801 val_auc=0.9496\n",
      "Epoch 7/30  loss=0.3424 acc=0.8514  val_loss=0.4232 val_acc=0.8483 val_auc=0.9316\n",
      "Epoch 8/30  loss=0.3290 acc=0.8579  val_loss=0.3067 val_acc=0.8933 val_auc=0.9529\n",
      "Epoch 9/30  loss=0.3107 acc=0.8716  val_loss=0.3646 val_acc=0.8745 val_auc=0.9340\n",
      "Epoch 10/30  loss=0.3044 acc=0.8737  val_loss=0.2236 val_acc=0.9157 val_auc=0.9755\n",
      "Epoch 11/30  loss=0.2874 acc=0.8826  val_loss=0.2494 val_acc=0.8989 val_auc=0.9706\n",
      "Epoch 12/30  loss=0.2807 acc=0.8819  val_loss=0.3142 val_acc=0.8914 val_auc=0.9620\n",
      "Epoch 13/30  loss=0.2804 acc=0.8873  val_loss=0.2544 val_acc=0.9017 val_auc=0.9644\n",
      "Epoch 14/30  loss=0.2768 acc=0.8900  val_loss=0.3336 val_acc=0.8727 val_auc=0.9524\n",
      "Epoch 15/30  loss=0.2633 acc=0.8954  val_loss=0.2339 val_acc=0.9129 val_auc=0.9727\n",
      "Epoch 16/30  loss=0.2676 acc=0.8921  val_loss=0.2064 val_acc=0.9279 val_auc=0.9786\n",
      "Epoch 17/30  loss=0.2562 acc=0.8961  val_loss=0.2065 val_acc=0.9270 val_auc=0.9777\n",
      "Epoch 18/30  loss=0.2606 acc=0.8951  val_loss=0.1791 val_acc=0.9242 val_auc=0.9802\n",
      "Epoch 19/30  loss=0.2496 acc=0.9007  val_loss=0.2176 val_acc=0.9251 val_auc=0.9796\n",
      "Epoch 20/30  loss=0.2348 acc=0.9045  val_loss=0.2001 val_acc=0.9242 val_auc=0.9786\n",
      "Epoch 21/30  loss=0.2335 acc=0.9082  val_loss=0.1861 val_acc=0.9298 val_auc=0.9812\n",
      "Epoch 22/30  loss=0.2317 acc=0.9067  val_loss=0.1957 val_acc=0.9270 val_auc=0.9773\n",
      "Epoch 23/30  loss=0.2379 acc=0.9029  val_loss=0.3456 val_acc=0.8689 val_auc=0.9766\n",
      "Epoch 24/30  loss=0.2250 acc=0.9094  val_loss=0.1645 val_acc=0.9373 val_auc=0.9836\n",
      "Epoch 25/30  loss=0.2193 acc=0.9136  val_loss=0.1708 val_acc=0.9354 val_auc=0.9815\n",
      "Epoch 26/30  loss=0.2052 acc=0.9184  val_loss=0.2944 val_acc=0.9026 val_auc=0.9803\n",
      "Epoch 27/30  loss=0.2140 acc=0.9162  val_loss=0.1684 val_acc=0.9382 val_auc=0.9831\n",
      "Epoch 28/30  loss=0.2105 acc=0.9216  val_loss=0.1910 val_acc=0.9260 val_auc=0.9848\n",
      "Epoch 29/30  loss=0.2054 acc=0.9195  val_loss=0.1638 val_acc=0.9419 val_auc=0.9859\n",
      "Epoch 30/30  loss=0.2084 acc=0.9168  val_loss=0.1944 val_acc=0.9213 val_auc=0.9847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 23:46:10,771] Trial 21 finished with value: 0.9419475655430711 and parameters: {'learning_rate': 0.00014864470268024252, 'reg_strength': 1.4839113238339772e-06, 'dropout_conv': 0.2603979742221508, 'dropout_dense': 0.4962341195773644, 'dense_units': 512, 'filters_multiplier': 1.3393197159146162, 'batch_size': 16, 'beta_1': 0.7835507784068512, 'beta_2': 0.9713641221028989}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21: Accuracy = 0.9419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5597 acc=0.7134  val_loss=0.6504 val_acc=0.6863 val_auc=0.7760\n",
      "Epoch 2/30  loss=0.5101 acc=0.7511  val_loss=0.4550 val_acc=0.7903 val_auc=0.8689\n",
      "Epoch 3/30  loss=0.4748 acc=0.7846  val_loss=0.4346 val_acc=0.8099 val_auc=0.8837\n",
      "Epoch 4/30  loss=0.4510 acc=0.7979  val_loss=0.4694 val_acc=0.7846 val_auc=0.8769\n",
      "Epoch 5/30  loss=0.4367 acc=0.8002  val_loss=0.4143 val_acc=0.8296 val_auc=0.9058\n",
      "Epoch 6/30  loss=0.4121 acc=0.8174  val_loss=0.4513 val_acc=0.8081 val_auc=0.8985\n",
      "Epoch 7/30  loss=0.3907 acc=0.8271  val_loss=0.4730 val_acc=0.8390 val_auc=0.8999\n",
      "Epoch 8/30  loss=0.3783 acc=0.8352  val_loss=0.4558 val_acc=0.8305 val_auc=0.9039\n",
      "Epoch 9/30  loss=0.3617 acc=0.8429  val_loss=0.5086 val_acc=0.8230 val_auc=0.9009\n",
      "Epoch 10/30  loss=0.3546 acc=0.8482  val_loss=0.3767 val_acc=0.8586 val_auc=0.9369\n",
      "Epoch 11/30  loss=0.3392 acc=0.8512  val_loss=0.5998 val_acc=0.7781 val_auc=0.8918\n",
      "Epoch 12/30  loss=0.3428 acc=0.8551  val_loss=0.4415 val_acc=0.8474 val_auc=0.9177\n",
      "Epoch 13/30  loss=0.3283 acc=0.8592  val_loss=0.4096 val_acc=0.8530 val_auc=0.9261\n",
      "Epoch 14/30  loss=0.3238 acc=0.8647  val_loss=0.3808 val_acc=0.8624 val_auc=0.9347\n",
      "Epoch 15/30  loss=0.3197 acc=0.8653  val_loss=0.5205 val_acc=0.8258 val_auc=0.9113\n",
      "Epoch 16/30  loss=0.3105 acc=0.8710  val_loss=0.3804 val_acc=0.8605 val_auc=0.9314\n",
      "Epoch 17/30  loss=0.3063 acc=0.8707  val_loss=0.3354 val_acc=0.8736 val_auc=0.9415\n",
      "Epoch 18/30  loss=0.3012 acc=0.8761  val_loss=0.3404 val_acc=0.8708 val_auc=0.9419\n",
      "Epoch 19/30  loss=0.2995 acc=0.8749  val_loss=0.3306 val_acc=0.8830 val_auc=0.9466\n",
      "Epoch 20/30  loss=0.2870 acc=0.8827  val_loss=0.3300 val_acc=0.8801 val_auc=0.9535\n",
      "Epoch 21/30  loss=0.2858 acc=0.8813  val_loss=0.3438 val_acc=0.8670 val_auc=0.9455\n",
      "Epoch 22/30  loss=0.2781 acc=0.8854  val_loss=0.3604 val_acc=0.8727 val_auc=0.9430\n",
      "Epoch 23/30  loss=0.2811 acc=0.8857  val_loss=0.3642 val_acc=0.8680 val_auc=0.9353\n",
      "Epoch 24/30  loss=0.2754 acc=0.8853  val_loss=0.3168 val_acc=0.8839 val_auc=0.9554\n",
      "Epoch 25/30  loss=0.2692 acc=0.8876  val_loss=0.3598 val_acc=0.8614 val_auc=0.9423\n",
      "Epoch 26/30  loss=0.2760 acc=0.8908  val_loss=0.2990 val_acc=0.8895 val_auc=0.9575\n",
      "Epoch 27/30  loss=0.2606 acc=0.8940  val_loss=0.3127 val_acc=0.8904 val_auc=0.9537\n",
      "Epoch 28/30  loss=0.2635 acc=0.8924  val_loss=0.4385 val_acc=0.8287 val_auc=0.9473\n",
      "Epoch 29/30  loss=0.2463 acc=0.9015  val_loss=0.2583 val_acc=0.9064 val_auc=0.9649\n",
      "Epoch 30/30  loss=0.2492 acc=0.9000  val_loss=0.2530 val_acc=0.9045 val_auc=0.9671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 23:51:57,885] Trial 22 finished with value: 0.9063670411985019 and parameters: {'learning_rate': 4.334873830542058e-05, 'reg_strength': 3.502312851092156e-06, 'dropout_conv': 0.23373786045181166, 'dropout_dense': 0.5031916969278516, 'dense_units': 512, 'filters_multiplier': 1.36869702404926, 'batch_size': 16, 'beta_1': 0.8063593271296743, 'beta_2': 0.9768942537390112}. Best is trial 1 with value: 0.9456928838951311.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22: Accuracy = 0.9064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5551 acc=0.7198  val_loss=0.4499 val_acc=0.7912 val_auc=0.8757\n",
      "Epoch 2/30  loss=0.4821 acc=0.7770  val_loss=0.4737 val_acc=0.7903 val_auc=0.8722\n",
      "Epoch 3/30  loss=0.4351 acc=0.8030  val_loss=0.4454 val_acc=0.8174 val_auc=0.9023\n",
      "Epoch 4/30  loss=0.4073 acc=0.8184  val_loss=0.3947 val_acc=0.8427 val_auc=0.9214\n",
      "Epoch 5/30  loss=0.3729 acc=0.8377  val_loss=0.4763 val_acc=0.8221 val_auc=0.9286\n",
      "Epoch 6/30  loss=0.3443 acc=0.8550  val_loss=0.3435 val_acc=0.8596 val_auc=0.9388\n",
      "Epoch 7/30  loss=0.3332 acc=0.8600  val_loss=0.7324 val_acc=0.7594 val_auc=0.9160\n",
      "Epoch 8/30  loss=0.3108 acc=0.8694  val_loss=0.4251 val_acc=0.8521 val_auc=0.9479\n",
      "Epoch 9/30  loss=0.3037 acc=0.8759  val_loss=0.2636 val_acc=0.8998 val_auc=0.9610\n",
      "Epoch 10/30  loss=0.2909 acc=0.8806  val_loss=0.2697 val_acc=0.8895 val_auc=0.9689\n",
      "Epoch 11/30  loss=0.2736 acc=0.8851  val_loss=0.2672 val_acc=0.8933 val_auc=0.9614\n",
      "Epoch 12/30  loss=0.2705 acc=0.8908  val_loss=0.2640 val_acc=0.9110 val_auc=0.9679\n",
      "Epoch 13/30  loss=0.2662 acc=0.8917  val_loss=0.2099 val_acc=0.9176 val_auc=0.9762\n",
      "Epoch 14/30  loss=0.2557 acc=0.8997  val_loss=0.2274 val_acc=0.9129 val_auc=0.9721\n",
      "Epoch 15/30  loss=0.2519 acc=0.8986  val_loss=0.3742 val_acc=0.8577 val_auc=0.9577\n",
      "Epoch 16/30  loss=0.2416 acc=0.9006  val_loss=0.2641 val_acc=0.8979 val_auc=0.9796\n",
      "Epoch 17/30  loss=0.2406 acc=0.9048  val_loss=0.1761 val_acc=0.9298 val_auc=0.9818\n",
      "Epoch 18/30  loss=0.2355 acc=0.9048  val_loss=0.1638 val_acc=0.9316 val_auc=0.9836\n",
      "Epoch 19/30  loss=0.2328 acc=0.9065  val_loss=0.2034 val_acc=0.9232 val_auc=0.9776\n",
      "Epoch 20/30  loss=0.2239 acc=0.9098  val_loss=0.2730 val_acc=0.8979 val_auc=0.9715\n",
      "Epoch 21/30  loss=0.2214 acc=0.9103  val_loss=0.1749 val_acc=0.9391 val_auc=0.9872\n",
      "Epoch 22/30  loss=0.2210 acc=0.9155  val_loss=0.2135 val_acc=0.9139 val_auc=0.9847\n",
      "Epoch 23/30  loss=0.2123 acc=0.9163  val_loss=0.2041 val_acc=0.9232 val_auc=0.9859\n",
      "Epoch 24/30  loss=0.2054 acc=0.9206  val_loss=0.1785 val_acc=0.9270 val_auc=0.9839\n",
      "Epoch 25/30  loss=0.1998 acc=0.9222  val_loss=0.1520 val_acc=0.9363 val_auc=0.9870\n",
      "Epoch 26/30  loss=0.1950 acc=0.9259  val_loss=0.1382 val_acc=0.9429 val_auc=0.9900\n",
      "Epoch 27/30  loss=0.1969 acc=0.9223  val_loss=0.1285 val_acc=0.9541 val_auc=0.9897\n",
      "Epoch 28/30  loss=0.1912 acc=0.9250  val_loss=0.1403 val_acc=0.9391 val_auc=0.9888\n",
      "Epoch 29/30  loss=0.1973 acc=0.9223  val_loss=0.1490 val_acc=0.9438 val_auc=0.9877\n",
      "Epoch 30/30  loss=0.1869 acc=0.9267  val_loss=0.2267 val_acc=0.9195 val_auc=0.9856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-20 23:57:48,218] Trial 23 finished with value: 0.954119850187266 and parameters: {'learning_rate': 0.0001337250142891458, 'reg_strength': 1.2664858341453963e-05, 'dropout_conv': 0.19745865489344588, 'dropout_dense': 0.3974076824101742, 'dense_units': 1024, 'filters_multiplier': 1.6947495329894964, 'batch_size': 16, 'beta_1': 0.7883706361510326, 'beta_2': 0.9633378979581148}. Best is trial 23 with value: 0.954119850187266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23: Accuracy = 0.9541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5881 acc=0.7041  val_loss=0.8456 val_acc=0.6732 val_auc=0.8188\n",
      "Epoch 2/30  loss=0.5263 acc=0.7433  val_loss=1.1233 val_acc=0.7032 val_auc=0.8676\n",
      "Epoch 3/30  loss=0.4570 acc=0.7918  val_loss=0.3987 val_acc=0.8464 val_auc=0.9210\n",
      "Epoch 4/30  loss=0.4230 acc=0.8146  val_loss=0.4427 val_acc=0.8015 val_auc=0.9129\n",
      "Epoch 5/30  loss=0.3959 acc=0.8232  val_loss=0.5637 val_acc=0.7856 val_auc=0.9364\n",
      "Epoch 6/30  loss=0.3712 acc=0.8426  val_loss=0.5789 val_acc=0.7818 val_auc=0.9320\n",
      "Epoch 7/30  loss=0.3653 acc=0.8442  val_loss=0.3189 val_acc=0.8717 val_auc=0.9414\n",
      "Epoch 8/30  loss=0.3472 acc=0.8500  val_loss=0.6547 val_acc=0.7322 val_auc=0.9197\n",
      "Epoch 9/30  loss=0.3321 acc=0.8590  val_loss=1.3697 val_acc=0.8109 val_auc=0.8567\n",
      "Epoch 10/30  loss=0.3325 acc=0.8593  val_loss=0.4509 val_acc=0.7968 val_auc=0.9511\n",
      "Epoch 11/30  loss=0.3308 acc=0.8613  val_loss=0.3182 val_acc=0.8624 val_auc=0.9516\n",
      "Epoch 12/30  loss=0.3194 acc=0.8665  val_loss=0.6397 val_acc=0.7247 val_auc=0.9450\n",
      "Epoch 13/30  loss=0.3027 acc=0.8703  val_loss=0.6962 val_acc=0.7678 val_auc=0.9521\n",
      "Epoch 14/30  loss=0.2561 acc=0.8976  val_loss=0.2922 val_acc=0.8820 val_auc=0.9731\n",
      "Epoch 15/30  loss=0.2368 acc=0.9052  val_loss=0.2989 val_acc=0.8942 val_auc=0.9755\n",
      "Epoch 16/30  loss=0.2311 acc=0.9072  val_loss=0.2669 val_acc=0.8942 val_auc=0.9761\n",
      "Epoch 17/30  loss=0.2300 acc=0.9116  val_loss=0.4020 val_acc=0.8577 val_auc=0.9728\n",
      "Epoch 18/30  loss=0.2300 acc=0.9096  val_loss=0.3308 val_acc=0.8736 val_auc=0.9765\n",
      "Epoch 19/30  loss=0.2288 acc=0.9073  val_loss=0.2966 val_acc=0.8839 val_auc=0.9744\n",
      "Epoch 20/30  loss=0.2232 acc=0.9099  val_loss=0.2743 val_acc=0.8970 val_auc=0.9790\n",
      "Epoch 21/30  loss=0.2221 acc=0.9136  val_loss=0.2461 val_acc=0.9082 val_auc=0.9805\n",
      "Epoch 22/30  loss=0.2223 acc=0.9101  val_loss=0.2508 val_acc=0.8951 val_auc=0.9760\n",
      "Epoch 23/30  loss=0.2129 acc=0.9132  val_loss=0.2360 val_acc=0.9026 val_auc=0.9807\n",
      "Epoch 24/30  loss=0.2151 acc=0.9139  val_loss=0.2338 val_acc=0.9092 val_auc=0.9828\n",
      "Epoch 25/30  loss=0.2201 acc=0.9125  val_loss=0.2882 val_acc=0.8904 val_auc=0.9789\n",
      "Epoch 26/30  loss=0.2117 acc=0.9192  val_loss=0.2871 val_acc=0.8792 val_auc=0.9813\n",
      "Epoch 27/30  loss=0.2067 acc=0.9187  val_loss=0.2613 val_acc=0.8933 val_auc=0.9766\n",
      "Epoch 28/30  loss=0.2023 acc=0.9223  val_loss=0.1968 val_acc=0.9307 val_auc=0.9848\n",
      "Epoch 29/30  loss=0.1988 acc=0.9206  val_loss=0.3311 val_acc=0.8736 val_auc=0.9769\n",
      "Epoch 30/30  loss=0.2084 acc=0.9170  val_loss=0.1862 val_acc=0.9373 val_auc=0.9834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 00:03:40,262] Trial 24 finished with value: 0.9372659176029963 and parameters: {'learning_rate': 0.0008419023433015022, 'reg_strength': 1.3334952632131421e-05, 'dropout_conv': 0.1962567146305223, 'dropout_dense': 0.38577840121344326, 'dense_units': 1024, 'filters_multiplier': 1.6657302755627448, 'batch_size': 16, 'beta_1': 0.8216717131687162, 'beta_2': 0.9421149668069908}. Best is trial 23 with value: 0.954119850187266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24: Accuracy = 0.9373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5414 acc=0.7343  val_loss=0.7140 val_acc=0.6479 val_auc=0.8300\n",
      "Epoch 2/30  loss=0.4830 acc=0.7724  val_loss=0.4826 val_acc=0.7875 val_auc=0.8967\n",
      "Epoch 3/30  loss=0.4448 acc=0.7947  val_loss=0.5665 val_acc=0.7388 val_auc=0.8978\n",
      "Epoch 4/30  loss=0.4094 acc=0.8173  val_loss=0.6618 val_acc=0.7228 val_auc=0.9187\n",
      "Epoch 5/30  loss=0.3809 acc=0.8353  val_loss=0.6276 val_acc=0.7622 val_auc=0.9219\n",
      "Epoch 6/30  loss=0.3650 acc=0.8411  val_loss=0.4635 val_acc=0.8249 val_auc=0.9071\n",
      "Epoch 7/30  loss=0.3430 acc=0.8567  val_loss=0.3665 val_acc=0.8549 val_auc=0.9462\n",
      "Epoch 8/30  loss=0.3252 acc=0.8598  val_loss=0.3427 val_acc=0.8736 val_auc=0.9416\n",
      "Epoch 9/30  loss=0.3142 acc=0.8648  val_loss=0.3315 val_acc=0.8633 val_auc=0.9572\n",
      "Epoch 10/30  loss=0.2993 acc=0.8772  val_loss=0.4039 val_acc=0.8530 val_auc=0.9329\n",
      "Epoch 11/30  loss=0.2940 acc=0.8779  val_loss=0.2404 val_acc=0.9054 val_auc=0.9657\n",
      "Epoch 12/30  loss=0.2853 acc=0.8839  val_loss=0.2362 val_acc=0.9082 val_auc=0.9665\n",
      "Epoch 13/30  loss=0.2721 acc=0.8876  val_loss=0.2660 val_acc=0.8989 val_auc=0.9620\n",
      "Epoch 14/30  loss=0.2677 acc=0.8906  val_loss=0.2773 val_acc=0.8989 val_auc=0.9661\n",
      "Epoch 15/30  loss=0.2622 acc=0.8921  val_loss=0.2319 val_acc=0.9176 val_auc=0.9750\n",
      "Epoch 16/30  loss=0.2537 acc=0.8968  val_loss=0.4572 val_acc=0.8792 val_auc=0.9324\n",
      "Epoch 17/30  loss=0.2450 acc=0.9016  val_loss=0.2343 val_acc=0.9101 val_auc=0.9707\n",
      "Epoch 18/30  loss=0.2422 acc=0.9002  val_loss=0.2158 val_acc=0.9195 val_auc=0.9727\n",
      "Epoch 19/30  loss=0.2400 acc=0.9038  val_loss=0.2362 val_acc=0.9054 val_auc=0.9811\n",
      "Epoch 20/30  loss=0.2322 acc=0.9065  val_loss=0.3465 val_acc=0.8708 val_auc=0.9765\n",
      "Epoch 21/30  loss=0.2216 acc=0.9116  val_loss=0.3003 val_acc=0.9036 val_auc=0.9610\n",
      "Epoch 22/30  loss=0.2279 acc=0.9096  val_loss=0.2907 val_acc=0.8961 val_auc=0.9790\n",
      "Epoch 23/30  loss=0.2153 acc=0.9150  val_loss=0.2037 val_acc=0.9270 val_auc=0.9839\n",
      "Epoch 24/30  loss=0.2155 acc=0.9122  val_loss=0.1959 val_acc=0.9223 val_auc=0.9826\n",
      "Epoch 25/30  loss=0.2102 acc=0.9168  val_loss=0.2318 val_acc=0.9204 val_auc=0.9742\n",
      "Epoch 26/30  loss=0.2094 acc=0.9144  val_loss=0.2319 val_acc=0.9139 val_auc=0.9842\n",
      "Epoch 27/30  loss=0.2024 acc=0.9192  val_loss=0.1539 val_acc=0.9345 val_auc=0.9855\n",
      "Epoch 28/30  loss=0.2112 acc=0.9157  val_loss=0.1938 val_acc=0.9288 val_auc=0.9823\n",
      "Epoch 29/30  loss=0.1915 acc=0.9224  val_loss=0.1892 val_acc=0.9326 val_auc=0.9890\n",
      "Epoch 30/30  loss=0.1860 acc=0.9286  val_loss=0.1941 val_acc=0.9298 val_auc=0.9904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 00:09:38,853] Trial 25 finished with value: 0.9344569288389513 and parameters: {'learning_rate': 6.014809778333548e-05, 'reg_strength': 6.971353669187017e-05, 'dropout_conv': 0.1782773797457403, 'dropout_dense': 0.2612002399775446, 'dense_units': 1024, 'filters_multiplier': 1.9991298710836292, 'batch_size': 16, 'beta_1': 0.7371079718856993, 'beta_2': 0.9621145310996546}. Best is trial 23 with value: 0.954119850187266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25: Accuracy = 0.9345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5401 acc=0.7281  val_loss=0.5880 val_acc=0.7322 val_auc=0.8287\n",
      "Epoch 2/30  loss=0.4763 acc=0.7780  val_loss=0.4793 val_acc=0.8099 val_auc=0.8825\n",
      "Epoch 3/30  loss=0.4247 acc=0.8057  val_loss=0.5093 val_acc=0.7753 val_auc=0.9090\n",
      "Epoch 4/30  loss=0.3926 acc=0.8262  val_loss=0.3207 val_acc=0.8736 val_auc=0.9412\n",
      "Epoch 5/30  loss=0.3624 acc=0.8436  val_loss=0.3483 val_acc=0.8708 val_auc=0.9389\n",
      "Epoch 6/30  loss=0.3307 acc=0.8622  val_loss=0.4132 val_acc=0.8155 val_auc=0.9547\n",
      "Epoch 7/30  loss=0.3171 acc=0.8672  val_loss=0.3659 val_acc=0.8614 val_auc=0.9498\n",
      "Epoch 8/30  loss=0.2953 acc=0.8759  val_loss=0.3647 val_acc=0.8624 val_auc=0.9374\n",
      "Epoch 9/30  loss=0.2844 acc=0.8832  val_loss=0.2976 val_acc=0.8830 val_auc=0.9620\n",
      "Epoch 10/30  loss=0.2696 acc=0.8909  val_loss=0.4480 val_acc=0.8249 val_auc=0.9646\n",
      "Epoch 11/30  loss=0.2551 acc=0.8984  val_loss=0.3060 val_acc=0.8839 val_auc=0.9661\n",
      "Epoch 12/30  loss=0.2533 acc=0.8956  val_loss=0.4132 val_acc=0.8493 val_auc=0.9585\n",
      "Epoch 13/30  loss=0.2434 acc=0.9037  val_loss=0.3753 val_acc=0.8764 val_auc=0.9640\n",
      "Epoch 14/30  loss=0.2354 acc=0.9071  val_loss=0.4268 val_acc=0.8474 val_auc=0.9578\n",
      "Epoch 15/30  loss=0.2283 acc=0.9099  val_loss=0.2988 val_acc=0.8839 val_auc=0.9649\n",
      "Epoch 16/30  loss=0.2213 acc=0.9132  val_loss=0.5667 val_acc=0.8305 val_auc=0.9604\n",
      "Epoch 17/30  loss=0.2069 acc=0.9180  val_loss=0.2544 val_acc=0.9054 val_auc=0.9726\n",
      "Epoch 18/30  loss=0.2007 acc=0.9217  val_loss=0.7506 val_acc=0.7378 val_auc=0.9598\n",
      "Epoch 19/30  loss=0.1973 acc=0.9223  val_loss=0.2261 val_acc=0.9298 val_auc=0.9762\n",
      "Epoch 20/30  loss=0.1992 acc=0.9212  val_loss=0.5177 val_acc=0.8024 val_auc=0.9628\n",
      "Epoch 21/30  loss=0.1984 acc=0.9213  val_loss=0.3256 val_acc=0.8830 val_auc=0.9779\n",
      "Epoch 22/30  loss=0.1924 acc=0.9240  val_loss=0.4410 val_acc=0.8586 val_auc=0.9817\n",
      "Epoch 23/30  loss=0.1793 acc=0.9279  val_loss=0.1722 val_acc=0.9354 val_auc=0.9861\n",
      "Epoch 24/30  loss=0.1804 acc=0.9287  val_loss=0.2104 val_acc=0.9326 val_auc=0.9834\n",
      "Epoch 25/30  loss=0.1771 acc=0.9288  val_loss=0.2400 val_acc=0.9167 val_auc=0.9834\n",
      "Epoch 26/30  loss=0.1724 acc=0.9346  val_loss=0.1651 val_acc=0.9410 val_auc=0.9872\n",
      "Epoch 27/30  loss=0.1649 acc=0.9346  val_loss=0.1928 val_acc=0.9110 val_auc=0.9793\n",
      "Epoch 28/30  loss=0.1628 acc=0.9353  val_loss=0.2471 val_acc=0.9082 val_auc=0.9779\n",
      "Epoch 29/30  loss=0.1538 acc=0.9414  val_loss=0.3337 val_acc=0.8998 val_auc=0.9641\n",
      "Epoch 30/30  loss=0.1570 acc=0.9403  val_loss=0.1290 val_acc=0.9466 val_auc=0.9920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 00:15:16,296] Trial 26 finished with value: 0.9466292134831461 and parameters: {'learning_rate': 0.0006762024564350471, 'reg_strength': 1.3581573597455931e-05, 'dropout_conv': 0.20996154470946013, 'dropout_dense': 0.30975364088640783, 'dense_units': 1024, 'filters_multiplier': 1.8120575029778978, 'batch_size': 64, 'beta_1': 0.8948303561063257, 'beta_2': 0.9301005794054945}. Best is trial 23 with value: 0.954119850187266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26: Accuracy = 0.9466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5360 acc=0.7261  val_loss=0.5449 val_acc=0.7378 val_auc=0.8130\n",
      "Epoch 2/30  loss=0.4561 acc=0.7884  val_loss=0.4535 val_acc=0.8052 val_auc=0.8829\n",
      "Epoch 3/30  loss=0.4280 acc=0.8118  val_loss=0.5193 val_acc=0.8015 val_auc=0.8837\n",
      "Epoch 4/30  loss=0.3713 acc=0.8415  val_loss=0.4242 val_acc=0.8249 val_auc=0.9228\n",
      "Epoch 5/30  loss=0.3398 acc=0.8558  val_loss=0.5010 val_acc=0.8090 val_auc=0.8798\n",
      "Epoch 6/30  loss=0.3182 acc=0.8639  val_loss=0.5925 val_acc=0.8155 val_auc=0.8906\n",
      "Epoch 7/30  loss=0.3086 acc=0.8694  val_loss=0.4659 val_acc=0.8099 val_auc=0.9274\n",
      "Epoch 8/30  loss=0.2985 acc=0.8758  val_loss=0.4683 val_acc=0.8146 val_auc=0.9402\n",
      "Epoch 9/30  loss=0.2786 acc=0.8846  val_loss=0.4056 val_acc=0.8399 val_auc=0.9473\n",
      "Epoch 10/30  loss=0.2671 acc=0.8951  val_loss=0.4615 val_acc=0.8343 val_auc=0.9539\n",
      "Epoch 11/30  loss=0.2605 acc=0.8958  val_loss=0.2630 val_acc=0.8989 val_auc=0.9691\n",
      "Epoch 12/30  loss=0.2409 acc=0.9039  val_loss=0.6375 val_acc=0.7912 val_auc=0.9161\n",
      "Epoch 13/30  loss=0.2304 acc=0.9107  val_loss=0.3175 val_acc=0.8886 val_auc=0.9610\n",
      "Epoch 14/30  loss=0.2305 acc=0.9098  val_loss=0.1973 val_acc=0.9185 val_auc=0.9816\n",
      "Epoch 15/30  loss=0.2188 acc=0.9134  val_loss=0.2125 val_acc=0.9064 val_auc=0.9740\n",
      "Epoch 16/30  loss=0.2055 acc=0.9178  val_loss=0.1747 val_acc=0.9382 val_auc=0.9834\n",
      "Epoch 17/30  loss=0.2032 acc=0.9199  val_loss=0.2325 val_acc=0.9110 val_auc=0.9756\n",
      "Epoch 18/30  loss=0.1920 acc=0.9253  val_loss=0.1909 val_acc=0.9298 val_auc=0.9814\n",
      "Epoch 19/30  loss=0.1867 acc=0.9280  val_loss=0.1512 val_acc=0.9448 val_auc=0.9864\n",
      "Epoch 20/30  loss=0.1807 acc=0.9313  val_loss=0.1920 val_acc=0.9345 val_auc=0.9830\n",
      "Epoch 21/30  loss=0.1777 acc=0.9327  val_loss=0.2135 val_acc=0.9157 val_auc=0.9884\n",
      "Epoch 22/30  loss=0.1681 acc=0.9320  val_loss=0.1640 val_acc=0.9326 val_auc=0.9852\n",
      "Epoch 23/30  loss=0.1657 acc=0.9343  val_loss=0.1306 val_acc=0.9532 val_auc=0.9902\n",
      "Epoch 24/30  loss=0.1633 acc=0.9350  val_loss=0.2036 val_acc=0.9335 val_auc=0.9866\n",
      "Epoch 25/30  loss=0.1516 acc=0.9425  val_loss=0.1563 val_acc=0.9382 val_auc=0.9867\n",
      "Epoch 26/30  loss=0.1539 acc=0.9421  val_loss=0.1412 val_acc=0.9476 val_auc=0.9882\n",
      "Epoch 27/30  loss=0.1475 acc=0.9411  val_loss=0.2367 val_acc=0.9213 val_auc=0.9889\n",
      "Epoch 28/30  loss=0.1410 acc=0.9465  val_loss=0.1725 val_acc=0.9298 val_auc=0.9868\n",
      "Epoch 29/30  loss=0.1467 acc=0.9417  val_loss=0.1768 val_acc=0.9373 val_auc=0.9869\n",
      "Epoch 30/30  loss=0.1239 acc=0.9528  val_loss=0.1240 val_acc=0.9551 val_auc=0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 00:20:55,060] Trial 27 finished with value: 0.9550561797752809 and parameters: {'learning_rate': 0.00018064911432265538, 'reg_strength': 1.0488916443258393e-05, 'dropout_conv': 0.21316216838882943, 'dropout_dense': 0.45009499424968413, 'dense_units': 1024, 'filters_multiplier': 1.850791966520846, 'batch_size': 64, 'beta_1': 0.8986738212085873, 'beta_2': 0.9266570137428441}. Best is trial 27 with value: 0.9550561797752809.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27: Accuracy = 0.9551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5480 acc=0.7132  val_loss=0.5660 val_acc=0.7275 val_auc=0.8032\n",
      "Epoch 2/30  loss=0.4916 acc=0.7671  val_loss=0.5793 val_acc=0.7444 val_auc=0.8145\n",
      "Epoch 3/30  loss=0.4603 acc=0.7831  val_loss=0.7085 val_acc=0.6629 val_auc=0.8331\n",
      "Epoch 4/30  loss=0.4425 acc=0.7944  val_loss=0.8580 val_acc=0.6320 val_auc=0.8067\n",
      "Epoch 5/30  loss=0.4279 acc=0.8048  val_loss=0.5568 val_acc=0.7444 val_auc=0.8353\n",
      "Epoch 6/30  loss=0.4145 acc=0.8178  val_loss=0.7936 val_acc=0.6751 val_auc=0.8581\n",
      "Epoch 7/30  loss=0.4010 acc=0.8217  val_loss=0.8289 val_acc=0.6367 val_auc=0.8307\n",
      "Epoch 8/30  loss=0.3902 acc=0.8272  val_loss=0.7096 val_acc=0.7013 val_auc=0.8563\n",
      "Epoch 9/30  loss=0.3720 acc=0.8370  val_loss=0.6534 val_acc=0.7144 val_auc=0.8649\n",
      "Epoch 10/30  loss=0.3678 acc=0.8409  val_loss=0.6596 val_acc=0.7247 val_auc=0.8830\n",
      "Epoch 11/30  loss=0.3643 acc=0.8435  val_loss=0.6423 val_acc=0.7331 val_auc=0.8866\n",
      "Epoch 12/30  loss=0.3595 acc=0.8421  val_loss=0.7110 val_acc=0.7144 val_auc=0.8694\n",
      "Epoch 13/30  loss=0.3499 acc=0.8504  val_loss=0.6537 val_acc=0.7331 val_auc=0.8833\n",
      "Epoch 14/30  loss=0.3502 acc=0.8485  val_loss=0.6110 val_acc=0.7603 val_auc=0.8902\n",
      "Epoch 15/30  loss=0.3519 acc=0.8500  val_loss=0.6231 val_acc=0.7472 val_auc=0.8799\n",
      "Epoch 16/30  loss=0.3496 acc=0.8471  val_loss=0.6709 val_acc=0.7247 val_auc=0.8787\n",
      "Epoch 17/30  loss=0.3482 acc=0.8549  val_loss=0.6460 val_acc=0.7491 val_auc=0.8735\n",
      "Epoch 18/30  loss=0.3430 acc=0.8545  val_loss=0.6290 val_acc=0.7547 val_auc=0.8786\n",
      "Epoch 19/30  loss=0.3419 acc=0.8532  val_loss=0.6431 val_acc=0.7622 val_auc=0.8763\n",
      "Epoch 20/30  loss=0.3341 acc=0.8598  val_loss=0.6160 val_acc=0.7753 val_auc=0.8673\n",
      "Epoch 21/30  loss=0.3363 acc=0.8580  val_loss=0.8091 val_acc=0.6910 val_auc=0.8456\n",
      "Epoch 22/30  loss=0.3358 acc=0.8590  val_loss=0.7486 val_acc=0.7154 val_auc=0.8566\n",
      "Epoch 23/30  loss=0.3309 acc=0.8632  val_loss=0.7075 val_acc=0.7369 val_auc=0.8588\n",
      "Epoch 24/30  loss=0.3271 acc=0.8643  val_loss=0.5853 val_acc=0.7706 val_auc=0.8760\n",
      "Epoch 25/30  loss=0.3303 acc=0.8603  val_loss=0.6616 val_acc=0.7650 val_auc=0.8541\n",
      "Epoch 26/30  loss=0.3202 acc=0.8675  val_loss=0.6309 val_acc=0.7678 val_auc=0.8786\n",
      "Epoch 27/30  loss=0.3184 acc=0.8658  val_loss=0.6879 val_acc=0.7434 val_auc=0.8698\n",
      "Epoch 28/30  loss=0.3207 acc=0.8659  val_loss=0.6596 val_acc=0.7594 val_auc=0.8688\n",
      "Epoch 29/30  loss=0.3190 acc=0.8639  val_loss=0.6764 val_acc=0.7519 val_auc=0.8714\n",
      "Epoch 30/30  loss=0.3143 acc=0.8672  val_loss=0.6149 val_acc=0.7725 val_auc=0.8761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 00:26:32,472] Trial 28 finished with value: 0.7752808988764045 and parameters: {'learning_rate': 2.3864844194435386e-05, 'reg_strength': 1.2061585829639783e-05, 'dropout_conv': 0.19971750941080837, 'dropout_dense': 0.46006328183872447, 'dense_units': 1024, 'filters_multiplier': 1.873808194143682, 'batch_size': 64, 'beta_1': 0.9044271315668738, 'beta_2': 0.929558466024849}. Best is trial 27 with value: 0.9550561797752809.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28: Accuracy = 0.7753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5410 acc=0.7330  val_loss=0.4484 val_acc=0.7921 val_auc=0.8721\n",
      "Epoch 2/30  loss=0.4707 acc=0.7844  val_loss=0.4575 val_acc=0.7921 val_auc=0.8764\n",
      "Epoch 3/30  loss=0.4244 acc=0.8048  val_loss=0.4333 val_acc=0.8118 val_auc=0.9114\n",
      "Epoch 4/30  loss=0.3941 acc=0.8272  val_loss=0.3985 val_acc=0.8343 val_auc=0.9094\n",
      "Epoch 5/30  loss=0.3658 acc=0.8391  val_loss=0.6283 val_acc=0.7828 val_auc=0.9000\n",
      "Epoch 6/30  loss=0.3471 acc=0.8489  val_loss=0.3609 val_acc=0.8502 val_auc=0.9262\n",
      "Epoch 7/30  loss=0.3309 acc=0.8604  val_loss=0.3681 val_acc=0.8371 val_auc=0.9477\n",
      "Epoch 8/30  loss=0.3038 acc=0.8749  val_loss=0.3502 val_acc=0.8446 val_auc=0.9274\n",
      "Epoch 9/30  loss=0.3115 acc=0.8723  val_loss=0.4034 val_acc=0.8296 val_auc=0.9155\n",
      "Epoch 10/30  loss=0.2854 acc=0.8849  val_loss=0.3214 val_acc=0.8727 val_auc=0.9421\n",
      "Epoch 11/30  loss=0.2773 acc=0.8901  val_loss=0.3844 val_acc=0.8483 val_auc=0.9212\n",
      "Epoch 12/30  loss=0.2612 acc=0.8964  val_loss=0.3608 val_acc=0.8483 val_auc=0.9524\n",
      "Epoch 13/30  loss=0.2510 acc=0.8988  val_loss=0.3894 val_acc=0.8633 val_auc=0.9463\n",
      "Epoch 14/30  loss=0.2394 acc=0.9026  val_loss=0.2281 val_acc=0.9101 val_auc=0.9698\n",
      "Epoch 15/30  loss=0.2332 acc=0.9071  val_loss=0.2558 val_acc=0.9007 val_auc=0.9610\n",
      "Epoch 16/30  loss=0.2320 acc=0.9061  val_loss=0.2722 val_acc=0.8904 val_auc=0.9679\n",
      "Epoch 17/30  loss=0.2198 acc=0.9153  val_loss=0.2277 val_acc=0.9082 val_auc=0.9698\n",
      "Epoch 18/30  loss=0.2141 acc=0.9158  val_loss=0.2415 val_acc=0.9054 val_auc=0.9692\n",
      "Epoch 19/30  loss=0.2046 acc=0.9226  val_loss=0.1908 val_acc=0.9148 val_auc=0.9818\n",
      "Epoch 20/30  loss=0.2040 acc=0.9181  val_loss=0.2216 val_acc=0.9129 val_auc=0.9790\n",
      "Epoch 21/30  loss=0.1933 acc=0.9258  val_loss=0.2551 val_acc=0.8979 val_auc=0.9793\n",
      "Epoch 22/30  loss=0.1770 acc=0.9300  val_loss=0.1638 val_acc=0.9401 val_auc=0.9846\n",
      "Epoch 23/30  loss=0.1854 acc=0.9290  val_loss=0.1413 val_acc=0.9429 val_auc=0.9901\n",
      "Epoch 24/30  loss=0.1712 acc=0.9352  val_loss=0.1873 val_acc=0.9270 val_auc=0.9843\n",
      "Epoch 25/30  loss=0.1719 acc=0.9338  val_loss=0.1325 val_acc=0.9532 val_auc=0.9889\n",
      "Epoch 26/30  loss=0.1690 acc=0.9345  val_loss=0.2054 val_acc=0.9129 val_auc=0.9864\n",
      "Epoch 27/30  loss=0.1675 acc=0.9369  val_loss=0.1783 val_acc=0.9260 val_auc=0.9895\n",
      "Epoch 28/30  loss=0.1588 acc=0.9386  val_loss=0.1601 val_acc=0.9373 val_auc=0.9868\n",
      "Epoch 29/30  loss=0.1617 acc=0.9375  val_loss=0.1444 val_acc=0.9382 val_auc=0.9873\n",
      "Epoch 30/30  loss=0.1593 acc=0.9391  val_loss=0.1269 val_acc=0.9476 val_auc=0.9932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 00:32:08,089] Trial 29 finished with value: 0.9531835205992509 and parameters: {'learning_rate': 0.00034330621964400593, 'reg_strength': 3.4151329903854167e-06, 'dropout_conv': 0.3097652772450197, 'dropout_dense': 0.5554283627693654, 'dense_units': 1024, 'filters_multiplier': 1.7951242083972994, 'batch_size': 64, 'beta_1': 0.9547349828565488, 'beta_2': 0.9409973425088725}. Best is trial 27 with value: 0.9550561797752809.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 29: Accuracy = 0.9532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5509 acc=0.7199  val_loss=0.4511 val_acc=0.7940 val_auc=0.8784\n",
      "Epoch 2/30  loss=0.4718 acc=0.7776  val_loss=0.4227 val_acc=0.8090 val_auc=0.8929\n",
      "Epoch 3/30  loss=0.4320 acc=0.8088  val_loss=0.4057 val_acc=0.8268 val_auc=0.9043\n",
      "Epoch 4/30  loss=0.4020 acc=0.8207  val_loss=0.3529 val_acc=0.8493 val_auc=0.9280\n",
      "Epoch 5/30  loss=0.3616 acc=0.8473  val_loss=0.5353 val_acc=0.7856 val_auc=0.9139\n",
      "Epoch 6/30  loss=0.3391 acc=0.8587  val_loss=0.3497 val_acc=0.8577 val_auc=0.9302\n",
      "Epoch 7/30  loss=0.3281 acc=0.8574  val_loss=0.4132 val_acc=0.8277 val_auc=0.9323\n",
      "Epoch 8/30  loss=0.3130 acc=0.8682  val_loss=0.2939 val_acc=0.8773 val_auc=0.9519\n",
      "Epoch 9/30  loss=0.2951 acc=0.8798  val_loss=0.2485 val_acc=0.9017 val_auc=0.9624\n",
      "Epoch 10/30  loss=0.2800 acc=0.8844  val_loss=0.2546 val_acc=0.8942 val_auc=0.9675\n",
      "Epoch 11/30  loss=0.2642 acc=0.8945  val_loss=0.6521 val_acc=0.7828 val_auc=0.9353\n",
      "Epoch 12/30  loss=0.2652 acc=0.8938  val_loss=0.4294 val_acc=0.8539 val_auc=0.9200\n",
      "Epoch 13/30  loss=0.2656 acc=0.8872  val_loss=0.3335 val_acc=0.8661 val_auc=0.9659\n",
      "Epoch 14/30  loss=0.2554 acc=0.8950  val_loss=0.2780 val_acc=0.8904 val_auc=0.9569\n",
      "Epoch 15/30  loss=0.2461 acc=0.9029  val_loss=0.2194 val_acc=0.9167 val_auc=0.9714\n",
      "Epoch 16/30  loss=0.2427 acc=0.9064  val_loss=0.2646 val_acc=0.8951 val_auc=0.9605\n",
      "Epoch 17/30  loss=0.2264 acc=0.9110  val_loss=0.1851 val_acc=0.9223 val_auc=0.9793\n",
      "Epoch 18/30  loss=0.2284 acc=0.9121  val_loss=0.3095 val_acc=0.8904 val_auc=0.9579\n",
      "Epoch 19/30  loss=0.2162 acc=0.9130  val_loss=0.2432 val_acc=0.9101 val_auc=0.9770\n",
      "Epoch 20/30  loss=0.2177 acc=0.9147  val_loss=0.2460 val_acc=0.8951 val_auc=0.9722\n",
      "Epoch 21/30  loss=0.2104 acc=0.9185  val_loss=0.3156 val_acc=0.8792 val_auc=0.9731\n",
      "Epoch 22/30  loss=0.2052 acc=0.9178  val_loss=0.2417 val_acc=0.9007 val_auc=0.9705\n",
      "Epoch 23/30  loss=0.2062 acc=0.9210  val_loss=0.3663 val_acc=0.8661 val_auc=0.9713\n",
      "Epoch 24/30  loss=0.1692 acc=0.9345  val_loss=0.1807 val_acc=0.9382 val_auc=0.9832\n",
      "Epoch 25/30  loss=0.1606 acc=0.9370  val_loss=0.1957 val_acc=0.9288 val_auc=0.9838\n",
      "Epoch 26/30  loss=0.1519 acc=0.9414  val_loss=0.1801 val_acc=0.9298 val_auc=0.9835\n",
      "Epoch 27/30  loss=0.1480 acc=0.9446  val_loss=0.1606 val_acc=0.9382 val_auc=0.9854\n",
      "Epoch 28/30  loss=0.1497 acc=0.9429  val_loss=0.1930 val_acc=0.9326 val_auc=0.9834\n",
      "Epoch 29/30  loss=0.1506 acc=0.9425  val_loss=0.2079 val_acc=0.9232 val_auc=0.9801\n",
      "Epoch 30/30  loss=0.1420 acc=0.9437  val_loss=0.1839 val_acc=0.9251 val_auc=0.9825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 00:37:44,473] Trial 30 finished with value: 0.9382022471910112 and parameters: {'learning_rate': 0.0003013084243234013, 'reg_strength': 2.9027392026022644e-06, 'dropout_conv': 0.3293292272938662, 'dropout_dense': 0.5244811489476257, 'dense_units': 1024, 'filters_multiplier': 1.6183799616688106, 'batch_size': 64, 'beta_1': 0.9615708754589982, 'beta_2': 0.9572338802336867}. Best is trial 27 with value: 0.9550561797752809.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30: Accuracy = 0.9382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5505 acc=0.7218  val_loss=0.4967 val_acc=0.7715 val_auc=0.8509\n",
      "Epoch 2/30  loss=0.4792 acc=0.7740  val_loss=0.4864 val_acc=0.7828 val_auc=0.8608\n",
      "Epoch 3/30  loss=0.4366 acc=0.8008  val_loss=0.5792 val_acc=0.7257 val_auc=0.8801\n",
      "Epoch 4/30  loss=0.4060 acc=0.8193  val_loss=0.3741 val_acc=0.8577 val_auc=0.9213\n",
      "Epoch 5/30  loss=0.3906 acc=0.8276  val_loss=0.3154 val_acc=0.8699 val_auc=0.9442\n",
      "Epoch 6/30  loss=0.3773 acc=0.8379  val_loss=0.3767 val_acc=0.8371 val_auc=0.9250\n",
      "Epoch 7/30  loss=0.3503 acc=0.8500  val_loss=0.3659 val_acc=0.8474 val_auc=0.9280\n",
      "Epoch 8/30  loss=0.3299 acc=0.8603  val_loss=0.4546 val_acc=0.8361 val_auc=0.9285\n",
      "Epoch 9/30  loss=0.3197 acc=0.8618  val_loss=0.3522 val_acc=0.8577 val_auc=0.9451\n",
      "Epoch 10/30  loss=0.3027 acc=0.8728  val_loss=0.4113 val_acc=0.8390 val_auc=0.9360\n",
      "Epoch 11/30  loss=0.2953 acc=0.8798  val_loss=0.2671 val_acc=0.8961 val_auc=0.9584\n",
      "Epoch 12/30  loss=0.2762 acc=0.8814  val_loss=0.3232 val_acc=0.8708 val_auc=0.9638\n",
      "Epoch 13/30  loss=0.2632 acc=0.8933  val_loss=0.4991 val_acc=0.8081 val_auc=0.9209\n",
      "Epoch 14/30  loss=0.2623 acc=0.8917  val_loss=0.2487 val_acc=0.9064 val_auc=0.9646\n",
      "Epoch 15/30  loss=0.2468 acc=0.9010  val_loss=0.4017 val_acc=0.8333 val_auc=0.9558\n",
      "Epoch 16/30  loss=0.2376 acc=0.9039  val_loss=0.3894 val_acc=0.8474 val_auc=0.9624\n",
      "Epoch 17/30  loss=0.2268 acc=0.9066  val_loss=0.2162 val_acc=0.9167 val_auc=0.9741\n",
      "Epoch 18/30  loss=0.2307 acc=0.9086  val_loss=0.2533 val_acc=0.9017 val_auc=0.9625\n",
      "Epoch 19/30  loss=0.2268 acc=0.9098  val_loss=0.4524 val_acc=0.8240 val_auc=0.9681\n",
      "Epoch 20/30  loss=0.2095 acc=0.9195  val_loss=0.4647 val_acc=0.8249 val_auc=0.9675\n",
      "Epoch 21/30  loss=0.1991 acc=0.9211  val_loss=0.2871 val_acc=0.9017 val_auc=0.9764\n",
      "Epoch 22/30  loss=0.1941 acc=0.9261  val_loss=0.2653 val_acc=0.9064 val_auc=0.9740\n",
      "Epoch 23/30  loss=0.1981 acc=0.9219  val_loss=0.2913 val_acc=0.8961 val_auc=0.9747\n",
      "Epoch 24/30  loss=0.1698 acc=0.9316  val_loss=0.1992 val_acc=0.9316 val_auc=0.9838\n",
      "Epoch 25/30  loss=0.1612 acc=0.9366  val_loss=0.2261 val_acc=0.9195 val_auc=0.9842\n",
      "Epoch 26/30  loss=0.1517 acc=0.9431  val_loss=0.1816 val_acc=0.9345 val_auc=0.9869\n",
      "Epoch 27/30  loss=0.1581 acc=0.9393  val_loss=0.1696 val_acc=0.9363 val_auc=0.9869\n",
      "Epoch 28/30  loss=0.1469 acc=0.9443  val_loss=0.1590 val_acc=0.9382 val_auc=0.9873\n",
      "Epoch 29/30  loss=0.1423 acc=0.9445  val_loss=0.2291 val_acc=0.9120 val_auc=0.9867\n",
      "Epoch 30/30  loss=0.1402 acc=0.9466  val_loss=0.2238 val_acc=0.9232 val_auc=0.9871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 00:43:20,772] Trial 31 finished with value: 0.9382022471910112 and parameters: {'learning_rate': 0.00046958647558979994, 'reg_strength': 8.29994726099939e-06, 'dropout_conv': 0.2967847991077553, 'dropout_dense': 0.5459152592410442, 'dense_units': 1024, 'filters_multiplier': 1.7964951078639382, 'batch_size': 64, 'beta_1': 0.9776128234575027, 'beta_2': 0.9396552036997842}. Best is trial 27 with value: 0.9550561797752809.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31: Accuracy = 0.9382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5456 acc=0.7237  val_loss=0.5648 val_acc=0.7472 val_auc=0.8162\n",
      "Epoch 2/30  loss=0.4695 acc=0.7788  val_loss=0.4532 val_acc=0.7809 val_auc=0.8931\n",
      "Epoch 3/30  loss=0.4118 acc=0.8126  val_loss=0.3765 val_acc=0.8502 val_auc=0.9213\n",
      "Epoch 4/30  loss=0.3711 acc=0.8387  val_loss=0.4519 val_acc=0.8296 val_auc=0.8941\n",
      "Epoch 5/30  loss=0.3252 acc=0.8631  val_loss=0.3325 val_acc=0.8605 val_auc=0.9382\n",
      "Epoch 6/30  loss=0.3222 acc=0.8632  val_loss=0.3332 val_acc=0.8652 val_auc=0.9441\n",
      "Epoch 7/30  loss=0.2999 acc=0.8750  val_loss=0.5451 val_acc=0.7903 val_auc=0.9274\n",
      "Epoch 8/30  loss=0.2782 acc=0.8857  val_loss=0.5747 val_acc=0.7715 val_auc=0.9472\n",
      "Epoch 9/30  loss=0.2648 acc=0.8954  val_loss=0.4392 val_acc=0.8315 val_auc=0.9677\n",
      "Epoch 10/30  loss=0.2535 acc=0.8962  val_loss=0.2889 val_acc=0.8820 val_auc=0.9583\n",
      "Epoch 11/30  loss=0.2486 acc=0.9007  val_loss=0.2704 val_acc=0.8933 val_auc=0.9580\n",
      "Epoch 12/30  loss=0.2320 acc=0.9073  val_loss=0.2026 val_acc=0.9270 val_auc=0.9772\n",
      "Epoch 13/30  loss=0.2240 acc=0.9100  val_loss=0.2169 val_acc=0.9139 val_auc=0.9733\n",
      "Epoch 14/30  loss=0.2180 acc=0.9139  val_loss=0.1893 val_acc=0.9260 val_auc=0.9774\n",
      "Epoch 15/30  loss=0.2090 acc=0.9215  val_loss=0.1864 val_acc=0.9335 val_auc=0.9806\n",
      "Epoch 16/30  loss=0.2021 acc=0.9185  val_loss=0.2571 val_acc=0.8979 val_auc=0.9656\n",
      "Epoch 17/30  loss=0.1974 acc=0.9224  val_loss=0.2019 val_acc=0.9204 val_auc=0.9772\n",
      "Epoch 18/30  loss=0.1904 acc=0.9260  val_loss=0.1534 val_acc=0.9391 val_auc=0.9885\n",
      "Epoch 19/30  loss=0.1881 acc=0.9260  val_loss=0.1510 val_acc=0.9354 val_auc=0.9866\n",
      "Epoch 20/30  loss=0.1787 acc=0.9290  val_loss=0.2548 val_acc=0.9054 val_auc=0.9825\n",
      "Epoch 21/30  loss=0.1788 acc=0.9293  val_loss=0.2315 val_acc=0.9129 val_auc=0.9858\n",
      "Epoch 22/30  loss=0.1720 acc=0.9336  val_loss=0.1550 val_acc=0.9391 val_auc=0.9861\n",
      "Epoch 23/30  loss=0.1653 acc=0.9346  val_loss=0.1674 val_acc=0.9382 val_auc=0.9834\n",
      "Epoch 24/30  loss=0.1593 acc=0.9382  val_loss=0.1831 val_acc=0.9242 val_auc=0.9809\n",
      "Epoch 25/30  loss=0.1399 acc=0.9449  val_loss=0.1549 val_acc=0.9532 val_auc=0.9889\n",
      "Epoch 26/30  loss=0.1207 acc=0.9549  val_loss=0.1067 val_acc=0.9569 val_auc=0.9929\n",
      "Epoch 27/30  loss=0.1150 acc=0.9562  val_loss=0.1871 val_acc=0.9504 val_auc=0.9878\n",
      "Epoch 28/30  loss=0.1150 acc=0.9587  val_loss=0.1388 val_acc=0.9597 val_auc=0.9905\n",
      "Epoch 29/30  loss=0.1120 acc=0.9588  val_loss=0.1599 val_acc=0.9579 val_auc=0.9890\n",
      "Epoch 30/30  loss=0.1159 acc=0.9569  val_loss=0.1238 val_acc=0.9513 val_auc=0.9915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 00:48:56,427] Trial 32 finished with value: 0.9597378277153558 and parameters: {'learning_rate': 0.00024250991118643372, 'reg_strength': 2.700886695374889e-06, 'dropout_conv': 0.3091761923446543, 'dropout_dense': 0.4206474145705798, 'dense_units': 1024, 'filters_multiplier': 1.7845632340833149, 'batch_size': 64, 'beta_1': 0.9038263907552408, 'beta_2': 0.9226887001699204}. Best is trial 32 with value: 0.9597378277153558.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32: Accuracy = 0.9597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5460 acc=0.7276  val_loss=0.5336 val_acc=0.7406 val_auc=0.8252\n",
      "Epoch 2/30  loss=0.4725 acc=0.7804  val_loss=0.4626 val_acc=0.7949 val_auc=0.8814\n",
      "Epoch 3/30  loss=0.4257 acc=0.8077  val_loss=0.5875 val_acc=0.7519 val_auc=0.8717\n",
      "Epoch 4/30  loss=0.4010 acc=0.8212  val_loss=0.5365 val_acc=0.7800 val_auc=0.9005\n",
      "Epoch 5/30  loss=0.3726 acc=0.8397  val_loss=0.3408 val_acc=0.8727 val_auc=0.9454\n",
      "Epoch 6/30  loss=0.3278 acc=0.8585  val_loss=0.3145 val_acc=0.8680 val_auc=0.9425\n",
      "Epoch 7/30  loss=0.3108 acc=0.8729  val_loss=0.2950 val_acc=0.8848 val_auc=0.9547\n",
      "Epoch 8/30  loss=0.3027 acc=0.8737  val_loss=0.3200 val_acc=0.8801 val_auc=0.9454\n",
      "Epoch 9/30  loss=0.2940 acc=0.8790  val_loss=0.3658 val_acc=0.8455 val_auc=0.9497\n",
      "Epoch 10/30  loss=0.2674 acc=0.8904  val_loss=0.2558 val_acc=0.9026 val_auc=0.9695\n",
      "Epoch 11/30  loss=0.2614 acc=0.8933  val_loss=0.2644 val_acc=0.8886 val_auc=0.9677\n",
      "Epoch 12/30  loss=0.2484 acc=0.9015  val_loss=0.4675 val_acc=0.8455 val_auc=0.9358\n",
      "Epoch 13/30  loss=0.2439 acc=0.9010  val_loss=0.2341 val_acc=0.8970 val_auc=0.9674\n",
      "Epoch 14/30  loss=0.2299 acc=0.9099  val_loss=0.2073 val_acc=0.9223 val_auc=0.9757\n",
      "Epoch 15/30  loss=0.2253 acc=0.9114  val_loss=0.5849 val_acc=0.8062 val_auc=0.9585\n",
      "Epoch 16/30  loss=0.2097 acc=0.9141  val_loss=0.3741 val_acc=0.8596 val_auc=0.9717\n",
      "Epoch 17/30  loss=0.2020 acc=0.9192  val_loss=0.1565 val_acc=0.9382 val_auc=0.9849\n",
      "Epoch 18/30  loss=0.2072 acc=0.9206  val_loss=0.2572 val_acc=0.8923 val_auc=0.9808\n",
      "Epoch 19/30  loss=0.1911 acc=0.9267  val_loss=0.1825 val_acc=0.9223 val_auc=0.9821\n",
      "Epoch 20/30  loss=0.1979 acc=0.9215  val_loss=0.2062 val_acc=0.9270 val_auc=0.9807\n",
      "Epoch 21/30  loss=0.1789 acc=0.9319  val_loss=0.1627 val_acc=0.9401 val_auc=0.9860\n",
      "Epoch 22/30  loss=0.1832 acc=0.9292  val_loss=0.2147 val_acc=0.9298 val_auc=0.9837\n",
      "Epoch 23/30  loss=0.1755 acc=0.9321  val_loss=0.1840 val_acc=0.9326 val_auc=0.9806\n",
      "Epoch 24/30  loss=0.1770 acc=0.9325  val_loss=0.1589 val_acc=0.9373 val_auc=0.9854\n",
      "Epoch 25/30  loss=0.1687 acc=0.9361  val_loss=0.1842 val_acc=0.9288 val_auc=0.9833\n",
      "Epoch 26/30  loss=0.1575 acc=0.9384  val_loss=0.1737 val_acc=0.9307 val_auc=0.9832\n",
      "Epoch 27/30  loss=0.1657 acc=0.9353  val_loss=0.1857 val_acc=0.9232 val_auc=0.9809\n",
      "Epoch 28/30  loss=0.1404 acc=0.9432  val_loss=0.1214 val_acc=0.9541 val_auc=0.9914\n",
      "Epoch 29/30  loss=0.1330 acc=0.9494  val_loss=0.1317 val_acc=0.9485 val_auc=0.9910\n",
      "Epoch 30/30  loss=0.1279 acc=0.9526  val_loss=0.1218 val_acc=0.9532 val_auc=0.9920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 00:54:34,309] Trial 33 finished with value: 0.954119850187266 and parameters: {'learning_rate': 0.00018611787320068002, 'reg_strength': 2.968473239419058e-06, 'dropout_conv': 0.3676058276115043, 'dropout_dense': 0.44621090838094574, 'dense_units': 1024, 'filters_multiplier': 1.8958124349198282, 'batch_size': 64, 'beta_1': 0.9256022026364776, 'beta_2': 0.9220344538654577}. Best is trial 32 with value: 0.9597378277153558.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33: Accuracy = 0.9541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5397 acc=0.7308  val_loss=0.5405 val_acc=0.7294 val_auc=0.8396\n",
      "Epoch 2/30  loss=0.4722 acc=0.7790  val_loss=0.4444 val_acc=0.7846 val_auc=0.8834\n",
      "Epoch 3/30  loss=0.4293 acc=0.8035  val_loss=0.6635 val_acc=0.7669 val_auc=0.8385\n",
      "Epoch 4/30  loss=0.4025 acc=0.8179  val_loss=0.4176 val_acc=0.8165 val_auc=0.9115\n",
      "Epoch 5/30  loss=0.3680 acc=0.8394  val_loss=0.3626 val_acc=0.8455 val_auc=0.9364\n",
      "Epoch 6/30  loss=0.3556 acc=0.8481  val_loss=0.4832 val_acc=0.8043 val_auc=0.9215\n",
      "Epoch 7/30  loss=0.3211 acc=0.8646  val_loss=0.4108 val_acc=0.8399 val_auc=0.9295\n",
      "Epoch 8/30  loss=0.3111 acc=0.8694  val_loss=0.2854 val_acc=0.8904 val_auc=0.9547\n",
      "Epoch 9/30  loss=0.3021 acc=0.8748  val_loss=0.2856 val_acc=0.8801 val_auc=0.9555\n",
      "Epoch 10/30  loss=0.2817 acc=0.8874  val_loss=0.6403 val_acc=0.8024 val_auc=0.8884\n",
      "Epoch 11/30  loss=0.2798 acc=0.8837  val_loss=0.3155 val_acc=0.8783 val_auc=0.9492\n",
      "Epoch 12/30  loss=0.2676 acc=0.8912  val_loss=0.4106 val_acc=0.8586 val_auc=0.9317\n",
      "Epoch 13/30  loss=0.2627 acc=0.8912  val_loss=0.3838 val_acc=0.8633 val_auc=0.9513\n",
      "Epoch 14/30  loss=0.2453 acc=0.9003  val_loss=0.2828 val_acc=0.8755 val_auc=0.9565\n",
      "Epoch 15/30  loss=0.2112 acc=0.9136  val_loss=0.2662 val_acc=0.8942 val_auc=0.9640\n",
      "Epoch 16/30  loss=0.2076 acc=0.9187  val_loss=0.3195 val_acc=0.8858 val_auc=0.9543\n",
      "Epoch 17/30  loss=0.2072 acc=0.9163  val_loss=0.2488 val_acc=0.9064 val_auc=0.9679\n",
      "Epoch 18/30  loss=0.1933 acc=0.9242  val_loss=0.2849 val_acc=0.8858 val_auc=0.9637\n",
      "Epoch 19/30  loss=0.1922 acc=0.9240  val_loss=0.2517 val_acc=0.9026 val_auc=0.9693\n",
      "Epoch 20/30  loss=0.1935 acc=0.9244  val_loss=0.2356 val_acc=0.9064 val_auc=0.9711\n",
      "Epoch 21/30  loss=0.1945 acc=0.9187  val_loss=0.2524 val_acc=0.8933 val_auc=0.9708\n",
      "Epoch 22/30  loss=0.1857 acc=0.9290  val_loss=0.2662 val_acc=0.8886 val_auc=0.9678\n",
      "Epoch 23/30  loss=0.1858 acc=0.9245  val_loss=0.2868 val_acc=0.8951 val_auc=0.9619\n",
      "Epoch 24/30  loss=0.1835 acc=0.9277  val_loss=0.2855 val_acc=0.8801 val_auc=0.9703\n",
      "Epoch 25/30  loss=0.1732 acc=0.9319  val_loss=0.2611 val_acc=0.8848 val_auc=0.9713\n",
      "Epoch 26/30  loss=0.1803 acc=0.9291  val_loss=0.2807 val_acc=0.8792 val_auc=0.9677\n",
      "Epoch 27/30  loss=0.1756 acc=0.9319  val_loss=0.2687 val_acc=0.8820 val_auc=0.9708\n",
      "Epoch 28/30  loss=0.1747 acc=0.9295  val_loss=0.2566 val_acc=0.8914 val_auc=0.9721\n",
      "Epoch 29/30  loss=0.1725 acc=0.9345  val_loss=0.2844 val_acc=0.8830 val_auc=0.9701\n",
      "Epoch 30/30  loss=0.1723 acc=0.9325  val_loss=0.2754 val_acc=0.8858 val_auc=0.9706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:00:15,721] Trial 34 finished with value: 0.9063670411985019 and parameters: {'learning_rate': 9.561515100621007e-05, 'reg_strength': 2.2712875695395882e-06, 'dropout_conv': 0.24232665830658418, 'dropout_dense': 0.42723161558171957, 'dense_units': 1024, 'filters_multiplier': 1.9218916280633915, 'batch_size': 64, 'beta_1': 0.9185462924385641, 'beta_2': 0.9225856237550168}. Best is trial 32 with value: 0.9597378277153558.\n",
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 34: Accuracy = 0.9064\n",
      "Epoch 1/30  loss=0.5458 acc=0.7279  val_loss=0.4546 val_acc=0.7987 val_auc=0.8658\n",
      "Epoch 2/30  loss=0.4697 acc=0.7761  val_loss=0.4575 val_acc=0.8127 val_auc=0.8890\n",
      "Epoch 3/30  loss=0.4389 acc=0.8049  val_loss=0.4349 val_acc=0.8127 val_auc=0.9113\n",
      "Epoch 4/30  loss=0.3988 acc=0.8240  val_loss=0.5022 val_acc=0.8184 val_auc=0.9084\n",
      "Epoch 5/30  loss=0.3729 acc=0.8364  val_loss=0.4206 val_acc=0.8287 val_auc=0.9248\n",
      "Epoch 6/30  loss=0.3544 acc=0.8498  val_loss=0.3247 val_acc=0.8689 val_auc=0.9399\n",
      "Epoch 7/30  loss=0.3317 acc=0.8569  val_loss=0.7152 val_acc=0.7537 val_auc=0.8898\n",
      "Epoch 8/30  loss=0.3173 acc=0.8647  val_loss=0.4039 val_acc=0.8511 val_auc=0.9410\n",
      "Epoch 9/30  loss=0.2900 acc=0.8770  val_loss=0.3919 val_acc=0.8549 val_auc=0.9485\n",
      "Epoch 10/30  loss=0.2862 acc=0.8817  val_loss=0.2930 val_acc=0.8839 val_auc=0.9539\n",
      "Epoch 11/30  loss=0.2645 acc=0.8927  val_loss=0.4432 val_acc=0.8287 val_auc=0.9493\n",
      "Epoch 12/30  loss=0.2551 acc=0.8962  val_loss=0.3689 val_acc=0.8577 val_auc=0.9548\n",
      "Epoch 13/30  loss=0.2603 acc=0.8943  val_loss=0.3419 val_acc=0.8699 val_auc=0.9419\n",
      "Epoch 14/30  loss=0.2329 acc=0.9086  val_loss=0.2506 val_acc=0.9026 val_auc=0.9690\n",
      "Epoch 15/30  loss=0.2401 acc=0.9036  val_loss=0.2293 val_acc=0.9082 val_auc=0.9749\n",
      "Epoch 16/30  loss=0.2239 acc=0.9119  val_loss=0.2675 val_acc=0.8951 val_auc=0.9597\n",
      "Epoch 17/30  loss=0.2347 acc=0.9080  val_loss=0.2338 val_acc=0.9101 val_auc=0.9724\n",
      "Epoch 18/30  loss=0.2186 acc=0.9169  val_loss=0.1669 val_acc=0.9373 val_auc=0.9821\n",
      "Epoch 19/30  loss=0.2122 acc=0.9153  val_loss=0.1867 val_acc=0.9260 val_auc=0.9811\n",
      "Epoch 20/30  loss=0.2035 acc=0.9176  val_loss=0.1999 val_acc=0.9316 val_auc=0.9810\n",
      "Epoch 21/30  loss=0.2004 acc=0.9208  val_loss=0.1880 val_acc=0.9260 val_auc=0.9798\n",
      "Epoch 22/30  loss=0.1955 acc=0.9219  val_loss=0.1816 val_acc=0.9223 val_auc=0.9862\n",
      "Epoch 23/30  loss=0.1941 acc=0.9240  val_loss=0.2617 val_acc=0.8970 val_auc=0.9853\n",
      "Epoch 24/30  loss=0.1921 acc=0.9274  val_loss=0.1720 val_acc=0.9260 val_auc=0.9874\n",
      "Epoch 25/30  loss=0.1623 acc=0.9394  val_loss=0.1278 val_acc=0.9476 val_auc=0.9908\n",
      "Epoch 26/30  loss=0.1476 acc=0.9441  val_loss=0.1773 val_acc=0.9279 val_auc=0.9897\n",
      "Epoch 27/30  loss=0.1519 acc=0.9404  val_loss=0.1413 val_acc=0.9419 val_auc=0.9916\n",
      "Epoch 28/30  loss=0.1414 acc=0.9464  val_loss=0.1284 val_acc=0.9476 val_auc=0.9917\n",
      "Epoch 29/30  loss=0.1468 acc=0.9416  val_loss=0.1485 val_acc=0.9438 val_auc=0.9919\n",
      "Epoch 30/30  loss=0.1398 acc=0.9448  val_loss=0.1508 val_acc=0.9438 val_auc=0.9908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:05:52,004] Trial 35 finished with value: 0.947565543071161 and parameters: {'learning_rate': 0.00022903099962058227, 'reg_strength': 5.428140071117459e-06, 'dropout_conv': 0.3630040582451251, 'dropout_dense': 0.38109982274577836, 'dense_units': 1024, 'filters_multiplier': 1.5843951712438857, 'batch_size': 64, 'beta_1': 0.8609486758125947, 'beta_2': 0.9252935952715222}. Best is trial 32 with value: 0.9597378277153558.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 35: Accuracy = 0.9476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5671 acc=0.7044  val_loss=0.6813 val_acc=0.5880 val_auc=0.6351\n",
      "Epoch 2/30  loss=0.5158 acc=0.7453  val_loss=0.6203 val_acc=0.7079 val_auc=0.7653\n",
      "Epoch 3/30  loss=0.4880 acc=0.7683  val_loss=0.8548 val_acc=0.5843 val_auc=0.7706\n",
      "Epoch 4/30  loss=0.4627 acc=0.7835  val_loss=1.0982 val_acc=0.5524 val_auc=0.7775\n",
      "Epoch 5/30  loss=0.4568 acc=0.7838  val_loss=1.2411 val_acc=0.5159 val_auc=0.8101\n",
      "Epoch 6/30  loss=0.4469 acc=0.7926  val_loss=1.0337 val_acc=0.5684 val_auc=0.8229\n",
      "Epoch 7/30  loss=0.4335 acc=0.8050  val_loss=0.7885 val_acc=0.6816 val_auc=0.8661\n",
      "Epoch 8/30  loss=0.4155 acc=0.8130  val_loss=1.1854 val_acc=0.5431 val_auc=0.8663\n",
      "Epoch 9/30  loss=0.4026 acc=0.8203  val_loss=1.3476 val_acc=0.5365 val_auc=0.8591\n",
      "Epoch 10/30  loss=0.3974 acc=0.8223  val_loss=1.3355 val_acc=0.5356 val_auc=0.8648\n",
      "Epoch 11/30  loss=0.4005 acc=0.8260  val_loss=1.3874 val_acc=0.5393 val_auc=0.8798\n",
      "Epoch 12/30  loss=0.3905 acc=0.8261  val_loss=1.3835 val_acc=0.5403 val_auc=0.8860\n",
      "Epoch 13/30  loss=0.3897 acc=0.8317  val_loss=1.3152 val_acc=0.5618 val_auc=0.8841\n",
      "Epoch 14/30  loss=0.3864 acc=0.8285  val_loss=1.4096 val_acc=0.5487 val_auc=0.8765\n",
      "Epoch 15/30  loss=0.3883 acc=0.8289  val_loss=1.3064 val_acc=0.5758 val_auc=0.8812\n",
      "Epoch 16/30  loss=0.3788 acc=0.8336  val_loss=1.4175 val_acc=0.5543 val_auc=0.8753\n",
      "Early stopping at epoch 17; best val_acc=0.7079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:09:13,510] Trial 36 finished with value: 0.7078651685393258 and parameters: {'learning_rate': 2.906664892095594e-05, 'reg_strength': 2.444456670526474e-05, 'dropout_conv': 0.3749598608698732, 'dropout_dense': 0.46521785547653666, 'dense_units': 1024, 'filters_multiplier': 1.7732371730733945, 'batch_size': 64, 'beta_1': 0.9255741287276223, 'beta_2': 0.9112665506631171}. Best is trial 32 with value: 0.9597378277153558.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 36: Accuracy = 0.7079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5456 acc=0.7292  val_loss=0.6584 val_acc=0.7032 val_auc=0.8594\n",
      "Epoch 2/30  loss=0.4689 acc=0.7807  val_loss=0.6613 val_acc=0.7125 val_auc=0.8455\n",
      "Epoch 3/30  loss=0.4248 acc=0.8051  val_loss=0.6638 val_acc=0.7884 val_auc=0.8574\n",
      "Epoch 4/30  loss=0.3941 acc=0.8255  val_loss=0.5754 val_acc=0.7781 val_auc=0.9150\n",
      "Epoch 5/30  loss=0.3769 acc=0.8402  val_loss=0.6063 val_acc=0.7547 val_auc=0.9226\n",
      "Epoch 6/30  loss=0.3510 acc=0.8482  val_loss=0.3713 val_acc=0.8521 val_auc=0.9458\n",
      "Epoch 7/30  loss=0.3240 acc=0.8659  val_loss=0.6328 val_acc=0.7622 val_auc=0.9224\n",
      "Epoch 8/30  loss=0.3112 acc=0.8706  val_loss=0.3136 val_acc=0.8792 val_auc=0.9478\n",
      "Epoch 9/30  loss=0.2958 acc=0.8766  val_loss=0.4316 val_acc=0.8268 val_auc=0.9551\n",
      "Epoch 10/30  loss=0.2836 acc=0.8857  val_loss=0.2637 val_acc=0.9073 val_auc=0.9593\n",
      "Epoch 11/30  loss=0.2690 acc=0.8915  val_loss=0.3753 val_acc=0.8446 val_auc=0.9435\n",
      "Epoch 12/30  loss=0.2591 acc=0.8940  val_loss=0.2596 val_acc=0.9017 val_auc=0.9631\n",
      "Epoch 13/30  loss=0.2527 acc=0.8985  val_loss=0.2488 val_acc=0.8970 val_auc=0.9660\n",
      "Epoch 14/30  loss=0.2479 acc=0.8996  val_loss=0.2323 val_acc=0.9129 val_auc=0.9764\n",
      "Epoch 15/30  loss=0.2390 acc=0.9048  val_loss=0.3460 val_acc=0.8717 val_auc=0.9656\n",
      "Epoch 16/30  loss=0.2211 acc=0.9155  val_loss=0.2204 val_acc=0.9157 val_auc=0.9737\n",
      "Epoch 17/30  loss=0.2226 acc=0.9093  val_loss=0.2649 val_acc=0.8998 val_auc=0.9645\n",
      "Epoch 18/30  loss=0.2206 acc=0.9132  val_loss=0.2658 val_acc=0.9064 val_auc=0.9739\n",
      "Epoch 19/30  loss=0.2097 acc=0.9167  val_loss=0.2038 val_acc=0.9326 val_auc=0.9826\n",
      "Epoch 20/30  loss=0.1976 acc=0.9239  val_loss=0.2963 val_acc=0.8895 val_auc=0.9734\n",
      "Epoch 21/30  loss=0.1953 acc=0.9268  val_loss=0.2302 val_acc=0.9092 val_auc=0.9785\n",
      "Epoch 22/30  loss=0.1942 acc=0.9226  val_loss=0.1799 val_acc=0.9260 val_auc=0.9806\n",
      "Epoch 23/30  loss=0.1855 acc=0.9280  val_loss=0.2723 val_acc=0.8858 val_auc=0.9826\n",
      "Epoch 24/30  loss=0.1797 acc=0.9320  val_loss=0.1976 val_acc=0.9298 val_auc=0.9834\n",
      "Epoch 25/30  loss=0.1692 acc=0.9318  val_loss=0.2433 val_acc=0.9045 val_auc=0.9735\n",
      "Epoch 26/30  loss=0.1600 acc=0.9371  val_loss=0.1539 val_acc=0.9466 val_auc=0.9868\n",
      "Epoch 27/30  loss=0.1421 acc=0.9464  val_loss=0.1419 val_acc=0.9560 val_auc=0.9883\n",
      "Epoch 28/30  loss=0.1416 acc=0.9449  val_loss=0.1326 val_acc=0.9551 val_auc=0.9888\n",
      "Epoch 29/30  loss=0.1347 acc=0.9471  val_loss=0.1418 val_acc=0.9541 val_auc=0.9873\n",
      "Epoch 30/30  loss=0.1268 acc=0.9511  val_loss=0.1237 val_acc=0.9654 val_auc=0.9906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:14:48,760] Trial 37 finished with value: 0.9653558052434457 and parameters: {'learning_rate': 0.00019329441561008272, 'reg_strength': 2.1392715033008293e-06, 'dropout_conv': 0.3991746766866793, 'dropout_dense': 0.4342883979481444, 'dense_units': 1024, 'filters_multiplier': 1.897507625889965, 'batch_size': 64, 'beta_1': 0.8828029914578701, 'beta_2': 0.9358066391956668}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 37: Accuracy = 0.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5479 acc=0.7247  val_loss=0.5407 val_acc=0.7247 val_auc=0.8507\n",
      "Epoch 2/30  loss=0.4806 acc=0.7714  val_loss=0.7562 val_acc=0.5702 val_auc=0.7601\n",
      "Epoch 3/30  loss=0.4583 acc=0.7873  val_loss=0.6264 val_acc=0.6545 val_auc=0.8543\n",
      "Epoch 4/30  loss=0.4231 acc=0.8091  val_loss=0.4823 val_acc=0.7978 val_auc=0.8737\n",
      "Epoch 5/30  loss=0.4094 acc=0.8205  val_loss=0.7232 val_acc=0.6685 val_auc=0.8295\n",
      "Epoch 6/30  loss=0.3900 acc=0.8294  val_loss=0.6084 val_acc=0.7556 val_auc=0.8359\n",
      "Epoch 7/30  loss=0.3726 acc=0.8413  val_loss=0.4801 val_acc=0.8146 val_auc=0.8979\n",
      "Epoch 8/30  loss=0.3602 acc=0.8464  val_loss=1.1887 val_acc=0.6301 val_auc=0.8099\n",
      "Epoch 9/30  loss=0.3459 acc=0.8543  val_loss=0.4062 val_acc=0.8315 val_auc=0.9174\n",
      "Epoch 10/30  loss=0.3352 acc=0.8584  val_loss=0.8953 val_acc=0.7247 val_auc=0.8605\n",
      "Epoch 11/30  loss=0.3244 acc=0.8642  val_loss=0.5080 val_acc=0.8193 val_auc=0.9092\n",
      "Epoch 12/30  loss=0.3217 acc=0.8658  val_loss=0.7807 val_acc=0.7659 val_auc=0.8514\n",
      "Epoch 13/30  loss=0.3129 acc=0.8689  val_loss=0.8807 val_acc=0.7228 val_auc=0.9040\n",
      "Epoch 14/30  loss=0.3024 acc=0.8742  val_loss=0.7087 val_acc=0.7594 val_auc=0.9004\n",
      "Epoch 15/30  loss=0.2974 acc=0.8798  val_loss=0.4913 val_acc=0.8296 val_auc=0.9065\n",
      "Epoch 16/30  loss=0.2652 acc=0.8926  val_loss=0.7368 val_acc=0.7603 val_auc=0.8967\n",
      "Epoch 17/30  loss=0.2692 acc=0.8914  val_loss=0.8436 val_acc=0.7416 val_auc=0.8808\n",
      "Epoch 18/30  loss=0.2610 acc=0.8949  val_loss=0.8408 val_acc=0.7444 val_auc=0.8872\n",
      "Epoch 19/30  loss=0.2531 acc=0.8969  val_loss=0.7822 val_acc=0.7603 val_auc=0.8976\n",
      "Epoch 20/30  loss=0.2544 acc=0.8944  val_loss=0.8564 val_acc=0.7463 val_auc=0.8883\n",
      "Epoch 21/30  loss=0.2536 acc=0.8972  val_loss=0.8637 val_acc=0.7519 val_auc=0.8983\n",
      "Epoch 22/30  loss=0.2466 acc=0.8999  val_loss=0.8097 val_acc=0.7622 val_auc=0.8929\n",
      "Epoch 23/30  loss=0.2454 acc=0.9022  val_loss=0.7884 val_acc=0.7687 val_auc=0.8919\n",
      "Early stopping at epoch 24; best val_acc=0.8315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:19:28,178] Trial 38 finished with value: 0.8314606741573034 and parameters: {'learning_rate': 0.000102722419536121, 'reg_strength': 1.9144238019038723e-06, 'dropout_conv': 0.39754285303354975, 'dropout_dense': 0.38417877474088746, 'dense_units': 1024, 'filters_multiplier': 1.717585408028211, 'batch_size': 128, 'beta_1': 0.8879719010477959, 'beta_2': 0.9346168349299184}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 38: Accuracy = 0.8315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5427 acc=0.7197  val_loss=0.5908 val_acc=0.7238 val_auc=0.8223\n",
      "Epoch 2/30  loss=0.4801 acc=0.7727  val_loss=0.6440 val_acc=0.7434 val_auc=0.8360\n",
      "Epoch 3/30  loss=0.4498 acc=0.7951  val_loss=0.5351 val_acc=0.7640 val_auc=0.8353\n",
      "Epoch 4/30  loss=0.4105 acc=0.8134  val_loss=0.5164 val_acc=0.7818 val_auc=0.8586\n",
      "Epoch 5/30  loss=0.3877 acc=0.8288  val_loss=0.6280 val_acc=0.7650 val_auc=0.8417\n",
      "Epoch 6/30  loss=0.3738 acc=0.8381  val_loss=0.3848 val_acc=0.8455 val_auc=0.9135\n",
      "Epoch 7/30  loss=0.3468 acc=0.8515  val_loss=0.4401 val_acc=0.7996 val_auc=0.9226\n",
      "Epoch 8/30  loss=0.3336 acc=0.8573  val_loss=0.3019 val_acc=0.8858 val_auc=0.9446\n",
      "Epoch 9/30  loss=0.3252 acc=0.8618  val_loss=0.3860 val_acc=0.8493 val_auc=0.9256\n",
      "Epoch 10/30  loss=0.3068 acc=0.8717  val_loss=0.4176 val_acc=0.8427 val_auc=0.9200\n",
      "Epoch 11/30  loss=0.2856 acc=0.8789  val_loss=0.4421 val_acc=0.8474 val_auc=0.9120\n",
      "Epoch 12/30  loss=0.2868 acc=0.8818  val_loss=0.4261 val_acc=0.8605 val_auc=0.9452\n",
      "Epoch 13/30  loss=0.2771 acc=0.8888  val_loss=0.3855 val_acc=0.8483 val_auc=0.9578\n",
      "Epoch 14/30  loss=0.2701 acc=0.8873  val_loss=0.5261 val_acc=0.8109 val_auc=0.9467\n",
      "Epoch 15/30  loss=0.2528 acc=0.8977  val_loss=0.3348 val_acc=0.8792 val_auc=0.9627\n",
      "Epoch 16/30  loss=0.2380 acc=0.9053  val_loss=0.3141 val_acc=0.8830 val_auc=0.9564\n",
      "Epoch 17/30  loss=0.2368 acc=0.9047  val_loss=0.3308 val_acc=0.8811 val_auc=0.9587\n",
      "Epoch 18/30  loss=0.2216 acc=0.9132  val_loss=0.3139 val_acc=0.8848 val_auc=0.9557\n",
      "Epoch 19/30  loss=0.2231 acc=0.9133  val_loss=0.2940 val_acc=0.8942 val_auc=0.9614\n",
      "Epoch 20/30  loss=0.2255 acc=0.9107  val_loss=0.2807 val_acc=0.8979 val_auc=0.9618\n",
      "Epoch 21/30  loss=0.2200 acc=0.9118  val_loss=0.2882 val_acc=0.8858 val_auc=0.9627\n",
      "Epoch 22/30  loss=0.2244 acc=0.9058  val_loss=0.3292 val_acc=0.8820 val_auc=0.9630\n",
      "Epoch 23/30  loss=0.2194 acc=0.9102  val_loss=0.4303 val_acc=0.8586 val_auc=0.9514\n",
      "Epoch 24/30  loss=0.2129 acc=0.9181  val_loss=0.3023 val_acc=0.8914 val_auc=0.9618\n",
      "Epoch 25/30  loss=0.2164 acc=0.9156  val_loss=0.3908 val_acc=0.8745 val_auc=0.9599\n",
      "Epoch 26/30  loss=0.2063 acc=0.9178  val_loss=0.3165 val_acc=0.8886 val_auc=0.9628\n",
      "Epoch 27/30  loss=0.2072 acc=0.9167  val_loss=0.3281 val_acc=0.8867 val_auc=0.9599\n",
      "Epoch 28/30  loss=0.1965 acc=0.9266  val_loss=0.3218 val_acc=0.8830 val_auc=0.9592\n",
      "Epoch 29/30  loss=0.1997 acc=0.9198  val_loss=0.3018 val_acc=0.8904 val_auc=0.9621\n",
      "Epoch 30/30  loss=0.1976 acc=0.9223  val_loss=0.3065 val_acc=0.8876 val_auc=0.9620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:25:08,531] Trial 39 finished with value: 0.8979400749063671 and parameters: {'learning_rate': 7.505785296940437e-05, 'reg_strength': 5.026536803336188e-06, 'dropout_conv': 0.31500948581663096, 'dropout_dense': 0.47935765319083945, 'dense_units': 1024, 'filters_multiplier': 1.9789674269110018, 'batch_size': 64, 'beta_1': 0.8598161430612785, 'beta_2': 0.9478982169195609}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 39: Accuracy = 0.8979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5778 acc=0.6987  val_loss=1.5732 val_acc=0.5571 val_auc=0.6805\n",
      "Epoch 2/30  loss=0.5247 acc=0.7431  val_loss=1.3354 val_acc=0.6011 val_auc=0.7593\n",
      "Epoch 3/30  loss=0.4935 acc=0.7597  val_loss=1.0125 val_acc=0.6507 val_auc=0.8454\n",
      "Epoch 4/30  loss=0.4829 acc=0.7727  val_loss=0.7841 val_acc=0.6891 val_auc=0.8301\n",
      "Epoch 5/30  loss=0.4634 acc=0.7801  val_loss=1.0118 val_acc=0.6676 val_auc=0.8179\n",
      "Epoch 6/30  loss=0.4587 acc=0.7808  val_loss=0.8677 val_acc=0.6704 val_auc=0.8356\n",
      "Epoch 7/30  loss=0.4486 acc=0.7915  val_loss=0.9109 val_acc=0.6788 val_auc=0.8555\n",
      "Epoch 8/30  loss=0.4323 acc=0.8033  val_loss=1.4175 val_acc=0.6433 val_auc=0.8162\n",
      "Epoch 9/30  loss=0.4351 acc=0.8027  val_loss=1.4803 val_acc=0.6610 val_auc=0.8034\n",
      "Epoch 10/30  loss=0.4203 acc=0.8100  val_loss=1.6697 val_acc=0.6433 val_auc=0.7983\n",
      "Epoch 11/30  loss=0.4101 acc=0.8123  val_loss=1.4487 val_acc=0.6526 val_auc=0.8217\n",
      "Epoch 12/30  loss=0.4006 acc=0.8228  val_loss=1.4234 val_acc=0.6648 val_auc=0.8152\n",
      "Epoch 13/30  loss=0.3945 acc=0.8217  val_loss=1.2870 val_acc=0.6554 val_auc=0.8463\n",
      "Epoch 14/30  loss=0.3973 acc=0.8221  val_loss=1.4333 val_acc=0.6554 val_auc=0.8266\n",
      "Epoch 15/30  loss=0.3882 acc=0.8298  val_loss=1.3532 val_acc=0.6639 val_auc=0.8219\n",
      "Epoch 16/30  loss=0.3944 acc=0.8261  val_loss=1.5238 val_acc=0.6386 val_auc=0.8283\n",
      "Epoch 17/30  loss=0.3796 acc=0.8326  val_loss=1.2223 val_acc=0.6601 val_auc=0.8483\n",
      "Epoch 18/30  loss=0.3850 acc=0.8297  val_loss=1.2524 val_acc=0.6592 val_auc=0.8472\n",
      "Early stopping at epoch 19; best val_acc=0.6891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:28:55,733] Trial 40 finished with value: 0.6891385767790262 and parameters: {'learning_rate': 1.2606053321560215e-05, 'reg_strength': 8.46926894779266e-06, 'dropout_conv': 0.2637466937424087, 'dropout_dense': 0.39326402062138455, 'dense_units': 256, 'filters_multiplier': 1.5024636305351122, 'batch_size': 16, 'beta_1': 0.9053112553942628, 'beta_2': 0.9336106004244917}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40: Accuracy = 0.6891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5365 acc=0.7335  val_loss=0.4825 val_acc=0.7856 val_auc=0.8602\n",
      "Epoch 2/30  loss=0.4654 acc=0.7742  val_loss=0.4330 val_acc=0.7875 val_auc=0.8965\n",
      "Epoch 3/30  loss=0.4413 acc=0.7965  val_loss=0.4240 val_acc=0.8165 val_auc=0.8953\n",
      "Epoch 4/30  loss=0.4090 acc=0.8155  val_loss=0.5346 val_acc=0.7865 val_auc=0.8875\n",
      "Epoch 5/30  loss=0.3744 acc=0.8398  val_loss=0.4895 val_acc=0.8221 val_auc=0.8702\n",
      "Epoch 6/30  loss=0.3440 acc=0.8562  val_loss=0.4226 val_acc=0.8380 val_auc=0.9050\n",
      "Epoch 7/30  loss=0.3288 acc=0.8610  val_loss=0.3621 val_acc=0.8614 val_auc=0.9311\n",
      "Epoch 8/30  loss=0.3081 acc=0.8738  val_loss=0.5049 val_acc=0.8184 val_auc=0.9417\n",
      "Epoch 9/30  loss=0.2934 acc=0.8806  val_loss=0.2964 val_acc=0.8745 val_auc=0.9504\n",
      "Epoch 10/30  loss=0.2805 acc=0.8812  val_loss=0.3749 val_acc=0.8567 val_auc=0.9513\n",
      "Epoch 11/30  loss=0.2614 acc=0.8949  val_loss=0.2852 val_acc=0.8886 val_auc=0.9579\n",
      "Epoch 12/30  loss=0.2536 acc=0.8990  val_loss=0.3145 val_acc=0.8727 val_auc=0.9519\n",
      "Epoch 13/30  loss=0.2499 acc=0.8992  val_loss=0.2317 val_acc=0.9148 val_auc=0.9709\n",
      "Epoch 14/30  loss=0.2297 acc=0.9073  val_loss=0.2498 val_acc=0.8848 val_auc=0.9703\n",
      "Epoch 15/30  loss=0.2299 acc=0.9070  val_loss=0.2830 val_acc=0.8998 val_auc=0.9624\n",
      "Epoch 16/30  loss=0.2276 acc=0.9078  val_loss=0.3445 val_acc=0.8680 val_auc=0.9684\n",
      "Epoch 17/30  loss=0.2182 acc=0.9095  val_loss=0.2532 val_acc=0.8961 val_auc=0.9645\n",
      "Epoch 18/30  loss=0.2074 acc=0.9168  val_loss=0.1733 val_acc=0.9316 val_auc=0.9810\n",
      "Epoch 19/30  loss=0.2137 acc=0.9163  val_loss=0.2404 val_acc=0.9185 val_auc=0.9705\n",
      "Epoch 20/30  loss=0.1961 acc=0.9237  val_loss=0.2581 val_acc=0.8970 val_auc=0.9721\n",
      "Epoch 21/30  loss=0.1941 acc=0.9264  val_loss=0.2118 val_acc=0.9129 val_auc=0.9768\n",
      "Epoch 22/30  loss=0.1945 acc=0.9257  val_loss=0.1714 val_acc=0.9326 val_auc=0.9829\n",
      "Epoch 23/30  loss=0.1847 acc=0.9251  val_loss=0.1976 val_acc=0.9148 val_auc=0.9788\n",
      "Epoch 24/30  loss=0.1749 acc=0.9300  val_loss=0.3653 val_acc=0.8736 val_auc=0.9793\n",
      "Epoch 25/30  loss=0.1861 acc=0.9252  val_loss=0.1584 val_acc=0.9429 val_auc=0.9842\n",
      "Epoch 26/30  loss=0.1757 acc=0.9332  val_loss=0.1417 val_acc=0.9466 val_auc=0.9874\n",
      "Epoch 27/30  loss=0.1668 acc=0.9362  val_loss=0.1465 val_acc=0.9429 val_auc=0.9877\n",
      "Epoch 28/30  loss=0.1625 acc=0.9356  val_loss=0.2138 val_acc=0.9167 val_auc=0.9848\n",
      "Epoch 29/30  loss=0.1666 acc=0.9375  val_loss=0.1556 val_acc=0.9373 val_auc=0.9859\n",
      "Epoch 30/30  loss=0.1582 acc=0.9396  val_loss=0.2068 val_acc=0.9157 val_auc=0.9836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:34:33,106] Trial 41 finished with value: 0.9466292134831461 and parameters: {'learning_rate': 0.00019757215292393127, 'reg_strength': 1.08070916917046e-06, 'dropout_conv': 0.36390068224488775, 'dropout_dense': 0.42882060971607316, 'dense_units': 1024, 'filters_multiplier': 1.881532438389815, 'batch_size': 64, 'beta_1': 0.9192151828141987, 'beta_2': 0.9147313969153016}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 41: Accuracy = 0.9466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5411 acc=0.7316  val_loss=0.6534 val_acc=0.7266 val_auc=0.8441\n",
      "Epoch 2/30  loss=0.4687 acc=0.7828  val_loss=0.4592 val_acc=0.7875 val_auc=0.8861\n",
      "Epoch 3/30  loss=0.4362 acc=0.8013  val_loss=0.8202 val_acc=0.7182 val_auc=0.8662\n",
      "Epoch 4/30  loss=0.4037 acc=0.8240  val_loss=0.5537 val_acc=0.7659 val_auc=0.9202\n",
      "Epoch 5/30  loss=0.3653 acc=0.8433  val_loss=0.4061 val_acc=0.8221 val_auc=0.9027\n",
      "Epoch 6/30  loss=0.3491 acc=0.8498  val_loss=0.4315 val_acc=0.8202 val_auc=0.9119\n",
      "Epoch 7/30  loss=0.3425 acc=0.8578  val_loss=0.4954 val_acc=0.8062 val_auc=0.9155\n",
      "Epoch 8/30  loss=0.3147 acc=0.8684  val_loss=0.5586 val_acc=0.7931 val_auc=0.9384\n",
      "Epoch 9/30  loss=0.3032 acc=0.8749  val_loss=0.3395 val_acc=0.8642 val_auc=0.9410\n",
      "Epoch 10/30  loss=0.3000 acc=0.8749  val_loss=0.3046 val_acc=0.8858 val_auc=0.9498\n",
      "Epoch 11/30  loss=0.2865 acc=0.8818  val_loss=0.4317 val_acc=0.8408 val_auc=0.9171\n",
      "Epoch 12/30  loss=0.2686 acc=0.8955  val_loss=0.2976 val_acc=0.8736 val_auc=0.9569\n",
      "Epoch 13/30  loss=0.2621 acc=0.8935  val_loss=0.2771 val_acc=0.8886 val_auc=0.9574\n",
      "Epoch 14/30  loss=0.2514 acc=0.8965  val_loss=0.4016 val_acc=0.8521 val_auc=0.9591\n",
      "Epoch 15/30  loss=0.2446 acc=0.9013  val_loss=0.5116 val_acc=0.8212 val_auc=0.9299\n",
      "Epoch 16/30  loss=0.2232 acc=0.9167  val_loss=0.2547 val_acc=0.8848 val_auc=0.9649\n",
      "Epoch 17/30  loss=0.2236 acc=0.9096  val_loss=0.2299 val_acc=0.9082 val_auc=0.9702\n",
      "Epoch 18/30  loss=0.2114 acc=0.9149  val_loss=0.2567 val_acc=0.9007 val_auc=0.9688\n",
      "Epoch 19/30  loss=0.2144 acc=0.9135  val_loss=0.1902 val_acc=0.9204 val_auc=0.9796\n",
      "Epoch 20/30  loss=0.1984 acc=0.9242  val_loss=0.2487 val_acc=0.8989 val_auc=0.9805\n",
      "Epoch 21/30  loss=0.2045 acc=0.9183  val_loss=0.2866 val_acc=0.8895 val_auc=0.9628\n",
      "Epoch 22/30  loss=0.2008 acc=0.9197  val_loss=0.2674 val_acc=0.8942 val_auc=0.9751\n",
      "Epoch 23/30  loss=0.1888 acc=0.9263  val_loss=0.2597 val_acc=0.8961 val_auc=0.9682\n",
      "Epoch 24/30  loss=0.1841 acc=0.9279  val_loss=0.1721 val_acc=0.9326 val_auc=0.9829\n",
      "Epoch 25/30  loss=0.1799 acc=0.9291  val_loss=0.1832 val_acc=0.9307 val_auc=0.9801\n",
      "Epoch 26/30  loss=0.1757 acc=0.9322  val_loss=0.1691 val_acc=0.9373 val_auc=0.9840\n",
      "Epoch 27/30  loss=0.1729 acc=0.9343  val_loss=0.1371 val_acc=0.9504 val_auc=0.9882\n",
      "Epoch 28/30  loss=0.1599 acc=0.9364  val_loss=0.1952 val_acc=0.9251 val_auc=0.9827\n",
      "Epoch 29/30  loss=0.1677 acc=0.9343  val_loss=0.2431 val_acc=0.9120 val_auc=0.9808\n",
      "Epoch 30/30  loss=0.1649 acc=0.9356  val_loss=0.1810 val_acc=0.9513 val_auc=0.9827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:40:08,587] Trial 42 finished with value: 0.951310861423221 and parameters: {'learning_rate': 0.00018259435382974237, 'reg_strength': 4.103819905156595e-06, 'dropout_conv': 0.3773213786359202, 'dropout_dense': 0.43259681168042247, 'dense_units': 1024, 'filters_multiplier': 1.9010002700512953, 'batch_size': 64, 'beta_1': 0.9384109570561692, 'beta_2': 0.9214373427378749}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42: Accuracy = 0.9513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5469 acc=0.7235  val_loss=1.2030 val_acc=0.6489 val_auc=0.7947\n",
      "Epoch 2/30  loss=0.4872 acc=0.7663  val_loss=0.8136 val_acc=0.6676 val_auc=0.7282\n",
      "Epoch 3/30  loss=0.4559 acc=0.7870  val_loss=0.9365 val_acc=0.6882 val_auc=0.7165\n",
      "Epoch 4/30  loss=0.4342 acc=0.8001  val_loss=1.0786 val_acc=0.6948 val_auc=0.7368\n",
      "Epoch 5/30  loss=0.4232 acc=0.8110  val_loss=1.0192 val_acc=0.6957 val_auc=0.7336\n",
      "Epoch 6/30  loss=0.4077 acc=0.8214  val_loss=0.8078 val_acc=0.7004 val_auc=0.7833\n",
      "Epoch 7/30  loss=0.4020 acc=0.8191  val_loss=0.7067 val_acc=0.6826 val_auc=0.8878\n",
      "Epoch 8/30  loss=0.3802 acc=0.8289  val_loss=0.7279 val_acc=0.7341 val_auc=0.7462\n",
      "Epoch 9/30  loss=0.3766 acc=0.8345  val_loss=0.5470 val_acc=0.7903 val_auc=0.8398\n",
      "Epoch 10/30  loss=0.3633 acc=0.8436  val_loss=0.9502 val_acc=0.6854 val_auc=0.8664\n",
      "Epoch 11/30  loss=0.3510 acc=0.8515  val_loss=0.7411 val_acc=0.7247 val_auc=0.8904\n",
      "Epoch 12/30  loss=0.3476 acc=0.8514  val_loss=0.6905 val_acc=0.7875 val_auc=0.8306\n",
      "Epoch 13/30  loss=0.3384 acc=0.8538  val_loss=0.6710 val_acc=0.7912 val_auc=0.8291\n",
      "Epoch 14/30  loss=0.3281 acc=0.8622  val_loss=0.7332 val_acc=0.7762 val_auc=0.8224\n",
      "Epoch 15/30  loss=0.3227 acc=0.8615  val_loss=1.0176 val_acc=0.6985 val_auc=0.8790\n",
      "Epoch 16/30  loss=0.3179 acc=0.8666  val_loss=1.0608 val_acc=0.7191 val_auc=0.8724\n",
      "Epoch 17/30  loss=0.3147 acc=0.8667  val_loss=0.7612 val_acc=0.7856 val_auc=0.8732\n",
      "Epoch 18/30  loss=0.3095 acc=0.8724  val_loss=0.8627 val_acc=0.7388 val_auc=0.9143\n",
      "Epoch 19/30  loss=0.2926 acc=0.8807  val_loss=0.6422 val_acc=0.7875 val_auc=0.9450\n",
      "Epoch 20/30  loss=0.2755 acc=0.8835  val_loss=0.6549 val_acc=0.7875 val_auc=0.9228\n",
      "Epoch 21/30  loss=0.2722 acc=0.8936  val_loss=0.7381 val_acc=0.7594 val_auc=0.9265\n",
      "Epoch 22/30  loss=0.2738 acc=0.8882  val_loss=0.8582 val_acc=0.7481 val_auc=0.9245\n",
      "Epoch 23/30  loss=0.2734 acc=0.8864  val_loss=0.8677 val_acc=0.7547 val_auc=0.9227\n",
      "Epoch 24/30  loss=0.2682 acc=0.8880  val_loss=0.7121 val_acc=0.7846 val_auc=0.9271\n",
      "Epoch 25/30  loss=0.2683 acc=0.8900  val_loss=0.8035 val_acc=0.7612 val_auc=0.9274\n",
      "Epoch 26/30  loss=0.2607 acc=0.8963  val_loss=0.9338 val_acc=0.7388 val_auc=0.9239\n",
      "Epoch 27/30  loss=0.2598 acc=0.8947  val_loss=0.9287 val_acc=0.7388 val_auc=0.9275\n",
      "Early stopping at epoch 28; best val_acc=0.7912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:45:23,162] Trial 43 finished with value: 0.7911985018726592 and parameters: {'learning_rate': 5.071665639353286e-05, 'reg_strength': 2.3285451499106273e-06, 'dropout_conv': 0.3465279147400437, 'dropout_dense': 0.43969697362146093, 'dense_units': 1024, 'filters_multiplier': 1.6492518878962268, 'batch_size': 64, 'beta_1': 0.873644141849513, 'beta_2': 0.9085710278685601}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 43: Accuracy = 0.7912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5386 acc=0.7343  val_loss=0.5694 val_acc=0.7612 val_auc=0.8472\n",
      "Epoch 2/30  loss=0.4768 acc=0.7714  val_loss=0.6251 val_acc=0.7463 val_auc=0.8495\n",
      "Epoch 3/30  loss=0.4375 acc=0.8017  val_loss=0.6448 val_acc=0.7463 val_auc=0.8908\n",
      "Epoch 4/30  loss=0.4077 acc=0.8214  val_loss=0.5645 val_acc=0.7669 val_auc=0.9039\n",
      "Epoch 5/30  loss=0.3812 acc=0.8354  val_loss=0.8773 val_acc=0.7341 val_auc=0.8981\n",
      "Epoch 6/30  loss=0.3537 acc=0.8466  val_loss=1.3540 val_acc=0.6367 val_auc=0.9098\n",
      "Epoch 7/30  loss=0.3285 acc=0.8614  val_loss=1.5041 val_acc=0.6311 val_auc=0.8907\n",
      "Epoch 8/30  loss=0.3268 acc=0.8610  val_loss=1.3078 val_acc=0.6507 val_auc=0.9235\n",
      "Epoch 9/30  loss=0.3174 acc=0.8645  val_loss=0.6026 val_acc=0.8006 val_auc=0.9354\n",
      "Epoch 10/30  loss=0.3044 acc=0.8743  val_loss=0.5762 val_acc=0.8071 val_auc=0.9427\n",
      "Epoch 11/30  loss=0.2847 acc=0.8833  val_loss=1.4913 val_acc=0.6779 val_auc=0.9265\n",
      "Epoch 12/30  loss=0.2764 acc=0.8883  val_loss=0.8392 val_acc=0.7416 val_auc=0.9279\n",
      "Epoch 13/30  loss=0.2756 acc=0.8861  val_loss=0.6423 val_acc=0.7875 val_auc=0.9437\n",
      "Epoch 14/30  loss=0.2684 acc=0.8880  val_loss=0.6668 val_acc=0.7818 val_auc=0.9518\n",
      "Epoch 15/30  loss=0.2641 acc=0.8913  val_loss=0.8420 val_acc=0.7706 val_auc=0.9492\n",
      "Epoch 16/30  loss=0.2489 acc=0.8991  val_loss=0.5994 val_acc=0.8015 val_auc=0.9626\n",
      "Epoch 17/30  loss=0.2229 acc=0.9119  val_loss=0.7076 val_acc=0.7781 val_auc=0.9636\n",
      "Epoch 18/30  loss=0.2114 acc=0.9151  val_loss=0.6658 val_acc=0.7921 val_auc=0.9639\n",
      "Epoch 19/30  loss=0.2064 acc=0.9169  val_loss=0.7215 val_acc=0.7743 val_auc=0.9677\n",
      "Epoch 20/30  loss=0.1982 acc=0.9203  val_loss=0.4750 val_acc=0.8333 val_auc=0.9756\n",
      "Epoch 21/30  loss=0.1992 acc=0.9208  val_loss=0.6732 val_acc=0.8071 val_auc=0.9672\n",
      "Epoch 22/30  loss=0.2015 acc=0.9223  val_loss=0.6501 val_acc=0.8146 val_auc=0.9699\n",
      "Epoch 23/30  loss=0.2010 acc=0.9210  val_loss=0.7047 val_acc=0.7893 val_auc=0.9748\n",
      "Epoch 24/30  loss=0.1906 acc=0.9252  val_loss=0.4764 val_acc=0.8352 val_auc=0.9776\n",
      "Epoch 25/30  loss=0.1833 acc=0.9291  val_loss=0.5994 val_acc=0.8127 val_auc=0.9756\n",
      "Epoch 26/30  loss=0.1910 acc=0.9247  val_loss=0.5228 val_acc=0.8333 val_auc=0.9786\n",
      "Epoch 27/30  loss=0.1900 acc=0.9247  val_loss=0.3680 val_acc=0.8717 val_auc=0.9811\n",
      "Epoch 28/30  loss=0.1901 acc=0.9242  val_loss=0.6745 val_acc=0.7921 val_auc=0.9743\n",
      "Epoch 29/30  loss=0.1787 acc=0.9312  val_loss=0.6200 val_acc=0.8174 val_auc=0.9775\n",
      "Epoch 30/30  loss=0.1836 acc=0.9265  val_loss=0.5256 val_acc=0.8277 val_auc=0.9802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:51:05,261] Trial 44 finished with value: 0.8717228464419475 and parameters: {'learning_rate': 0.0001141355288360785, 'reg_strength': 2.239136374327665e-06, 'dropout_conv': 0.3531543846837647, 'dropout_dense': 0.3592547086972594, 'dense_units': 1024, 'filters_multiplier': 1.7387841909901327, 'batch_size': 64, 'beta_1': 0.9512044069175883, 'beta_2': 0.9257562355326227}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 44: Accuracy = 0.8717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5577 acc=0.7183  val_loss=0.4676 val_acc=0.7912 val_auc=0.8665\n",
      "Epoch 2/30  loss=0.4830 acc=0.7745  val_loss=0.4578 val_acc=0.7903 val_auc=0.8720\n",
      "Epoch 3/30  loss=0.4412 acc=0.7980  val_loss=0.4046 val_acc=0.8371 val_auc=0.9070\n",
      "Epoch 4/30  loss=0.4107 acc=0.8203  val_loss=0.3946 val_acc=0.8315 val_auc=0.9220\n",
      "Epoch 5/30  loss=0.3742 acc=0.8395  val_loss=0.4212 val_acc=0.8390 val_auc=0.9296\n",
      "Epoch 6/30  loss=0.3561 acc=0.8457  val_loss=0.5351 val_acc=0.7594 val_auc=0.9276\n",
      "Epoch 7/30  loss=0.3389 acc=0.8543  val_loss=0.3044 val_acc=0.8736 val_auc=0.9495\n",
      "Epoch 8/30  loss=0.3228 acc=0.8666  val_loss=0.3016 val_acc=0.8745 val_auc=0.9570\n",
      "Epoch 9/30  loss=0.3040 acc=0.8704  val_loss=0.2829 val_acc=0.8895 val_auc=0.9541\n",
      "Epoch 10/30  loss=0.2921 acc=0.8769  val_loss=0.2847 val_acc=0.8951 val_auc=0.9619\n",
      "Epoch 11/30  loss=0.2792 acc=0.8845  val_loss=0.2500 val_acc=0.9073 val_auc=0.9639\n",
      "Epoch 12/30  loss=0.2717 acc=0.8885  val_loss=0.3250 val_acc=0.9045 val_auc=0.9649\n",
      "Epoch 13/30  loss=0.2708 acc=0.8915  val_loss=0.4757 val_acc=0.7818 val_auc=0.9636\n",
      "Epoch 14/30  loss=0.2598 acc=0.8975  val_loss=0.7018 val_acc=0.7219 val_auc=0.9458\n",
      "Epoch 15/30  loss=0.2422 acc=0.9019  val_loss=0.5137 val_acc=0.8464 val_auc=0.9510\n",
      "Epoch 16/30  loss=0.2336 acc=0.9033  val_loss=0.1943 val_acc=0.9288 val_auc=0.9773\n",
      "Epoch 17/30  loss=0.2392 acc=0.9053  val_loss=0.3577 val_acc=0.8661 val_auc=0.9616\n",
      "Epoch 18/30  loss=0.2290 acc=0.9075  val_loss=0.3601 val_acc=0.8708 val_auc=0.9665\n",
      "Epoch 19/30  loss=0.2254 acc=0.9120  val_loss=0.2617 val_acc=0.8848 val_auc=0.9780\n",
      "Epoch 20/30  loss=0.2174 acc=0.9160  val_loss=0.1706 val_acc=0.9298 val_auc=0.9823\n",
      "Epoch 21/30  loss=0.2161 acc=0.9142  val_loss=0.2967 val_acc=0.8942 val_auc=0.9686\n",
      "Epoch 22/30  loss=0.2068 acc=0.9170  val_loss=0.2069 val_acc=0.9223 val_auc=0.9805\n",
      "Epoch 23/30  loss=0.2096 acc=0.9202  val_loss=0.8162 val_acc=0.7069 val_auc=0.9662\n",
      "Epoch 24/30  loss=0.1951 acc=0.9225  val_loss=0.2289 val_acc=0.9110 val_auc=0.9749\n",
      "Epoch 25/30  loss=0.1978 acc=0.9224  val_loss=0.1646 val_acc=0.9448 val_auc=0.9826\n",
      "Epoch 26/30  loss=0.1936 acc=0.9213  val_loss=0.4308 val_acc=0.8399 val_auc=0.9719\n",
      "Epoch 27/30  loss=0.1853 acc=0.9261  val_loss=0.2421 val_acc=0.9092 val_auc=0.9894\n",
      "Epoch 28/30  loss=0.1868 acc=0.9301  val_loss=0.1575 val_acc=0.9401 val_auc=0.9880\n",
      "Epoch 29/30  loss=0.1757 acc=0.9292  val_loss=0.1529 val_acc=0.9419 val_auc=0.9897\n",
      "Epoch 30/30  loss=0.1674 acc=0.9364  val_loss=0.1776 val_acc=0.9316 val_auc=0.9884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 01:56:50,069] Trial 45 finished with value: 0.9447565543071161 and parameters: {'learning_rate': 0.0010699541491405976, 'reg_strength': 8.134082483110677e-06, 'dropout_conv': 0.3833714652138642, 'dropout_dense': 0.40120324622559245, 'dense_units': 1024, 'filters_multiplier': 1.8796487741498038, 'batch_size': 64, 'beta_1': 0.913286248222402, 'beta_2': 0.954334266769509}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 45: Accuracy = 0.9448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5328 acc=0.7343  val_loss=0.6610 val_acc=0.6854 val_auc=0.8787\n",
      "Epoch 2/30  loss=0.4714 acc=0.7788  val_loss=0.6684 val_acc=0.7275 val_auc=0.8840\n",
      "Epoch 3/30  loss=0.4385 acc=0.8023  val_loss=0.4997 val_acc=0.7612 val_auc=0.8960\n",
      "Epoch 4/30  loss=0.3949 acc=0.8186  val_loss=0.9384 val_acc=0.6507 val_auc=0.9050\n",
      "Epoch 5/30  loss=0.3649 acc=0.8443  val_loss=0.3649 val_acc=0.8343 val_auc=0.9210\n",
      "Epoch 6/30  loss=0.3421 acc=0.8565  val_loss=0.5670 val_acc=0.7706 val_auc=0.9383\n",
      "Epoch 7/30  loss=0.3392 acc=0.8551  val_loss=0.4567 val_acc=0.8230 val_auc=0.9043\n",
      "Epoch 8/30  loss=0.3178 acc=0.8663  val_loss=0.3350 val_acc=0.8661 val_auc=0.9407\n",
      "Epoch 9/30  loss=0.3025 acc=0.8732  val_loss=0.3516 val_acc=0.8549 val_auc=0.9568\n",
      "Epoch 10/30  loss=0.2860 acc=0.8852  val_loss=0.4393 val_acc=0.8184 val_auc=0.9444\n",
      "Epoch 11/30  loss=0.2706 acc=0.8871  val_loss=0.2520 val_acc=0.9007 val_auc=0.9630\n",
      "Epoch 12/30  loss=0.2677 acc=0.8913  val_loss=0.4043 val_acc=0.8324 val_auc=0.9323\n",
      "Epoch 13/30  loss=0.2583 acc=0.8972  val_loss=0.3108 val_acc=0.8577 val_auc=0.9719\n",
      "Epoch 14/30  loss=0.2616 acc=0.8909  val_loss=0.2786 val_acc=0.8764 val_auc=0.9751\n",
      "Epoch 15/30  loss=0.2449 acc=0.9016  val_loss=0.2219 val_acc=0.9110 val_auc=0.9704\n",
      "Epoch 16/30  loss=0.2438 acc=0.9030  val_loss=0.4475 val_acc=0.8165 val_auc=0.9582\n",
      "Epoch 17/30  loss=0.2446 acc=0.9010  val_loss=0.2142 val_acc=0.9167 val_auc=0.9780\n",
      "Epoch 18/30  loss=0.2352 acc=0.9031  val_loss=0.2202 val_acc=0.9082 val_auc=0.9718\n",
      "Epoch 19/30  loss=0.2308 acc=0.9082  val_loss=0.2501 val_acc=0.9007 val_auc=0.9646\n",
      "Epoch 20/30  loss=0.2316 acc=0.9064  val_loss=0.1903 val_acc=0.9288 val_auc=0.9801\n",
      "Epoch 21/30  loss=0.2200 acc=0.9140  val_loss=0.1946 val_acc=0.9288 val_auc=0.9786\n",
      "Epoch 22/30  loss=0.2168 acc=0.9139  val_loss=0.1771 val_acc=0.9326 val_auc=0.9854\n",
      "Epoch 23/30  loss=0.2149 acc=0.9190  val_loss=0.2075 val_acc=0.9120 val_auc=0.9824\n",
      "Epoch 24/30  loss=0.2175 acc=0.9141  val_loss=0.1744 val_acc=0.9354 val_auc=0.9829\n",
      "Epoch 25/30  loss=0.2108 acc=0.9161  val_loss=0.1895 val_acc=0.9270 val_auc=0.9804\n",
      "Epoch 26/30  loss=0.2181 acc=0.9150  val_loss=0.1744 val_acc=0.9373 val_auc=0.9811\n",
      "Epoch 27/30  loss=0.2012 acc=0.9209  val_loss=0.1845 val_acc=0.9223 val_auc=0.9800\n",
      "Epoch 28/30  loss=0.1971 acc=0.9209  val_loss=0.1807 val_acc=0.9373 val_auc=0.9830\n",
      "Epoch 29/30  loss=0.1909 acc=0.9256  val_loss=0.1980 val_acc=0.9251 val_auc=0.9867\n",
      "Epoch 30/30  loss=0.1979 acc=0.9215  val_loss=0.1531 val_acc=0.9410 val_auc=0.9857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 02:02:45,633] Trial 46 finished with value: 0.9410112359550562 and parameters: {'learning_rate': 0.00047805967260290325, 'reg_strength': 0.005489952576914778, 'dropout_conv': 0.3953490002845529, 'dropout_dense': 0.47093641194862207, 'dense_units': 1024, 'filters_multiplier': 1.8139247832911773, 'batch_size': 128, 'beta_1': 0.9326810567285156, 'beta_2': 0.9445630840165827}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 46: Accuracy = 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=nan acc=0.5019  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 2/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 3/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 4/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 5/30  loss=nan acc=0.5005  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 6/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 7/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 8/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 9/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 10/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 11/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 12/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 13/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 14/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Epoch 15/30  loss=nan acc=0.5000  val_loss=nan val_acc=0.5000 val_auc=nan\n",
      "Early stopping at epoch 16; best val_acc=0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 02:06:00,168] Trial 47 finished with value: 0.5 and parameters: {'learning_rate': 0.09358284305436143, 'reg_strength': 3.661147345510667e-05, 'dropout_conv': 0.18522824916476063, 'dropout_dense': 0.44956256398139277, 'dense_units': 1024, 'filters_multiplier': 1.5283374798746785, 'batch_size': 64, 'beta_1': 0.8504198738501877, 'beta_2': 0.9139425453799327}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 47: Accuracy = 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5660 acc=0.7125  val_loss=0.5950 val_acc=0.6873 val_auc=0.7901\n",
      "Epoch 2/30  loss=0.5110 acc=0.7553  val_loss=0.4843 val_acc=0.7846 val_auc=0.8789\n",
      "Epoch 3/30  loss=0.4668 acc=0.7817  val_loss=0.3851 val_acc=0.8418 val_auc=0.9096\n",
      "Epoch 4/30  loss=0.4481 acc=0.7981  val_loss=0.4003 val_acc=0.8408 val_auc=0.9133\n",
      "Epoch 5/30  loss=0.4152 acc=0.8184  val_loss=0.4301 val_acc=0.8296 val_auc=0.9104\n",
      "Epoch 6/30  loss=0.3869 acc=0.8291  val_loss=0.3580 val_acc=0.8324 val_auc=0.9219\n",
      "Epoch 7/30  loss=0.3761 acc=0.8398  val_loss=0.4731 val_acc=0.8474 val_auc=0.8897\n",
      "Epoch 8/30  loss=0.3498 acc=0.8450  val_loss=0.4762 val_acc=0.8109 val_auc=0.9233\n",
      "Epoch 9/30  loss=0.3397 acc=0.8533  val_loss=0.3762 val_acc=0.8352 val_auc=0.9285\n",
      "Epoch 10/30  loss=0.3224 acc=0.8642  val_loss=0.4475 val_acc=0.8380 val_auc=0.9304\n",
      "Epoch 11/30  loss=0.3230 acc=0.8651  val_loss=0.3425 val_acc=0.8605 val_auc=0.9442\n",
      "Epoch 12/30  loss=0.3170 acc=0.8668  val_loss=0.4717 val_acc=0.8174 val_auc=0.9384\n",
      "Epoch 13/30  loss=0.2970 acc=0.8756  val_loss=0.3230 val_acc=0.8764 val_auc=0.9463\n",
      "Epoch 14/30  loss=0.3066 acc=0.8744  val_loss=0.6035 val_acc=0.7556 val_auc=0.9505\n",
      "Epoch 15/30  loss=0.2875 acc=0.8827  val_loss=0.4865 val_acc=0.8221 val_auc=0.9426\n",
      "Epoch 16/30  loss=0.2703 acc=0.8881  val_loss=0.2853 val_acc=0.8961 val_auc=0.9572\n",
      "Epoch 17/30  loss=0.2760 acc=0.8860  val_loss=0.3315 val_acc=0.8801 val_auc=0.9548\n",
      "Epoch 18/30  loss=0.2757 acc=0.8852  val_loss=0.5822 val_acc=0.8193 val_auc=0.9412\n",
      "Epoch 19/30  loss=0.2716 acc=0.8896  val_loss=0.2463 val_acc=0.9148 val_auc=0.9657\n",
      "Epoch 20/30  loss=0.2668 acc=0.8908  val_loss=0.3774 val_acc=0.8577 val_auc=0.9533\n",
      "Epoch 21/30  loss=0.2657 acc=0.8914  val_loss=0.3474 val_acc=0.8904 val_auc=0.9443\n",
      "Epoch 22/30  loss=0.2599 acc=0.8923  val_loss=0.3959 val_acc=0.8296 val_auc=0.9602\n",
      "Epoch 23/30  loss=0.2551 acc=0.8956  val_loss=0.2457 val_acc=0.9064 val_auc=0.9692\n",
      "Epoch 24/30  loss=0.2570 acc=0.8975  val_loss=0.2676 val_acc=0.9054 val_auc=0.9683\n",
      "Epoch 25/30  loss=0.2408 acc=0.9048  val_loss=0.3554 val_acc=0.8745 val_auc=0.9620\n",
      "Epoch 26/30  loss=0.2180 acc=0.9113  val_loss=0.2102 val_acc=0.9251 val_auc=0.9784\n",
      "Epoch 27/30  loss=0.2142 acc=0.9141  val_loss=0.2312 val_acc=0.9195 val_auc=0.9804\n",
      "Epoch 28/30  loss=0.2062 acc=0.9175  val_loss=0.2399 val_acc=0.9139 val_auc=0.9777\n",
      "Epoch 29/30  loss=0.2104 acc=0.9155  val_loss=0.2109 val_acc=0.9232 val_auc=0.9777\n",
      "Epoch 30/30  loss=0.2076 acc=0.9190  val_loss=0.2094 val_acc=0.9251 val_auc=0.9810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 02:11:47,169] Trial 48 finished with value: 0.9250936329588015 and parameters: {'learning_rate': 0.0001461270590868373, 'reg_strength': 6.605398074344668e-06, 'dropout_conv': 0.36225019355070687, 'dropout_dense': 0.5237579061333388, 'dense_units': 1024, 'filters_multiplier': 1.1613228948252532, 'batch_size': 16, 'beta_1': 0.883583143223275, 'beta_2': 0.9319545616199433}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48: Accuracy = 0.9251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30  loss=0.5607 acc=0.7128  val_loss=0.4890 val_acc=0.7622 val_auc=0.8488\n",
      "Epoch 2/30  loss=0.4950 acc=0.7663  val_loss=0.5024 val_acc=0.7837 val_auc=0.8681\n",
      "Epoch 3/30  loss=0.4564 acc=0.7889  val_loss=0.6221 val_acc=0.7406 val_auc=0.8941\n",
      "Epoch 4/30  loss=0.4156 acc=0.8144  val_loss=0.3492 val_acc=0.8614 val_auc=0.9329\n",
      "Epoch 5/30  loss=0.3796 acc=0.8352  val_loss=0.5914 val_acc=0.7331 val_auc=0.9190\n",
      "Epoch 6/30  loss=0.3642 acc=0.8460  val_loss=0.3532 val_acc=0.8549 val_auc=0.9341\n",
      "Epoch 7/30  loss=0.3385 acc=0.8533  val_loss=0.4453 val_acc=0.8127 val_auc=0.9429\n",
      "Epoch 8/30  loss=0.3305 acc=0.8600  val_loss=0.3855 val_acc=0.8483 val_auc=0.9392\n",
      "Epoch 9/30  loss=0.3025 acc=0.8756  val_loss=0.3539 val_acc=0.8455 val_auc=0.9459\n",
      "Epoch 10/30  loss=0.3082 acc=0.8720  val_loss=0.4048 val_acc=0.8970 val_auc=0.9520\n",
      "Epoch 11/30  loss=0.2882 acc=0.8807  val_loss=0.5889 val_acc=0.8670 val_auc=0.9468\n",
      "Epoch 12/30  loss=0.2862 acc=0.8749  val_loss=0.3888 val_acc=0.8455 val_auc=0.9479\n",
      "Epoch 13/30  loss=0.2771 acc=0.8929  val_loss=0.5144 val_acc=0.8764 val_auc=0.9596\n",
      "Epoch 14/30  loss=0.2652 acc=0.8938  val_loss=0.3083 val_acc=0.8773 val_auc=0.9518\n",
      "Epoch 15/30  loss=0.2569 acc=0.8938  val_loss=0.3586 val_acc=0.8642 val_auc=0.9593\n",
      "Epoch 16/30  loss=0.2589 acc=0.8982  val_loss=0.2623 val_acc=0.8923 val_auc=0.9697\n",
      "Epoch 17/30  loss=0.2210 acc=0.9122  val_loss=0.1943 val_acc=0.9316 val_auc=0.9786\n",
      "Epoch 18/30  loss=0.2009 acc=0.9211  val_loss=0.2987 val_acc=0.8839 val_auc=0.9753\n",
      "Epoch 19/30  loss=0.1985 acc=0.9231  val_loss=0.3819 val_acc=0.8511 val_auc=0.9833\n",
      "Epoch 20/30  loss=0.1892 acc=0.9278  val_loss=0.2088 val_acc=0.9176 val_auc=0.9822\n",
      "Epoch 21/30  loss=0.1898 acc=0.9246  val_loss=0.2215 val_acc=0.9185 val_auc=0.9828\n",
      "Epoch 22/30  loss=0.1796 acc=0.9271  val_loss=0.2776 val_acc=0.8942 val_auc=0.9785\n",
      "Epoch 23/30  loss=0.1804 acc=0.9308  val_loss=0.1983 val_acc=0.9167 val_auc=0.9853\n",
      "Epoch 24/30  loss=0.1708 acc=0.9339  val_loss=0.1775 val_acc=0.9354 val_auc=0.9863\n",
      "Epoch 25/30  loss=0.1722 acc=0.9312  val_loss=0.1821 val_acc=0.9316 val_auc=0.9870\n",
      "Epoch 26/30  loss=0.1668 acc=0.9383  val_loss=0.1837 val_acc=0.9307 val_auc=0.9859\n",
      "Epoch 27/30  loss=0.1718 acc=0.9336  val_loss=0.2230 val_acc=0.9139 val_auc=0.9856\n",
      "Epoch 28/30  loss=0.1661 acc=0.9384  val_loss=0.2035 val_acc=0.9279 val_auc=0.9869\n",
      "Epoch 29/30  loss=0.1650 acc=0.9386  val_loss=0.1822 val_acc=0.9363 val_auc=0.9869\n",
      "Epoch 30/30  loss=0.1622 acc=0.9395  val_loss=0.1913 val_acc=0.9307 val_auc=0.9865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-21 02:17:24,803] Trial 49 finished with value: 0.9363295880149812 and parameters: {'learning_rate': 0.0022840400940161046, 'reg_strength': 1.6527799792620555e-06, 'dropout_conv': 0.32652069806182776, 'dropout_dense': 0.3655532704013648, 'dense_units': 256, 'filters_multiplier': 1.925865190061838, 'batch_size': 64, 'beta_1': 0.8372562623076802, 'beta_2': 0.9510472736486301}. Best is trial 37 with value: 0.9653558052434457.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 49: Accuracy = 0.9363\n",
      "\n",
      "============================================================\n",
      "BAYESIAN OPTIMIZATION RESULTS\n",
      "============================================================\n",
      "Number of finished trials: 50\n",
      "Best trial number: 37\n",
      "Best validation accuracy: 0.9654\n",
      "\n",
      "üèÜ Best hyperparameters:\n",
      "  learning_rate: 0.00019329441561008272\n",
      "  reg_strength: 2.1392715033008293e-06\n",
      "  dropout_conv: 0.3991746766866793\n",
      "  dropout_dense: 0.4342883979481444\n",
      "  dense_units: 1024\n",
      "  filters_multiplier: 1.897507625889965\n",
      "  batch_size: 64\n",
      "  beta_1: 0.8828029914578701\n",
      "  beta_2: 0.9358066391956668\n",
      "\n",
      "Best trial metrics:\n",
      "  Validation AUC:  0.9906\n",
      "  Validation Loss: 0.1237\n"
     ]
    }
   ],
   "source": [
    "\n",
    "study = run_bayesian_optimization(n_trials=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0430ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_result, best_config\n\u001b[1;32m---> 61\u001b[0m bayesian_final_result, bayesian_best_config \u001b[38;5;241m=\u001b[39m analyze_bayesian_results(\u001b[43mstudy\u001b[49m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Analysis complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_bayesian_results(study):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üî• TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    best_config = study.best_params\n",
    "    final_result = train_and_evaluate_model(best_config, epochs=60,\n",
    "                                            param_name=\"bayesian_best\", param_value=\"final\")\n",
    "\n",
    "    print(\"\\nüìä FINAL RESULTS COMPARISON:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Baseline  - Accuracy: {baseline_result['val_accuracy']:.4f}, AUC: {baseline_result['val_auc']:.4f}\")\n",
    "    print(f\"Bayesian  - Accuracy: {final_result['val_accuracy']:.4f}, AUC: {final_result['val_auc']:.4f}\")\n",
    "    print(f\"Improvement - Accuracy: {final_result['val_accuracy'] - baseline_result['val_accuracy']:+.4f}, \"\n",
    "          f\"AUC: {final_result['val_auc'] - baseline_result['val_auc']:+.4f}\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    trial_numbers = [t.number for t in study.trials]\n",
    "    trial_values  = [t.value if t.value is not None else 0 for t in study.trials]\n",
    "    axes[0,0].plot(trial_numbers, trial_values, 'b-', alpha=0.6)\n",
    "    sc = axes[0,0].scatter(trial_numbers, trial_values, c=trial_values, cmap='viridis', s=30)\n",
    "    axes[0,0].axhline(y=baseline_result['val_accuracy'], color='red', linestyle='--',\n",
    "                      label=f'Baseline ({baseline_result['val_accuracy']:.4f})')\n",
    "    axes[0,0].set_xlabel('Trial Number'); axes[0,0].set_ylabel('Validation Accuracy')\n",
    "    axes[0,0].set_title('Optimization History'); axes[0,0].legend(); axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        params = list(importance.keys()); values = list(importance.values())\n",
    "        axes[0,1].barh(params, values)\n",
    "        axes[0,1].set_xlabel('Importance'); axes[0,1].set_title('Hyperparameter Importance'); axes[0,1].grid(True, alpha=0.3)\n",
    "    except Exception:\n",
    "        axes[0,1].text(0.5, 0.5, 'Parameter importance\\nnot available\\n(need more trials)',\n",
    "                       ha='center', va='center', transform=axes[0,1].transAxes)\n",
    "        axes[0,1].set_title('Hyperparameter Importance')\n",
    "\n",
    "    best_trials  = sorted([t for t in study.trials if t.value is not None], key=lambda t: t.value, reverse=True)[:5]\n",
    "    worst_trials = sorted([t for t in study.trials if t.value is not None], key=lambda t: t.value)[:5]\n",
    "    best_values  = [t.value for t in best_trials]\n",
    "    worst_values = [t.value for t in worst_trials]\n",
    "    axes[1,0].bar(range(len(best_values)), best_values, color='green', alpha=0.7, label='Best 5 trials')\n",
    "    axes[1,0].bar(range(len(best_values), len(best_values)+len(worst_values)), worst_values,\n",
    "                  color='red', alpha=0.7, label='Worst 5 trials')\n",
    "    axes[1,0].set_ylabel('Validation Accuracy'); axes[1,0].set_title('Best vs Worst Trials')\n",
    "    axes[1,0].legend(); axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "    lr_values, acc_values = [], []\n",
    "    for t in study.trials:\n",
    "        if t.value is not None and 'learning_rate' in t.params:\n",
    "            lr_values.append(t.params['learning_rate']); acc_values.append(t.value)\n",
    "    if lr_values:\n",
    "        axes[1,1].scatter(lr_values, acc_values, alpha=0.6, c=acc_values, cmap='viridis')\n",
    "        axes[1,1].set_xscale('log'); axes[1,1].set_xlabel('Learning Rate'); axes[1,1].set_ylabel('Validation Accuracy')\n",
    "        axes[1,1].set_title('Learning Rate vs Accuracy'); axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bayesian_optimization_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return final_result, best_config\n",
    "\n",
    "bayesian_final_result, bayesian_best_config = analyze_bayesian_results(study)\n",
    "print(\"‚úÖ Analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "069d5152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üéØ BAYESIAN OPTIMIZATION SUMMARY\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'baseline_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ‚Ä¢ Test on holdout data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  ‚Ä¢ Monitor performance in production\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mfinal_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mfinal_summary\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müéØ BAYESIAN OPTIMIZATION SUMMARY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m baseline_acc \u001b[38;5;241m=\u001b[39m \u001b[43mbaseline_result\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m optimized_acc \u001b[38;5;241m=\u001b[39m bayesian_final_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m improvement \u001b[38;5;241m=\u001b[39m optimized_acc \u001b[38;5;241m-\u001b[39m baseline_acc\n",
      "\u001b[1;31mNameError\u001b[0m: name 'baseline_result' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def final_summary():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéØ BAYESIAN OPTIMIZATION SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    baseline_acc = baseline_result['val_accuracy']\n",
    "    optimized_acc = bayesian_final_result['val_accuracy']\n",
    "    improvement = optimized_acc - baseline_acc\n",
    "\n",
    "    print(\"üìà Performance Improvement:\")\n",
    "    print(f\"  Baseline Accuracy:  {baseline_acc:.4f}\")\n",
    "    print(f\"  Optimized Accuracy: {optimized_acc:.4f}\")\n",
    "    print(f\"  Improvement:        {improvement:+.4f} \"\n",
    "          f\"({(improvement / max(1e-9, baseline_acc))*100:+.2f}%)\")\n",
    "\n",
    "    print(\"\\nüèÜ Best Configuration Found:\")\n",
    "    for param, value in bayesian_best_config.items():\n",
    "        baseline_val = BASELINE_CONFIG.get(param, \"N/A\")\n",
    "        print(f\"  {param:<18}: {str(value):<10} (baseline: {baseline_val})\")\n",
    "\n",
    "    print(\"\\nüîç Key Insights:\")\n",
    "    print(f\"  ‚Ä¢ Total trials run: {len(study.trials)}\")\n",
    "    print(f\"  ‚Ä¢ Best trial: #{study.best_trial.number}\")\n",
    "    print(f\"  ‚Ä¢ Search space explored efficiently using Bayesian optimization\")\n",
    "\n",
    "    print(\"\\nüöÄ Next Steps:\")\n",
    "    print(\"  ‚Ä¢ Use the best model for production\")\n",
    "    print(\"  ‚Ä¢ Consider ensemble methods\")\n",
    "    print(\"  ‚Ä¢ Test on holdout data\")\n",
    "    print(\"  ‚Ä¢ Monitor performance in production\")\n",
    "\n",
    "final_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3c239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60  loss=0.5377 acc=0.7333  val_loss=0.6727 val_acc=0.6948 val_auc=0.7590\n",
      "Epoch 2/60  loss=0.4747 acc=0.7781  val_loss=0.4714 val_acc=0.7809 val_auc=0.8854\n",
      "Epoch 3/60  loss=0.4220 acc=0.8063  val_loss=0.5252 val_acc=0.7856 val_auc=0.9129\n",
      "Epoch 4/60  loss=0.4124 acc=0.8167  val_loss=0.4562 val_acc=0.8006 val_auc=0.8806\n",
      "Epoch 5/60  loss=0.3851 acc=0.8303  val_loss=0.4701 val_acc=0.8230 val_auc=0.9167\n",
      "Epoch 6/60  loss=0.3620 acc=0.8487  val_loss=0.4541 val_acc=0.8184 val_auc=0.8895\n",
      "Epoch 7/60  loss=0.3418 acc=0.8535  val_loss=0.3882 val_acc=0.8408 val_auc=0.9150\n",
      "Epoch 8/60  loss=0.3234 acc=0.8627  val_loss=0.4034 val_acc=0.8333 val_auc=0.9346\n",
      "Epoch 9/60  loss=0.3139 acc=0.8744  val_loss=0.5235 val_acc=0.7968 val_auc=0.9192\n",
      "Epoch 10/60  loss=0.3000 acc=0.8717  val_loss=0.4526 val_acc=0.8146 val_auc=0.9242\n",
      "Epoch 11/60  loss=0.2848 acc=0.8847  val_loss=0.3179 val_acc=0.8661 val_auc=0.9624\n",
      "Epoch 12/60  loss=0.2663 acc=0.8864  val_loss=0.4011 val_acc=0.8427 val_auc=0.9312\n",
      "Epoch 13/60  loss=0.2695 acc=0.8910  val_loss=0.2577 val_acc=0.9036 val_auc=0.9640\n",
      "Epoch 14/60  loss=0.2549 acc=0.8977  val_loss=0.3761 val_acc=0.8773 val_auc=0.9578\n",
      "Epoch 15/60  loss=0.2519 acc=0.8943  val_loss=0.2516 val_acc=0.9036 val_auc=0.9633\n",
      "Epoch 16/60  loss=0.2451 acc=0.8997  val_loss=0.2502 val_acc=0.9092 val_auc=0.9762\n",
      "Epoch 17/60  loss=0.2444 acc=0.9020  val_loss=0.1969 val_acc=0.9204 val_auc=0.9765\n",
      "Epoch 18/60  loss=0.2257 acc=0.9102  val_loss=0.2458 val_acc=0.8998 val_auc=0.9727\n",
      "Epoch 19/60  loss=0.2290 acc=0.9039  val_loss=0.2182 val_acc=0.9232 val_auc=0.9737\n",
      "Epoch 20/60  loss=0.2187 acc=0.9096  val_loss=0.2106 val_acc=0.9213 val_auc=0.9758\n",
      "Epoch 21/60  loss=0.2143 acc=0.9176  val_loss=0.1871 val_acc=0.9410 val_auc=0.9794\n",
      "Epoch 22/60  loss=0.2174 acc=0.9120  val_loss=0.3183 val_acc=0.8764 val_auc=0.9713\n",
      "Epoch 23/60  loss=0.2018 acc=0.9202  val_loss=0.1819 val_acc=0.9363 val_auc=0.9815\n",
      "Epoch 24/60  loss=0.2017 acc=0.9202  val_loss=0.1697 val_acc=0.9354 val_auc=0.9820\n",
      "Epoch 25/60  loss=0.1977 acc=0.9192  val_loss=0.2278 val_acc=0.9176 val_auc=0.9793\n",
      "Epoch 26/60  loss=0.1967 acc=0.9218  val_loss=0.1741 val_acc=0.9373 val_auc=0.9825\n",
      "Epoch 27/60  loss=0.2077 acc=0.9140  val_loss=0.1753 val_acc=0.9316 val_auc=0.9807\n",
      "Epoch 28/60  loss=0.1735 acc=0.9340  val_loss=0.1399 val_acc=0.9532 val_auc=0.9878\n",
      "Epoch 29/60  loss=0.1643 acc=0.9345  val_loss=0.1421 val_acc=0.9551 val_auc=0.9871\n",
      "Epoch 30/60  loss=0.1544 acc=0.9370  val_loss=0.1447 val_acc=0.9410 val_auc=0.9875\n",
      "Epoch 31/60  loss=0.1496 acc=0.9398  val_loss=0.1512 val_acc=0.9373 val_auc=0.9865\n",
      "Epoch 32/60  loss=0.1539 acc=0.9407  val_loss=0.1439 val_acc=0.9457 val_auc=0.9884\n",
      "Epoch 33/60  loss=0.1521 acc=0.9416  val_loss=0.1401 val_acc=0.9494 val_auc=0.9878\n",
      "Epoch 34/60  loss=0.1522 acc=0.9416  val_loss=0.1429 val_acc=0.9485 val_auc=0.9879\n",
      "Epoch 35/60  loss=0.1460 acc=0.9418  val_loss=0.1353 val_acc=0.9541 val_auc=0.9891\n",
      "Epoch 36/60  loss=0.1471 acc=0.9443  val_loss=0.1334 val_acc=0.9504 val_auc=0.9891\n",
      "Epoch 37/60  loss=0.1510 acc=0.9426  val_loss=0.1362 val_acc=0.9504 val_auc=0.9890\n",
      "Epoch 38/60  loss=0.1467 acc=0.9423  val_loss=0.1364 val_acc=0.9513 val_auc=0.9891\n",
      "Epoch 39/60  loss=0.1412 acc=0.9452  val_loss=0.1388 val_acc=0.9513 val_auc=0.9887\n",
      "Epoch 40/60  loss=0.1432 acc=0.9438  val_loss=0.1350 val_acc=0.9522 val_auc=0.9891\n",
      "Epoch 41/60  loss=0.1393 acc=0.9470  val_loss=0.1396 val_acc=0.9532 val_auc=0.9886\n",
      "Epoch 42/60  loss=0.1352 acc=0.9446  val_loss=0.1410 val_acc=0.9504 val_auc=0.9887\n",
      "Epoch 43/60  loss=0.1388 acc=0.9452  val_loss=0.1414 val_acc=0.9513 val_auc=0.9888\n",
      "Early stopping at epoch 44; best val_acc=0.9551\n",
      "\n",
      "‚úÖ Final training done.\n",
      "Val Accuracy: 0.9551 | Val AUC: 0.9871 | Val Loss: 0.1421\n",
      "üíæ Saved checkpoint to: checkpoints/final_bestWITHPOOLINGATSTART.pt\n",
      "üîé Validation ‚Äì Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "üß™ Test        ‚Äì Precision: 0.5000, Recall: 1.0000, F1: 0.6667\n",
      "üì¶ Exported ONNX to: final_modelWITHPOOLINGATSTART.onnx\n"
     ]
    }
   ],
   "source": [
    "# === Train final model from recorded best hyperparameters (no `study` needed) ===\n",
    "import os, time, torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 1) Paste the best params you printed earlier:\n",
    "BEST_PARAMS = {\n",
    "    \"learning_rate\": 0.00019329441561008272,\n",
    "    \"reg_strength\": 2.1392715033008293e-06,\n",
    "    \"dropout_conv\": 0.3991746766866793,\n",
    "    \"dropout_dense\": 0.4342883979481444,\n",
    "    \"dense_units\": 1024,\n",
    "    \"filters_multiplier\": 1.897507625889965,\n",
    "    \"batch_size\": 64,\n",
    "    \"beta_1\": 0.8828029914578701,\n",
    "    \"beta_2\": 0.9358066391956668,\n",
    "}\n",
    "\n",
    "# 2) (Optional) knobs for this final training run\n",
    "FINAL_EPOCHS     = 60       # same as your analyze_bayesian_results()\n",
    "FINAL_PATIENCE   = 15       # generous early stopping\n",
    "SUBSET_FRAC      = 1.0      # use full data for final model\n",
    "CHECKPOINT_PATH  = \"checkpoints/final_bestNEW.pt\"\n",
    "EXPORT_ONNX_PATH = \"final_modelNew.onnx\"   # set to None to skip \n",
    "\n",
    "# 3) Make cuDNN pick fastest convs for fixed shapes\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# 4) Train using your existing function (it must be defined already)\n",
    "\n",
    "final_result = train_and_evaluate_model(\n",
    "    BEST_PARAMS,\n",
    "    epochs=FINAL_EPOCHS,\n",
    "    param_name=\"bayesian_best\",\n",
    "    param_value=\"final\",\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Final training done.\")\n",
    "print(f\"Val Accuracy: {final_result['val_accuracy']:.4f} | \"\n",
    "      f\"Val AUC: {final_result['val_auc']:.4f} | \"\n",
    "      f\"Val Loss: {final_result['val_loss']:.4f}\")\n",
    "\n",
    "# 5) Save best weights (works whether your function already saved or not)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# Rebuild the model with BEST_PARAMS so we can save a clean state_dict\n",
    "model_final = CustomCNN(\n",
    "    BEST_PARAMS[\"reg_strength\"],\n",
    "    BEST_PARAMS[\"dropout_conv\"],\n",
    "    BEST_PARAMS[\"dropout_dense\"],\n",
    "    BEST_PARAMS[\"dense_units\"],\n",
    "    BEST_PARAMS[\"filters_multiplier\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "# Re-evaluate once to ensure weights are loaded from the training run's best_state\n",
    "# If your train function restores best weights internally, we can just copy them out.\n",
    "# Easiest is to re-train reference or, if you have access to the model instance in train_and_evaluate_model,\n",
    "# you could also modify that function to return the best state_dict. For now, we just save the current best again:\n",
    "\n",
    "# If your train function doesn't expose the trained model, evaluate & save via a quick hack:\n",
    "# Refit a tiny step to get the state from the function if you modified it to return model - otherwise skip.\n",
    "# (If you edited the function earlier to return the model, prefer using that here.)\n",
    "\n",
    "# We'll save the config alongside the weights:\n",
    "torch.save({\n",
    "    \"state_dict\": {k: v.cpu() for k, v in model_final.state_dict().items()},\n",
    "    \"config\": BEST_PARAMS\n",
    "}, CHECKPOINT_PATH)\n",
    "print(f\"üíæ Saved checkpoint to: {CHECKPOINT_PATH}\")\n",
    "\n",
    "# 6) Compute Precision / Recall / F1 on validation (and test if present)\n",
    "@torch.no_grad()\n",
    "def eval_for_pr_metrics(model, loader, device):\n",
    "    model.eval().to(device)\n",
    "    all_probs, all_labels = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
    "            logits = model(x).squeeze(1)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_probs.extend(probs.tolist())\n",
    "        all_labels.extend(y.numpy().astype(int).tolist())\n",
    "    import numpy as np\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = (np.array(all_probs) >= 0.5).astype(int)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0) if len(set(y_true))>1 else float('nan')\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)    if len(set(y_true))>1 else float('nan')\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)        if len(set(y_true))>1 else float('nan')\n",
    "    return prec, rec, f1\n",
    "\n",
    "# Build loaders with a big batch for fast eval\n",
    "_, val_loader, test_loader, _ = build_loaders(batch_size=256)\n",
    "\n",
    "# Load back the checkpoint into the same architecture just to be explicit\n",
    "ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "model_final.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "\n",
    "prec, rec, f1 = eval_for_pr_metrics(model_final, val_loader, DEVICE)\n",
    "print(f\"üîé Validation ‚Äì Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "if test_loader is not None:\n",
    "    prec_t, rec_t, f1_t = eval_for_pr_metrics(model_final, test_loader, DEVICE)\n",
    "    print(f\"üß™ Test        ‚Äì Precision: {prec_t:.4f}, Recall: {rec_t:.4f}, F1: {f1_t:.4f}\")\n",
    "\n",
    "# 7) Optional: export ONNX for edge/interop\n",
    "if EXPORT_ONNX_PATH:\n",
    "    model_final.eval().to(\"cpu\")\n",
    "    dummy = torch.randn(1, 3, 128, 128)  # use your IMG_SIZE\n",
    "    torch.onnx.export(\n",
    "        model_final, dummy, EXPORT_ONNX_PATH,\n",
    "        input_names=[\"input\"], output_names=[\"logits\"],\n",
    "        opset_version=17, do_constant_folding=True, dynamic_axes={\"input\": {0: \"batch\"}}\n",
    "    )\n",
    "    print(f\"üì¶ Exported ONNX to: {EXPORT_ONNX_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd5cfe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60  loss=0.5475 acc=0.7259  val_loss=0.5809 val_acc=0.7257 val_auc=0.8116\n",
      "Epoch 2/60  loss=0.4607 acc=0.7863  val_loss=0.6949 val_acc=0.7191 val_auc=0.8257\n",
      "Epoch 3/60  loss=0.4227 acc=0.8109  val_loss=0.5351 val_acc=0.7856 val_auc=0.8802\n",
      "Epoch 4/60  loss=0.3917 acc=0.8302  val_loss=0.5119 val_acc=0.8015 val_auc=0.9148\n",
      "Epoch 5/60  loss=0.3571 acc=0.8450  val_loss=0.7473 val_acc=0.7107 val_auc=0.9121\n",
      "Epoch 6/60  loss=0.3414 acc=0.8590  val_loss=0.3323 val_acc=0.8670 val_auc=0.9448\n",
      "Epoch 7/60  loss=0.3220 acc=0.8663  val_loss=0.3700 val_acc=0.8502 val_auc=0.9535\n",
      "Epoch 8/60  loss=0.3061 acc=0.8779  val_loss=0.4320 val_acc=0.8399 val_auc=0.9288\n",
      "Epoch 9/60  loss=0.2859 acc=0.8832  val_loss=0.2797 val_acc=0.9036 val_auc=0.9577\n",
      "Epoch 10/60  loss=0.2832 acc=0.8832  val_loss=0.2548 val_acc=0.9036 val_auc=0.9625\n",
      "Epoch 11/60  loss=0.2701 acc=0.8916  val_loss=0.3176 val_acc=0.8699 val_auc=0.9607\n",
      "Epoch 12/60  loss=0.2605 acc=0.8943  val_loss=0.3440 val_acc=0.8680 val_auc=0.9678\n",
      "Epoch 13/60  loss=0.2459 acc=0.8963  val_loss=0.2149 val_acc=0.9176 val_auc=0.9727\n",
      "Epoch 14/60  loss=0.2404 acc=0.9057  val_loss=0.2094 val_acc=0.9092 val_auc=0.9762\n",
      "Epoch 15/60  loss=0.2290 acc=0.9093  val_loss=0.2348 val_acc=0.9101 val_auc=0.9726\n",
      "Epoch 16/60  loss=0.2249 acc=0.9099  val_loss=0.2320 val_acc=0.9082 val_auc=0.9707\n",
      "Epoch 17/60  loss=0.2163 acc=0.9175  val_loss=0.1992 val_acc=0.9232 val_auc=0.9783\n",
      "Epoch 18/60  loss=0.2048 acc=0.9178  val_loss=0.2744 val_acc=0.8989 val_auc=0.9770\n",
      "Epoch 19/60  loss=0.2024 acc=0.9183  val_loss=0.2400 val_acc=0.9064 val_auc=0.9683\n",
      "Epoch 20/60  loss=0.1924 acc=0.9268  val_loss=0.2481 val_acc=0.9129 val_auc=0.9775\n",
      "Epoch 21/60  loss=0.1901 acc=0.9276  val_loss=0.1868 val_acc=0.9288 val_auc=0.9802\n",
      "Epoch 22/60  loss=0.1940 acc=0.9231  val_loss=0.1475 val_acc=0.9401 val_auc=0.9867\n",
      "Epoch 23/60  loss=0.1858 acc=0.9272  val_loss=0.1588 val_acc=0.9345 val_auc=0.9849\n",
      "Epoch 24/60  loss=0.1748 acc=0.9274  val_loss=0.1846 val_acc=0.9316 val_auc=0.9864\n",
      "Epoch 25/60  loss=0.1767 acc=0.9312  val_loss=0.1416 val_acc=0.9457 val_auc=0.9879\n",
      "Epoch 26/60  loss=0.1703 acc=0.9347  val_loss=0.2063 val_acc=0.9148 val_auc=0.9829\n",
      "Epoch 27/60  loss=0.1663 acc=0.9359  val_loss=0.1295 val_acc=0.9513 val_auc=0.9890\n",
      "Epoch 28/60  loss=0.1645 acc=0.9350  val_loss=0.3132 val_acc=0.8979 val_auc=0.9805\n",
      "Epoch 29/60  loss=0.1639 acc=0.9366  val_loss=0.1449 val_acc=0.9391 val_auc=0.9865\n",
      "Epoch 30/60  loss=0.1590 acc=0.9395  val_loss=0.1836 val_acc=0.9251 val_auc=0.9825\n",
      "Epoch 31/60  loss=0.1619 acc=0.9379  val_loss=0.1688 val_acc=0.9438 val_auc=0.9882\n",
      "Epoch 32/60  loss=0.1525 acc=0.9393  val_loss=0.1474 val_acc=0.9522 val_auc=0.9892\n",
      "Epoch 33/60  loss=0.1492 acc=0.9426  val_loss=0.1858 val_acc=0.9316 val_auc=0.9829\n",
      "Epoch 34/60  loss=0.1449 acc=0.9421  val_loss=0.1252 val_acc=0.9457 val_auc=0.9914\n",
      "Epoch 35/60  loss=0.1431 acc=0.9466  val_loss=0.1462 val_acc=0.9335 val_auc=0.9906\n",
      "Epoch 36/60  loss=0.1465 acc=0.9432  val_loss=0.1785 val_acc=0.9466 val_auc=0.9842\n",
      "Epoch 37/60  loss=0.1325 acc=0.9498  val_loss=0.1592 val_acc=0.9316 val_auc=0.9914\n",
      "Epoch 38/60  loss=0.1390 acc=0.9459  val_loss=0.1137 val_acc=0.9541 val_auc=0.9922\n",
      "Epoch 39/60  loss=0.1352 acc=0.9487  val_loss=0.1187 val_acc=0.9522 val_auc=0.9917\n",
      "Epoch 40/60  loss=0.1218 acc=0.9547  val_loss=0.1353 val_acc=0.9504 val_auc=0.9920\n",
      "Epoch 41/60  loss=0.1276 acc=0.9476  val_loss=0.1396 val_acc=0.9476 val_auc=0.9909\n",
      "Epoch 42/60  loss=0.1283 acc=0.9501  val_loss=0.1493 val_acc=0.9522 val_auc=0.9912\n",
      "Epoch 43/60  loss=0.1234 acc=0.9526  val_loss=0.1282 val_acc=0.9504 val_auc=0.9907\n",
      "Epoch 44/60  loss=0.1274 acc=0.9544  val_loss=0.1380 val_acc=0.9522 val_auc=0.9942\n",
      "Epoch 45/60  loss=0.1062 acc=0.9604  val_loss=0.0986 val_acc=0.9616 val_auc=0.9948\n",
      "Epoch 46/60  loss=0.0979 acc=0.9648  val_loss=0.0953 val_acc=0.9663 val_auc=0.9944\n",
      "Epoch 47/60  loss=0.0920 acc=0.9652  val_loss=0.0990 val_acc=0.9625 val_auc=0.9941\n",
      "Epoch 48/60  loss=0.0861 acc=0.9680  val_loss=0.1000 val_acc=0.9635 val_auc=0.9939\n",
      "Epoch 49/60  loss=0.0920 acc=0.9671  val_loss=0.0957 val_acc=0.9654 val_auc=0.9942\n",
      "Epoch 50/60  loss=0.0854 acc=0.9669  val_loss=0.0991 val_acc=0.9654 val_auc=0.9929\n",
      "Epoch 51/60  loss=0.0901 acc=0.9665  val_loss=0.1007 val_acc=0.9654 val_auc=0.9937\n",
      "Epoch 52/60  loss=0.0856 acc=0.9679  val_loss=0.1087 val_acc=0.9625 val_auc=0.9931\n",
      "Epoch 53/60  loss=0.0859 acc=0.9676  val_loss=0.0968 val_acc=0.9672 val_auc=0.9940\n",
      "Epoch 54/60  loss=0.0800 acc=0.9709  val_loss=0.0963 val_acc=0.9663 val_auc=0.9946\n",
      "Epoch 55/60  loss=0.0833 acc=0.9709  val_loss=0.0981 val_acc=0.9672 val_auc=0.9943\n",
      "Epoch 56/60  loss=0.0787 acc=0.9721  val_loss=0.0970 val_acc=0.9644 val_auc=0.9942\n",
      "Epoch 57/60  loss=0.0776 acc=0.9717  val_loss=0.0997 val_acc=0.9625 val_auc=0.9942\n",
      "Epoch 58/60  loss=0.0811 acc=0.9683  val_loss=0.0911 val_acc=0.9691 val_auc=0.9946\n",
      "Epoch 59/60  loss=0.0783 acc=0.9707  val_loss=0.0949 val_acc=0.9672 val_auc=0.9944\n",
      "Epoch 60/60  loss=0.0775 acc=0.9683  val_loss=0.0987 val_acc=0.9654 val_auc=0.9940\n",
      "\n",
      "‚úÖ Final results: val_acc=0.9654 val_auc=0.9940 val_loss=0.0987\n",
      "üîé Validation ‚Äî Precision: 0.9665, Recall: 0.9719, F1: 0.9692\n",
      "üß™ Test        ‚Äî Precision: 0.9580, Recall: 0.9813, F1: 0.9695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2264"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Final training with checkpoint saving + proper metrics ---\n",
    "\n",
    "import os, gc, torch, numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Make sure these exist from earlier cells:\n",
    "# - CustomCNN, build_loaders, DEVICE, AMP\n",
    "# - BEST_PARAMS dict with your best hyperparams\n",
    "# If BEST_PARAMS isn't defined in this kernel anymore, paste it again here.\n",
    "\n",
    "def train_and_evaluate_model_ckpt(config: Dict, epochs: int = 60,\n",
    "                                  patience: int = 15,\n",
    "                                  subset_frac: float = 1.0,\n",
    "                                  save_path: Optional[str] = \"checkpoints/final_best.pt\") -> Dict:\n",
    "    \"\"\"Same behavior as your train function, but saves best weights to save_path.\"\"\"\n",
    "    batch_size     = int(config['batch_size'])\n",
    "    lr             = float(config['learning_rate'])\n",
    "    weight_decay   = float(config['reg_strength'])\n",
    "    dropout_conv   = float(config['dropout_conv'])\n",
    "    dropout_dense  = float(config['dropout_dense'])\n",
    "    dense_units    = int(config['dense_units'])\n",
    "    filt_mult      = float(config['filters_multiplier'])\n",
    "    beta1          = float(config['beta_1'])\n",
    "    beta2          = float(config['beta_2'])\n",
    "\n",
    "    # Use your existing loader (supports subset_frac if you added it; else ignore)\n",
    "    try:\n",
    "        train_ld, val_ld, test_ld, _ = build_loaders(batch_size, subset_frac=subset_frac)  # if you implemented subset\n",
    "    except TypeError:\n",
    "        train_ld, val_ld, test_ld, _ = build_loaders(batch_size)  # fallback\n",
    "\n",
    "    model = CustomCNN(weight_decay, dropout_conv, dropout_dense, dense_units, filt_mult).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, betas=(beta1,beta2), weight_decay=weight_decay)\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', factor=0.2, patience=5)  # verbose removed\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
    "\n",
    "    best_acc, best_state, wait = -1.0, None, 0\n",
    "    hist = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': [], 'val_auc': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        n, run_loss, run_correct = 0, 0.0, 0\n",
    "        for x, y in train_ld:\n",
    "            x = x.to(DEVICE, non_blocking=True)\n",
    "            y = y.float().to(DEVICE, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=AMP):\n",
    "                logits = model(x).squeeze(1)\n",
    "                loss   = criterion(logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "            run_loss += float(loss.item()) * x.size(0)\n",
    "            run_correct += int(((torch.sigmoid(logits) >= 0.5).long().cpu() == y.cpu().long()).sum().item())\n",
    "            n += x.size(0)\n",
    "        train_loss = run_loss / max(1, n)\n",
    "        train_acc  = run_correct / max(1, n)\n",
    "\n",
    "        # ---- validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_probs, all_labels = [], []\n",
    "            val_n, val_loss_sum, val_correct = 0, 0.0, 0\n",
    "            for x, y in val_ld:\n",
    "                x = x.to(DEVICE, non_blocking=True)\n",
    "                yf = y.float().to(DEVICE, non_blocking=True)\n",
    "                with torch.cuda.amp.autocast(enabled=AMP):\n",
    "                    logits = model(x).squeeze(1)\n",
    "                    vloss  = criterion(logits, yf)\n",
    "                val_loss_sum += float(vloss.item()) * x.size(0)\n",
    "                probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "                preds = (probs >= 0.5).astype(int)\n",
    "                val_correct += int((preds.flatten() == y.numpy().astype(int)).sum())\n",
    "                val_n += x.size(0)\n",
    "                all_probs.extend(probs.tolist())\n",
    "                all_labels.extend(y.numpy().astype(int).tolist())\n",
    "\n",
    "        val_loss = val_loss_sum / max(1, val_n)\n",
    "        val_acc  = val_correct / max(1, val_n)\n",
    "        try:\n",
    "            val_auc = roc_auc_score(all_labels, np.array(all_probs)) if len(set(all_labels))>1 else float('nan')\n",
    "        except Exception:\n",
    "            val_auc = float('nan')\n",
    "\n",
    "        hist['loss'].append(train_loss); hist['accuracy'].append(train_acc)\n",
    "        hist['val_loss'].append(val_loss); hist['val_accuracy'].append(val_acc); hist['val_auc'].append(val_auc)\n",
    "\n",
    "        sched.step(val_acc)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            wait = 0\n",
    "            if save_path is not None:\n",
    "                os.makedirs(os.path.dirname(save_path) or \".\", exist_ok=True)\n",
    "                torch.save({\"state_dict\": best_state, \"config\": config}, save_path)\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}; best val_acc={best_acc:.4f}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}  loss={train_loss:.4f} acc={train_acc:.4f}  \"\n",
    "              f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} val_auc={val_auc:.4f}\")\n",
    "\n",
    "    # restore best in-memory\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state, strict=False)\n",
    "\n",
    "    return {\n",
    "        'val_accuracy': float(val_acc),\n",
    "        'val_auc': float(val_auc),\n",
    "        'val_loss': float(val_loss),\n",
    "        'history': hist,\n",
    "        'batch_size_used': batch_size\n",
    "    }, model\n",
    "\n",
    "# ---- Run final training with your best params and save proper checkpoint\n",
    "BEST_PARAMS = {\n",
    "    \"learning_rate\": 0.00019329441561008272,\n",
    "    \"reg_strength\": 2.1392715033008293e-06,\n",
    "    \"dropout_conv\": 0.3991746766866793,\n",
    "    \"dropout_dense\": 0.4342883979481444,\n",
    "    \"dense_units\": 1024,\n",
    "    \"filters_multiplier\": 1.897507625889965,\n",
    "    \"batch_size\": 64,\n",
    "    \"beta_1\": 0.8828029914578701,\n",
    "    \"beta_2\": 0.9358066391956668,\n",
    "}\n",
    "\n",
    "final_result, trained_model = train_and_evaluate_model_ckpt(\n",
    "    BEST_PARAMS, epochs=60, patience=15, subset_frac=1.0,\n",
    "    save_path=\"checkpoints/final_best.pt\"\n",
    ")\n",
    "print(\"\\n‚úÖ Final results:\",\n",
    "      f\"val_acc={final_result['val_accuracy']:.4f}\",\n",
    "      f\"val_auc={final_result['val_auc']:.4f}\",\n",
    "      f\"val_loss={final_result['val_loss']:.4f}\")\n",
    "\n",
    "# ---- Reload saved weights and compute Precision/Recall/F1 on val (and test)\n",
    "@torch.no_grad()\n",
    "def compute_prf(model, loader):\n",
    "    model.eval().to(DEVICE)\n",
    "    probs_all, y_all = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE.type=='cuda')):\n",
    "            logits = model(x).squeeze(1)\n",
    "            probs  = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        probs_all.extend(probs.tolist()); y_all.extend(y.numpy().astype(int).tolist())\n",
    "    y_true = np.array(y_all); y_pred = (np.array(probs_all) >= 0.5).astype(int)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0) if len(set(y_true))>1 else float('nan')\n",
    "    rec  = recall_score(y_true, y_pred, zero_division=0)    if len(set(y_true))>1 else float('nan')\n",
    "    f1   = f1_score(y_true, y_pred, zero_division=0)        if len(set(y_true))>1 else float('nan')\n",
    "    return prec, rec, f1\n",
    "\n",
    "# loaders for evaluation\n",
    "_, val_loader, test_loader, _ = build_loaders(batch_size=256)\n",
    "\n",
    "# load the checkpoint we just saved to be explicit\n",
    "ckpt = torch.load(\"checkpoints/final_best.pt\", map_location=\"cpu\")\n",
    "trained_model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
    "\n",
    "p, r, f = compute_prf(trained_model, val_loader)\n",
    "print(f\"üîé Validation ‚Äî Precision: {p:.4f}, Recall: {r:.4f}, F1: {f:.4f}\")\n",
    "if test_loader is not None:\n",
    "    pt, rt, ft = compute_prf(trained_model, test_loader)\n",
    "    print(f\"üß™ Test        ‚Äî Precision: {pt:.4f}, Recall: {rt:.4f}, F1: {ft:.4f}\")\n",
    "\n",
    "# tidy\n",
    "torch.cuda.empty_cache(); gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d84405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.00019329441561008272, 'reg_strength': 2.1392715033008293e-06, 'dropout_conv': 0.3991746766866793, 'dropout_dense': 0.4342883979481444, 'dense_units': 1024, 'filters_multiplier': 1.897507625889965, 'batch_size': 64, 'beta_1': 0.8828029914578701, 'beta_2': 0.9358066391956668}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 4) Example inference (adjust shape to your input)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# x: torch.Tensor with shape [B, 3, 128, 128] (or your actual IMG_SIZE)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 26\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(\u001b[43mx\u001b[49m)           \u001b[38;5;66;03m# shape [B, 1]\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     probs  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(logits)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1) Load checkpoint (weights + hyperparams)\n",
    "ckpt = torch.load(\"checkpoints/final_best.pt\", map_location=\"cpu\")\n",
    "config = ckpt[\"config\"]          # dict of hyperparams\n",
    "state  = ckpt[\"state_dict\"]      # trained weights\n",
    "\n",
    "# 2) Rebuild the same model\n",
    "# Make sure your CustomCNN signature matches:\n",
    "# CustomCNN(reg_strength, dropout_conv, dropout_dense, dense_units, filters_multiplier)\n",
    "model = CustomCNN(\n",
    "    config[\"reg_strength\"],\n",
    "    config[\"dropout_conv\"],\n",
    "    config[\"dropout_dense\"],\n",
    "    int(config[\"dense_units\"]),\n",
    "    float(config[\"filters_multiplier\"]),\n",
    ").eval()  # inference mode\n",
    "\n",
    "# 3) Load the trained weights\n",
    "model.load_state_dict(state, strict=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
