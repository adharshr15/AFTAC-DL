{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.9.24)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))  # should NOT be []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyxuWT1jmYYf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as nps\n",
    "import json\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, BatchNormalization, Input, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B0gBkmf8nuwU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extracted to: cats-v-non-cats/\n"
     ]
    }
   ],
   "source": [
    "# Unzip dataset in Google Colab\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "zip_file_path = \"cats-v-non-cats.zip\"\n",
    "extract_dir = \"cats-v-non-cats/\"\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        for file_info in zip_ref.infolist():\n",
    "            # Extract only the files we want, avoiding the extra directory structure and __MACOSX\n",
    "            if not file_info.filename.startswith('__MACOSX/'):\n",
    "                # Remove the leading 'cats-v-dogs/' from the filename if it exists\n",
    "                arcname = file_info.filename\n",
    "                if arcname.startswith('cats-v-non-cats/'):\n",
    "                    arcname = arcname[len('cats-v-non-cats/'):]\n",
    "\n",
    "                # Only extract if the filename is not empty after removing the prefix\n",
    "                if arcname:\n",
    "                    file_info.filename = arcname\n",
    "                    zip_ref.extract(file_info, extract_dir)\n",
    "\n",
    "\n",
    "    print(f\"File extracted to: {extract_dir}\")\n",
    "\n",
    "    # Remove the __MACOSX folder if it exists\n",
    "    macosx_folder = os.path.join(\"/content/\", '__MACOSX')\n",
    "    if os.path.exists(macosx_folder):\n",
    "        shutil.rmtree(macosx_folder)\n",
    "        print(f\"Folder '{macosx_folder}' deleted successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during unzipping: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vvzFQ3vKmbqF"
   },
   "outputs": [],
   "source": [
    "# Configuration and Data Paths\n",
    "TRAINING_DIR = \"cats-v-non-cats/training/\"\n",
    "VALIDATION_DIR = \"cats-v-non-cats/validation/\"\n",
    "TESTING_DIR = \"cats-v-non-cats/test/\"\n",
    "\n",
    "# Define whether to include test split or not\n",
    "INCLUDE_TEST = True\n",
    "\n",
    "# Baseline Configuration for comparison\n",
    "BASELINE_CONFIG = {\n",
    "    'learning_rate': 0.001,\n",
    "    'reg_strength': 0.00001,\n",
    "    'dropout_conv': 0.15,\n",
    "    'dropout_dense': 0.4,\n",
    "    'dense_units': 512,\n",
    "    'filters_multiplier': 0.75,\n",
    "    'batch_size': 32,\n",
    "    'beta_1': 0.8,\n",
    "    'beta_2': 0.99\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "W0GBfK9DmgPg"
   },
   "outputs": [],
   "source": [
    "# Set up data generators\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "if INCLUDE_TEST:\n",
    "    test_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "l3ApKndwmh9e"
   },
   "outputs": [],
   "source": [
    "# Model Architecture Function\n",
    "def create_tuned_model(reg_strength=0.0001, dropout_conv=0.2, dropout_dense=0.4,\n",
    "                      dense_units=512, filters_multiplier=1):\n",
    "\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "\n",
    "    # Calculate filter sizes\n",
    "    filters1 = int(32 * filters_multiplier)\n",
    "    filters2 = int(64 * filters_multiplier)\n",
    "    filters3 = int(128 * filters_multiplier)\n",
    "\n",
    "    # First block\n",
    "    x = Conv2D(filters1, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters1, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Dropout(dropout_conv)(x)\n",
    "\n",
    "    # Second block\n",
    "    x = Conv2D(filters2, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters2, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Dropout(dropout_conv)(x)\n",
    "\n",
    "    # Third block\n",
    "    x = Conv2D(filters3, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters3, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Dropout(dropout_conv)(x)\n",
    "\n",
    "    # Global pooling and dense layers\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(dense_units, activation='relu', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_dense)(x)\n",
    "    x = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_strength))(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-DqKAzYYmk0h"
   },
   "outputs": [],
   "source": [
    "# Training and Evaluation Function\n",
    "def train_and_evaluate_model(config, epochs=5, param_name=None, param_value=None):\n",
    "    try:\n",
    "        # Create model with current configuration\n",
    "        model = create_tuned_model(\n",
    "            reg_strength=config['reg_strength'],\n",
    "            dropout_conv=config['dropout_conv'],\n",
    "            dropout_dense=config['dropout_dense'],\n",
    "            dense_units=config['dense_units'],\n",
    "            filters_multiplier=config['filters_multiplier']\n",
    "        )\n",
    "\n",
    "        # Compile model with current optimizer settings\n",
    "        optimizer = Adam(\n",
    "            learning_rate=config['learning_rate'],\n",
    "            beta_1=config['beta_1'],\n",
    "            beta_2=config['beta_2']\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'AUC']\n",
    "        )\n",
    "\n",
    "        train_generator = train_gen.flow_from_directory(\n",
    "            TRAINING_DIR,\n",
    "            target_size=(128, 128),\n",
    "            batch_size=config['batch_size'],\n",
    "            class_mode='binary',\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        validation_generator = validation_gen.flow_from_directory(\n",
    "            VALIDATION_DIR,\n",
    "            target_size=(128, 128),\n",
    "            batch_size=config['batch_size'],\n",
    "            class_mode='binary',\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        if INCLUDE_TEST:\n",
    "            test_generator = test_gen.flow_from_directory(\n",
    "                TESTING_DIR,\n",
    "                target_size=(128, 128),\n",
    "                batch_size=config['batch_size'],\n",
    "                class_mode='binary',\n",
    "                shuffle=True\n",
    "            )\n",
    "\n",
    "        # Define callbacks for training (no checkpoint saving)\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_accuracy',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        )\n",
    "\n",
    "        # Only use learning rate reduction and early stopping - no checkpoint saving\n",
    "        callbacks = [reduce_lr, early_stop]\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Get final validation metrics\n",
    "        val_loss, val_accuracy, val_auc = model.evaluate(validation_generator, verbose=0)\n",
    "\n",
    "        # Get test metrics if test set is available\n",
    "        test_results = {}\n",
    "        if INCLUDE_TEST:\n",
    "            test_loss, test_accuracy, test_auc = model.evaluate(test_generator, verbose=0)\n",
    "            test_results = {\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'test_auc': test_auc,\n",
    "                'test_loss': test_loss\n",
    "            }\n",
    "\n",
    "        results = {\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_auc': val_auc,\n",
    "            'val_loss': val_loss,\n",
    "            'history': history.history,\n",
    "            'batch_size_used': config['batch_size']\n",
    "        }\n",
    "\n",
    "        # Add test results if available\n",
    "        results.update(test_results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in training: {e}\")\n",
    "        return {\n",
    "            'val_accuracy': 0.0,\n",
    "            'val_auc': 0.0,\n",
    "            'val_loss': float('inf'),\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FUWQKKmDmw_s"
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimization Setup\n",
    "def objective(trial):\n",
    "    config = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True),\n",
    "        'reg_strength': trial.suggest_float('reg_strength', 1e-6, 1e-2, log=True),\n",
    "        'dropout_conv': trial.suggest_float('dropout_conv', 0.1, 0.4),\n",
    "        'dropout_dense': trial.suggest_float('dropout_dense', 0.2, 0.7),\n",
    "        'dense_units': trial.suggest_categorical('dense_units', [128, 256, 512, 1024, 2048]),\n",
    "        'filters_multiplier': trial.suggest_float('filters_multiplier', 0.5, 2.0),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "        'beta_1': trial.suggest_float('beta_1', 0.7, 0.99),\n",
    "        'beta_2': trial.suggest_float('beta_2', 0.9, 0.9999)\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        result = train_and_evaluate_model(\n",
    "            config,\n",
    "            epochs=30,\n",
    "            param_name=f\"trial_{trial.number}\",\n",
    "            param_value=\"bayesian\"\n",
    "        )\n",
    "        accuracy = result['val_accuracy']\n",
    "\n",
    "        trial.set_user_attr('val_auc', result['val_auc'])\n",
    "        trial.set_user_attr('val_loss', result['val_loss'])\n",
    "\n",
    "        print(f\"Trial {trial.number}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed: {e}\")\n",
    "        # Return a low value for failed trials\n",
    "        return 0.0\n",
    "\n",
    "def run_bayesian_optimization(n_trials=50, timeout=None):\n",
    "\n",
    "    print(f\"Starting Bayesian Optimization with {n_trials} trials...\")\n",
    "\n",
    "    # Create study object\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',  # We want to maximize accuracy\n",
    "        sampler=TPESampler(seed=42),  # Tree-structured Parzen Estimator\n",
    "        study_name='cnn_hyperparameter_optimization'\n",
    "    )\n",
    "\n",
    "    # Run optimization\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BAYESIAN OPTIMIZATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "    print(f\"Best trial number: {study.best_trial.number}\")\n",
    "    print(f\"Best validation accuracy: {study.best_value:.4f}\")\n",
    "\n",
    "    print(f\"\\nðŸ† Best hyperparameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Get additional metrics for best trial\n",
    "    best_trial = study.best_trial\n",
    "    if 'val_auc' in best_trial.user_attrs:\n",
    "        print(f\"\\nBest trial metrics:\")\n",
    "        print(f\"  Validation AUC: {best_trial.user_attrs['val_auc']:.4f}\")\n",
    "        print(f\"  Validation Loss: {best_trial.user_attrs['val_loss']:.4f}\")\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9HCDFSrxmycr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model...\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "267/267 [==============================] - 214s 793ms/step - loss: 0.5545 - accuracy: 0.7452 - auc: 0.8185 - val_loss: 0.8130 - val_accuracy: 0.5150 - val_auc: 0.6164 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 214s 793ms/step - loss: 0.5545 - accuracy: 0.7452 - auc: 0.8185 - val_loss: 0.8130 - val_accuracy: 0.5150 - val_auc: 0.6164 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "267/267 [==============================] - 143s 536ms/step - loss: 0.4587 - accuracy: 0.8003 - auc: 0.8746 - val_loss: 1.2041 - val_accuracy: 0.7051 - val_auc: 0.8227 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 143s 536ms/step - loss: 0.4587 - accuracy: 0.8003 - auc: 0.8746 - val_loss: 1.2041 - val_accuracy: 0.7051 - val_auc: 0.8227 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "267/267 [==============================] - 144s 537ms/step - loss: 0.3929 - accuracy: 0.8313 - auc: 0.9080 - val_loss: 0.6073 - val_accuracy: 0.7331 - val_auc: 0.9204 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 144s 537ms/step - loss: 0.3929 - accuracy: 0.8313 - auc: 0.9080 - val_loss: 0.6073 - val_accuracy: 0.7331 - val_auc: 0.9204 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "267/267 [==============================] - 149s 556ms/step - loss: 0.3617 - accuracy: 0.8471 - auc: 0.9222 - val_loss: 0.5853 - val_accuracy: 0.7322 - val_auc: 0.8298 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 149s 556ms/step - loss: 0.3617 - accuracy: 0.8471 - auc: 0.9222 - val_loss: 0.5853 - val_accuracy: 0.7322 - val_auc: 0.8298 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "267/267 [==============================] - 162s 608ms/step - loss: 0.3357 - accuracy: 0.8605 - auc: 0.9338 - val_loss: 1.0797 - val_accuracy: 0.6077 - val_auc: 0.9143 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 162s 608ms/step - loss: 0.3357 - accuracy: 0.8605 - auc: 0.9338 - val_loss: 1.0797 - val_accuracy: 0.6077 - val_auc: 0.9143 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "267/267 [==============================] - 154s 578ms/step - loss: 0.3099 - accuracy: 0.8716 - auc: 0.9435 - val_loss: 0.6875 - val_accuracy: 0.7116 - val_auc: 0.8806 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 154s 578ms/step - loss: 0.3099 - accuracy: 0.8716 - auc: 0.9435 - val_loss: 0.6875 - val_accuracy: 0.7116 - val_auc: 0.8806 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "267/267 [==============================] - 155s 581ms/step - loss: 0.2838 - accuracy: 0.8874 - auc: 0.9528 - val_loss: 0.5289 - val_accuracy: 0.8165 - val_auc: 0.9449 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 155s 581ms/step - loss: 0.2838 - accuracy: 0.8874 - auc: 0.9528 - val_loss: 0.5289 - val_accuracy: 0.8165 - val_auc: 0.9449 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "267/267 [==============================] - 158s 590ms/step - loss: 0.2718 - accuracy: 0.8924 - auc: 0.9570 - val_loss: 0.5552 - val_accuracy: 0.7846 - val_auc: 0.9562 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "267/267 [==============================] - 158s 590ms/step - loss: 0.2718 - accuracy: 0.8924 - auc: 0.9570 - val_loss: 0.5552 - val_accuracy: 0.7846 - val_auc: 0.9562 - lr: 0.0010\n",
      "Epoch 9/50\n",
      " 53/267 [====>.........................] - ETA: 1:59 - loss: 0.2734 - accuracy: 0.8868 - auc: 0.9568"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6680\\1726182177.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training baseline model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbaseline_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBASELINE_CONFIG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nBaseline Results:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Accuracy: {baseline_result['val_accuracy']:.4f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6680\\785732454.py\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[1;34m(config, epochs, param_name, param_value)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         )\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m--> 135\u001b[1;33m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    381\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 53\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training baseline model...\")\n",
    "baseline_result = train_and_evaluate_model(BASELINE_CONFIG, epochs=50)\n",
    "\n",
    "print(\"\\nBaseline Results:\")\n",
    "print(f\"Accuracy: {baseline_result['val_accuracy']:.4f}\")\n",
    "print(f\"AUC: {baseline_result['val_auc']:.4f}\")\n",
    "print(f\"Loss: {baseline_result['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes here on this runtime, the model peaks at around epoch 7/8, afterwards the accuracy continues to increase (but validation accuracy decreases), which is a sign of overfitting.\n",
    "\n",
    "So may want to slowdown the training so it doesnt start overfitting this early. Our peak validation accuracy is around 85%. This is without any bayesian parameter optimization happening yet, so going to make another file to compare with a pretrained model the same circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MobileNetV2 baseline (transfer learning)...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
      "9406464/9406464 [==============================] - 1s 0us/step\n",
      "9406464/9406464 [==============================] - 1s 0us/step\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "Epoch 1/10\n",
      "267/267 [==============================] - 61s 222ms/step - loss: 0.1020 - accuracy: 0.9682 - auc: 0.9933 - val_loss: 0.0351 - val_accuracy: 0.9925 - val_auc: 0.9996 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "267/267 [==============================] - 61s 222ms/step - loss: 0.1020 - accuracy: 0.9682 - auc: 0.9933 - val_loss: 0.0351 - val_accuracy: 0.9925 - val_auc: 0.9996 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "267/267 [==============================] - 55s 204ms/step - loss: 0.0767 - accuracy: 0.9772 - auc: 0.9959 - val_loss: 0.0370 - val_accuracy: 0.9906 - val_auc: 0.9996 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "267/267 [==============================] - 55s 204ms/step - loss: 0.0767 - accuracy: 0.9772 - auc: 0.9959 - val_loss: 0.0370 - val_accuracy: 0.9906 - val_auc: 0.9996 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "267/267 [==============================] - 55s 206ms/step - loss: 0.0707 - accuracy: 0.9792 - auc: 0.9966 - val_loss: 0.0525 - val_accuracy: 0.9822 - val_auc: 0.9988 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "267/267 [==============================] - 55s 206ms/step - loss: 0.0707 - accuracy: 0.9792 - auc: 0.9966 - val_loss: 0.0525 - val_accuracy: 0.9822 - val_auc: 0.9988 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0604 - accuracy: 0.9828 - auc: 0.9977 - val_loss: 0.0319 - val_accuracy: 0.9925 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0604 - accuracy: 0.9828 - auc: 0.9977 - val_loss: 0.0319 - val_accuracy: 0.9925 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0638 - accuracy: 0.9809 - auc: 0.9973 - val_loss: 0.0328 - val_accuracy: 0.9925 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0638 - accuracy: 0.9809 - auc: 0.9973 - val_loss: 0.0328 - val_accuracy: 0.9925 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "267/267 [==============================] - 55s 204ms/step - loss: 0.0622 - accuracy: 0.9815 - auc: 0.9974 - val_loss: 0.0280 - val_accuracy: 0.9944 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "267/267 [==============================] - 55s 204ms/step - loss: 0.0622 - accuracy: 0.9815 - auc: 0.9974 - val_loss: 0.0280 - val_accuracy: 0.9944 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0589 - accuracy: 0.9827 - auc: 0.9977 - val_loss: 0.0436 - val_accuracy: 0.9860 - val_auc: 0.9988 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0589 - accuracy: 0.9827 - auc: 0.9977 - val_loss: 0.0436 - val_accuracy: 0.9860 - val_auc: 0.9988 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0569 - accuracy: 0.9823 - auc: 0.9981 - val_loss: 0.0518 - val_accuracy: 0.9860 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0569 - accuracy: 0.9823 - auc: 0.9981 - val_loss: 0.0518 - val_accuracy: 0.9860 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "267/267 [==============================] - 55s 206ms/step - loss: 0.0579 - accuracy: 0.9802 - auc: 0.9981 - val_loss: 0.0342 - val_accuracy: 0.9906 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "267/267 [==============================] - 55s 206ms/step - loss: 0.0579 - accuracy: 0.9802 - auc: 0.9981 - val_loss: 0.0342 - val_accuracy: 0.9906 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0525 - accuracy: 0.9838 - auc: 0.9984 - val_loss: 0.0443 - val_accuracy: 0.9906 - val_auc: 0.9988 - lr: 0.0010\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0525 - accuracy: 0.9838 - auc: 0.9984 - val_loss: 0.0443 - val_accuracy: 0.9906 - val_auc: 0.9988 - lr: 0.0010\n",
      "\n",
      "MobileNetV2 Baseline Results:\n",
      "Accuracy: 0.9906\n",
      "AUC: 0.9988\n",
      "Loss: 0.0443\n",
      "\n",
      "=== QUICK COMPARISON ===\n",
      "Custom CNN  - Accuracy: 0.7285, AUC: 0.9526\n",
      "MobileNetV2 - Accuracy: 0.9906, AUC: 0.9988\n",
      "Accuracy diff (MobileNet - Custom): +0.2622\n",
      "\n",
      "MobileNetV2 Baseline Results:\n",
      "Accuracy: 0.9906\n",
      "AUC: 0.9988\n",
      "Loss: 0.0443\n",
      "\n",
      "=== QUICK COMPARISON ===\n",
      "Custom CNN  - Accuracy: 0.7285, AUC: 0.9526\n",
      "MobileNetV2 - Accuracy: 0.9906, AUC: 0.9988\n",
      "Accuracy diff (MobileNet - Custom): +0.2622\n"
     ]
    }
   ],
   "source": [
    "# MobileNetV2 transfer-learning baseline confined to this cell\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "def create_mobilenetv2_model(reg_strength=1e-4, dropout_dense=0.4, dense_units=512, input_shape=(128,128,3), train_base=False):\n",
    "    \"\"\"Build a MobileNetV2-based binary classifier (transfer learning).\n",
    "    Set train_base=True to fine-tune the backbone.\"\"\"\n",
    "    base = MobileNetV2(include_top=False, weights='imagenet', input_shape=input_shape, pooling='avg')\n",
    "    base.trainable = train_base\n",
    "    x = base.output\n",
    "    x = Dense(dense_units, activation='relu', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_dense)(x)\n",
    "    x = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_strength))(x)\n",
    "    return Model(inputs=base.input, outputs=x)\n",
    "\n",
    "def train_and_evaluate_mobilenet(config, epochs=5):\n",
    "    \"\"\"Train MobileNetV2 using MobileNet preprocessing and return same metrics structure as the custom trainer.\"\"\"\n",
    "    try:\n",
    "        # Use MobileNet preprocess_input for consistency with pretrained weights\n",
    "        mn_train_gen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                          rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                          shear_range=0.2, zoom_range=0.2, horizontal_flip=True,\n",
    "                                          brightness_range=[0.8,1.2], fill_mode='nearest')\n",
    "        mn_val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "        mn_test_gen = ImageDataGenerator(preprocessing_function=preprocess_input) if INCLUDE_TEST else None\n",
    "\n",
    "        model = create_mobilenetv2_model(reg_strength=config.get('reg_strength', 1e-4),\n",
    "                                         dropout_dense=config.get('dropout_dense', 0.4),\n",
    "                                         dense_units=config.get('dense_units', 512),\n",
    "                                         train_base=False)\n",
    "\n",
    "        optimizer = Adam(learning_rate=config['learning_rate'], beta_1=config['beta_1'], beta_2=config['beta_2'])\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "        train_generator = mn_train_gen.flow_from_directory(TRAINING_DIR, target_size=(128,128),\n",
    "                                                          batch_size=config['batch_size'], class_mode='binary', shuffle=True)\n",
    "        validation_generator = mn_val_gen.flow_from_directory(VALIDATION_DIR, target_size=(128,128),\n",
    "                                                               batch_size=config['batch_size'], class_mode='binary', shuffle=False)\n",
    "\n",
    "        if INCLUDE_TEST:\n",
    "            test_generator = mn_test_gen.flow_from_directory(TESTING_DIR, target_size=(128,128),\n",
    "                                                            batch_size=config['batch_size'], class_mode='binary', shuffle=True)\n",
    "\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr=1e-8, verbose=1)\n",
    "        early_stop = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1, mode='max')\n",
    "        callbacks = [reduce_lr, early_stop]\n",
    "\n",
    "        history = model.fit(train_generator, validation_data=validation_generator, epochs=epochs, verbose=1, callbacks=callbacks)\n",
    "\n",
    "        val_loss, val_accuracy, val_auc = model.evaluate(validation_generator, verbose=0)\n",
    "        results = {'val_accuracy': val_accuracy, 'val_auc': val_auc, 'val_loss': val_loss, 'history': history.history, 'batch_size_used': config['batch_size']}\n",
    "\n",
    "        if INCLUDE_TEST:\n",
    "            test_loss, test_accuracy, test_auc = model.evaluate(test_generator, verbose=0)\n",
    "            results.update({'test_accuracy': test_accuracy, 'test_auc': test_auc, 'test_loss': test_loss})\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'MobileNet training error: {e}')\n",
    "        return {'val_accuracy': 0.0, 'val_auc': 0.0, 'val_loss': float('inf'), 'error': str(e)}\n",
    "\n",
    "# Run MobileNetV2 baseline here (safe to run multiple times; checks for baseline_result)\n",
    "if 'baseline_result' in globals():\n",
    "    print('\\nTraining MobileNetV2 baseline (transfer learning)...')\n",
    "    mobilenet_config = BASELINE_CONFIG.copy()\n",
    "    mobilenet_result = train_and_evaluate_mobilenet(mobilenet_config, epochs=10)\n",
    "\n",
    "    print('\\nMobileNetV2 Baseline Results:')\n",
    "    print(f\"Accuracy: {mobilenet_result['val_accuracy']:.4f}\")\n",
    "    print(f\"AUC: {mobilenet_result['val_auc']:.4f}\")\n",
    "    print(f\"Loss: {mobilenet_result['val_loss']:.4f}\")\n",
    "\n",
    "    try:\n",
    "        print('\\n=== QUICK COMPARISON ===')\n",
    "        print(f\"Custom CNN  - Accuracy: {baseline_result['val_accuracy']:.4f}, AUC: {baseline_result['val_auc']:.4f}\")\n",
    "        print(f\"MobileNetV2 - Accuracy: {mobilenet_result['val_accuracy']:.4f}, AUC: {mobilenet_result['val_auc']:.4f}\")\n",
    "        acc_diff = mobilenet_result['val_accuracy'] - baseline_result['val_accuracy']\n",
    "        print(f\"Accuracy diff (MobileNet - Custom): {acc_diff:+.4f}\")\n",
    "    except Exception as e:\n",
    "        print('Comparison failed:', e)\n",
    "else:\n",
    "    print('baseline_result not found - run the custom baseline cell first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZ957ewtmzm4"
   },
   "outputs": [],
   "source": [
    "study = run_bayesian_optimization(n_trials=50)  # Increase to 100-200 for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQGbBXbwm04I"
   },
   "outputs": [],
   "source": [
    "# Analyze and Visualize Results\n",
    "def analyze_bayesian_results(study):\n",
    "    # Train final model with best parameters\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ðŸ”¥ TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    best_config = study.best_params\n",
    "\n",
    "    # Train with more epochs for final model\n",
    "    final_result = train_and_evaluate_model(\n",
    "        best_config,\n",
    "        epochs=60,  # More epochs for final training\n",
    "        param_name=\"bayesian_best\",\n",
    "        param_value=\"final\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nðŸ“Š FINAL RESULTS COMPARISON:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Baseline  - Accuracy: {baseline_result['val_accuracy']:.4f}, AUC: {baseline_result['val_auc']:.4f}\")\n",
    "    print(f\"Bayesian  - Accuracy: {final_result['val_accuracy']:.4f}, AUC: {final_result['val_auc']:.4f}\")\n",
    "    print(f\"Improvement - Accuracy: {final_result['val_accuracy'] - baseline_result['val_accuracy']:+.4f}, AUC: {final_result['val_auc'] - baseline_result['val_auc']:+.4f}\")\n",
    "\n",
    "    # Plot optimization history\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # 1. Optimization history\n",
    "    trial_numbers = [trial.number for trial in study.trials]\n",
    "    trial_values = [trial.value if trial.value is not None else 0 for trial in study.trials]\n",
    "\n",
    "    axes[0, 0].plot(trial_numbers, trial_values, 'b-', alpha=0.6)\n",
    "    axes[0, 0].scatter(trial_numbers, trial_values, c=trial_values, cmap='viridis', s=30)\n",
    "    axes[0, 0].axhline(y=baseline_result['val_accuracy'], color='red', linestyle='--',\n",
    "                       label=f'Baseline ({baseline_result[\"val_accuracy\"]:.4f})')\n",
    "    axes[0, 0].set_xlabel('Trial Number')\n",
    "    axes[0, 0].set_ylabel('Validation Accuracy')\n",
    "    axes[0, 0].set_title('Optimization History')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Parameter importance (if available)\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        params = list(importance.keys())\n",
    "        values = list(importance.values())\n",
    "\n",
    "        axes[0, 1].barh(params, values)\n",
    "        axes[0, 1].set_xlabel('Importance')\n",
    "        axes[0, 1].set_title('Hyperparameter Importance')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    except:\n",
    "        axes[0, 1].text(0.5, 0.5, 'Parameter importance\\nnot available\\n(need more trials)',\n",
    "                        ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        axes[0, 1].set_title('Hyperparameter Importance')\n",
    "\n",
    "    # 3. Best vs worst trials comparison\n",
    "    best_trials = sorted(study.trials, key=lambda t: t.value if t.value else 0, reverse=True)[:5]\n",
    "    worst_trials = sorted(study.trials, key=lambda t: t.value if t.value else 0)[:5]\n",
    "\n",
    "    best_values = [t.value for t in best_trials if t.value is not None]\n",
    "    worst_values = [t.value for t in worst_trials if t.value is not None]\n",
    "\n",
    "    axes[1, 0].bar(range(len(best_values)), best_values, color='green', alpha=0.7, label='Best 5 trials')\n",
    "    axes[1, 0].bar(range(len(best_values), len(best_values) + len(worst_values)),\n",
    "                   worst_values, color='red', alpha=0.7, label='Worst 5 trials')\n",
    "    axes[1, 0].set_ylabel('Validation Accuracy')\n",
    "    axes[1, 0].set_title('Best vs Worst Trials')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Learning rate vs accuracy scatter\n",
    "    lr_values = []\n",
    "    acc_values = []\n",
    "    for trial in study.trials:\n",
    "        if trial.value is not None and 'learning_rate' in trial.params:\n",
    "            lr_values.append(trial.params['learning_rate'])\n",
    "            acc_values.append(trial.value)\n",
    "\n",
    "    if lr_values:\n",
    "        axes[1, 1].scatter(lr_values, acc_values, alpha=0.6, c=acc_values, cmap='viridis')\n",
    "        axes[1, 1].set_xscale('log')\n",
    "        axes[1, 1].set_xlabel('Learning Rate')\n",
    "        axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "        axes[1, 1].set_title('Learning Rate vs Accuracy')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bayesian_optimization_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return final_result, best_config\n",
    "\n",
    "# Run analysis\n",
    "if 'study' in locals():\n",
    "    bayesian_final_result, bayesian_best_config = analyze_bayesian_results(study)\n",
    "    print(\"âœ… Analysis complete!\")\n",
    "else:\n",
    "    print(\"âŒ Run Bayesian optimization first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwNKJpGdm1_w"
   },
   "outputs": [],
   "source": [
    "# Final Summary and Model Comparison\n",
    "def final_summary():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"ðŸŽ¯ BAYESIAN OPTIMIZATION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    if 'bayesian_final_result' in locals():\n",
    "        baseline_acc = baseline_result['val_accuracy']\n",
    "        optimized_acc = bayesian_final_result['val_accuracy']\n",
    "        improvement = optimized_acc - baseline_acc\n",
    "\n",
    "        print(f\"ðŸ“ˆ Performance Improvement:\")\n",
    "        print(f\"  Baseline Accuracy:  {baseline_acc:.4f}\")\n",
    "        print(f\"  Optimized Accuracy: {optimized_acc:.4f}\")\n",
    "        print(f\"  Improvement:        {improvement:+.4f} ({improvement/baseline_acc*100:+.2f}%)\")\n",
    "\n",
    "        print(f\"\\nðŸ† Best Configuration Found:\")\n",
    "        for param, value in bayesian_best_config.items():\n",
    "            baseline_val = BASELINE_CONFIG.get(param, \"N/A\")\n",
    "            print(f\"  {param:<18}: {value:<10} (baseline: {baseline_val})\")\n",
    "\n",
    "        print(f\"\\nðŸ” Key Insights:\")\n",
    "        print(f\"  â€¢ Total trials run: {len(study.trials)}\")\n",
    "        print(f\"  â€¢ Best trial: #{study.best_trial.number}\")\n",
    "        print(f\"  â€¢ Search space explored efficiently using Bayesian optimization\")\n",
    "        print(f\"  â€¢ Model saved as: {bayesian_final_result['model_filename']}\")\n",
    "\n",
    "        # Determine if optimization was successful\n",
    "        if improvement > 0.01:  # 1% improvement threshold\n",
    "            print(f\"\\nâœ… Optimization SUCCESSFUL! Significant improvement achieved.\")\n",
    "        elif improvement > 0:\n",
    "            print(f\"\\nâœ… Optimization successful with modest improvement.\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸  Baseline was already quite good. Consider:\")\n",
    "            print(f\"     â€¢ Running more trials\")\n",
    "            print(f\"     â€¢ Expanding search space\")\n",
    "            print(f\"     â€¢ Different optimization objectives\")\n",
    "\n",
    "    else:\n",
    "        print(\"âŒ Bayesian optimization results not available.\")\n",
    "\n",
    "    print(f\"\\nðŸš€ Next Steps:\")\n",
    "    print(f\"  â€¢ Use the best model for production\")\n",
    "    print(f\"  â€¢ Consider ensemble methods\")\n",
    "    print(f\"  â€¢ Test on holdout data\")\n",
    "    print(f\"  â€¢ Monitor performance in production\")\n",
    "\n",
    "# Run final summary\n",
    "final_summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNZ4Q9S58aix/ISGyxtFBeP",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
