{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyxuWT1jmYYf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nicholas\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, BatchNormalization, Input, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "B0gBkmf8nuwU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File extracted to: cats-v-non-cats/\n"
     ]
    }
   ],
   "source": [
    "# Unzip dataset in Google Colab\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "zip_file_path = \"cats-v-non-cats.zip\"\n",
    "extract_dir = \"cats-v-non-cats/\"\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        for file_info in zip_ref.infolist():\n",
    "            # Extract only the files we want, avoiding the extra directory structure and __MACOSX\n",
    "            if not file_info.filename.startswith('__MACOSX/'):\n",
    "                # Remove the leading 'cats-v-dogs/' from the filename if it exists\n",
    "                arcname = file_info.filename\n",
    "                if arcname.startswith('cats-v-non-cats/'):\n",
    "                    arcname = arcname[len('cats-v-non-cats/'):]\n",
    "\n",
    "                # Only extract if the filename is not empty after removing the prefix\n",
    "                if arcname:\n",
    "                    file_info.filename = arcname\n",
    "                    zip_ref.extract(file_info, extract_dir)\n",
    "\n",
    "\n",
    "    print(f\"File extracted to: {extract_dir}\")\n",
    "\n",
    "    # Remove the __MACOSX folder if it exists\n",
    "    macosx_folder = os.path.join(\"/content/\", '__MACOSX')\n",
    "    if os.path.exists(macosx_folder):\n",
    "        shutil.rmtree(macosx_folder)\n",
    "        print(f\"Folder '{macosx_folder}' deleted successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during unzipping: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vvzFQ3vKmbqF"
   },
   "outputs": [],
   "source": [
    "# Configuration and Data Paths\n",
    "TRAINING_DIR = \"cats-v-non-cats/training/\"\n",
    "VALIDATION_DIR = \"cats-v-non-cats/validation/\"\n",
    "TESTING_DIR = \"cats-v-non-cats/test/\"\n",
    "\n",
    "# Define whether to include test split or not\n",
    "INCLUDE_TEST = True\n",
    "\n",
    "# Baseline Configuration for comparison\n",
    "BASELINE_CONFIG = {\n",
    "    'learning_rate': 0.001,\n",
    "    'reg_strength': 0.00001,\n",
    "    'dropout_conv': 0.15,\n",
    "    'dropout_dense': 0.4,\n",
    "    'dense_units': 512,\n",
    "    'filters_multiplier': 0.75,\n",
    "    'batch_size': 32,\n",
    "    'beta_1': 0.8,\n",
    "    'beta_2': 0.99\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W0GBfK9DmgPg"
   },
   "outputs": [],
   "source": [
    "# Set up data generators\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "if INCLUDE_TEST:\n",
    "    test_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "l3ApKndwmh9e"
   },
   "outputs": [],
   "source": [
    "# Model Architecture Function\n",
    "def create_tuned_model(reg_strength=0.0001, dropout_conv=0.2, dropout_dense=0.4,\n",
    "                      dense_units=512, filters_multiplier=1):\n",
    "\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "\n",
    "    # Calculate filter sizes\n",
    "    filters1 = int(32 * filters_multiplier)\n",
    "    filters2 = int(64 * filters_multiplier)\n",
    "    filters3 = int(128 * filters_multiplier)\n",
    "\n",
    "    # First block\n",
    "    x = Conv2D(filters1, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters1, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Dropout(dropout_conv)(x)\n",
    "\n",
    "    # Second block\n",
    "    x = Conv2D(filters2, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters2, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Dropout(dropout_conv)(x)\n",
    "\n",
    "    # Third block\n",
    "    x = Conv2D(filters3, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters3, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Dropout(dropout_conv)(x)\n",
    "\n",
    "    # Global pooling and dense layers\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(dense_units, activation='relu', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_dense)(x)\n",
    "    x = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_strength))(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-DqKAzYYmk0h"
   },
   "outputs": [],
   "source": [
    "# Training and Evaluation Function\n",
    "def train_and_evaluate_model(config, epochs=5, param_name=None, param_value=None):\n",
    "    try:\n",
    "        # Create model with current configuration\n",
    "        model = create_tuned_model(\n",
    "            reg_strength=config['reg_strength'],\n",
    "            dropout_conv=config['dropout_conv'],\n",
    "            dropout_dense=config['dropout_dense'],\n",
    "            dense_units=config['dense_units'],\n",
    "            filters_multiplier=config['filters_multiplier']\n",
    "        )\n",
    "\n",
    "        # Compile model with current optimizer settings\n",
    "        optimizer = Adam(\n",
    "            learning_rate=config['learning_rate'],\n",
    "            beta_1=config['beta_1'],\n",
    "            beta_2=config['beta_2']\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', 'AUC']\n",
    "        )\n",
    "\n",
    "        train_generator = train_gen.flow_from_directory(\n",
    "            TRAINING_DIR,\n",
    "            target_size=(128, 128),\n",
    "            batch_size=config['batch_size'],\n",
    "            class_mode='binary',\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        validation_generator = validation_gen.flow_from_directory(\n",
    "            VALIDATION_DIR,\n",
    "            target_size=(128, 128),\n",
    "            batch_size=config['batch_size'],\n",
    "            class_mode='binary',\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        if INCLUDE_TEST:\n",
    "            test_generator = test_gen.flow_from_directory(\n",
    "                TESTING_DIR,\n",
    "                target_size=(128, 128),\n",
    "                batch_size=config['batch_size'],\n",
    "                class_mode='binary',\n",
    "                shuffle=True\n",
    "            )\n",
    "\n",
    "        # Define callbacks for training (no checkpoint saving)\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor='val_accuracy',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-8,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            mode='max'\n",
    "        )\n",
    "\n",
    "        # Only use learning rate reduction and early stopping - no checkpoint saving\n",
    "        callbacks = [reduce_lr, early_stop]\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "\n",
    "        # Get final validation metrics\n",
    "        val_loss, val_accuracy, val_auc = model.evaluate(validation_generator, verbose=0)\n",
    "\n",
    "        # Get test metrics if test set is available\n",
    "        test_results = {}\n",
    "        if INCLUDE_TEST:\n",
    "            test_loss, test_accuracy, test_auc = model.evaluate(test_generator, verbose=0)\n",
    "            test_results = {\n",
    "                'test_accuracy': test_accuracy,\n",
    "                'test_auc': test_auc,\n",
    "                'test_loss': test_loss\n",
    "            }\n",
    "\n",
    "        results = {\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_auc': val_auc,\n",
    "            'val_loss': val_loss,\n",
    "            'history': history.history,\n",
    "            'batch_size_used': config['batch_size']\n",
    "        }\n",
    "\n",
    "        # Add test results if available\n",
    "        results.update(test_results)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in training: {e}\")\n",
    "        return {\n",
    "            'val_accuracy': 0.0,\n",
    "            'val_auc': 0.0,\n",
    "            'val_loss': float('inf'),\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FUWQKKmDmw_s"
   },
   "outputs": [],
   "source": [
    "# Bayesian Optimization Setup\n",
    "def objective(trial):\n",
    "    config = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True),\n",
    "        'reg_strength': trial.suggest_float('reg_strength', 1e-6, 1e-2, log=True),\n",
    "        'dropout_conv': trial.suggest_float('dropout_conv', 0.1, 0.4),\n",
    "        'dropout_dense': trial.suggest_float('dropout_dense', 0.2, 0.7),\n",
    "        'dense_units': trial.suggest_categorical('dense_units', [128, 256, 512, 1024, 2048]),\n",
    "        'filters_multiplier': trial.suggest_float('filters_multiplier', 0.5, 2.0),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n",
    "        'beta_1': trial.suggest_float('beta_1', 0.7, 0.99),\n",
    "        'beta_2': trial.suggest_float('beta_2', 0.9, 0.9999)\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        result = train_and_evaluate_model(\n",
    "            config,\n",
    "            epochs=30,\n",
    "            param_name=f\"trial_{trial.number}\",\n",
    "            param_value=\"bayesian\"\n",
    "        )\n",
    "        accuracy = result['val_accuracy']\n",
    "\n",
    "        trial.set_user_attr('val_auc', result['val_auc'])\n",
    "        trial.set_user_attr('val_loss', result['val_loss'])\n",
    "\n",
    "        print(f\"Trial {trial.number}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Trial {trial.number} failed: {e}\")\n",
    "        # Return a low value for failed trials\n",
    "        return 0.0\n",
    "\n",
    "def run_bayesian_optimization(n_trials=50, timeout=None):\n",
    "\n",
    "    print(f\"Starting Bayesian Optimization with {n_trials} trials...\")\n",
    "\n",
    "    # Create study object\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',  # We want to maximize accuracy\n",
    "        sampler=TPESampler(seed=42),  # Tree-structured Parzen Estimator\n",
    "        study_name='cnn_hyperparameter_optimization'\n",
    "    )\n",
    "\n",
    "    # Run optimization\n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BAYESIAN OPTIMIZATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "    print(f\"Best trial number: {study.best_trial.number}\")\n",
    "    print(f\"Best validation accuracy: {study.best_value:.4f}\")\n",
    "\n",
    "    print(f\"\\nüèÜ Best hyperparameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Get additional metrics for best trial\n",
    "    best_trial = study.best_trial\n",
    "    if 'val_auc' in best_trial.user_attrs:\n",
    "        print(f\"\\nBest trial metrics:\")\n",
    "        print(f\"  Validation AUC: {best_trial.user_attrs['val_auc']:.4f}\")\n",
    "        print(f\"  Validation Loss: {best_trial.user_attrs['val_loss']:.4f}\")\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9HCDFSrxmycr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model...\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "267/267 [==============================] - 159s 590ms/step - loss: 0.5725 - accuracy: 0.7397 - auc: 0.8093 - val_loss: 0.7442 - val_accuracy: 0.5290 - val_auc: 0.8058 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "267/267 [==============================] - 128s 479ms/step - loss: 0.4627 - accuracy: 0.7910 - auc: 0.8703 - val_loss: 1.0826 - val_accuracy: 0.5796 - val_auc: 0.8180 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "267/267 [==============================] - 128s 477ms/step - loss: 0.4172 - accuracy: 0.8219 - auc: 0.8955 - val_loss: 0.4670 - val_accuracy: 0.7800 - val_auc: 0.9115 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "267/267 [==============================] - 129s 482ms/step - loss: 0.3691 - accuracy: 0.8432 - auc: 0.9187 - val_loss: 0.3393 - val_accuracy: 0.8539 - val_auc: 0.9348 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "267/267 [==============================] - 131s 489ms/step - loss: 0.3366 - accuracy: 0.8605 - auc: 0.9329 - val_loss: 0.4395 - val_accuracy: 0.8549 - val_auc: 0.9323 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "267/267 [==============================] - 130s 487ms/step - loss: 0.3129 - accuracy: 0.8720 - auc: 0.9424 - val_loss: 1.1386 - val_accuracy: 0.6199 - val_auc: 0.9162 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "267/267 [==============================] - 130s 488ms/step - loss: 0.3043 - accuracy: 0.8769 - auc: 0.9457 - val_loss: 0.3771 - val_accuracy: 0.8521 - val_auc: 0.9633 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "267/267 [==============================] - 130s 485ms/step - loss: 0.2851 - accuracy: 0.8864 - auc: 0.9526 - val_loss: 0.3529 - val_accuracy: 0.8511 - val_auc: 0.9575 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "267/267 [==============================] - 131s 491ms/step - loss: 0.2708 - accuracy: 0.8949 - auc: 0.9575 - val_loss: 0.7603 - val_accuracy: 0.7865 - val_auc: 0.9438 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "267/267 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.8926 - auc: 0.9587\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "267/267 [==============================] - 142s 533ms/step - loss: 0.2667 - accuracy: 0.8926 - auc: 0.9587 - val_loss: 0.6042 - val_accuracy: 0.7285 - val_auc: 0.9526 - lr: 0.0010\n",
      "\n",
      "Baseline Results:\n",
      "Accuracy: 0.7285\n",
      "AUC: 0.9526\n",
      "Loss: 0.6042\n"
     ]
    }
   ],
   "source": [
    "print(\"Training baseline model...\")\n",
    "baseline_result = train_and_evaluate_model(BASELINE_CONFIG, epochs=10)\n",
    "\n",
    "print(\"\\nBaseline Results:\")\n",
    "print(f\"Accuracy: {baseline_result['val_accuracy']:.4f}\")\n",
    "print(f\"AUC: {baseline_result['val_auc']:.4f}\")\n",
    "print(f\"Loss: {baseline_result['val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes here on this runtime, the model peaks at around epoch 7/8, afterwards the accuracy continues to increase (but validation accuracy decreases), which is a sign of overfitting.\n",
    "\n",
    "So may want to slowdown the training so it doesnt start overfitting this early. Our peak validation accuracy is around 85%. This is without any bayesian parameter optimization happening yet, so going to make another file to compare with a pretrained model the same circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MobileNetV2 baseline (transfer learning)...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
      "9406464/9406464 [==============================] - 1s 0us/step\n",
      "9406464/9406464 [==============================] - 1s 0us/step\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "Epoch 1/10\n",
      "267/267 [==============================] - 61s 222ms/step - loss: 0.1020 - accuracy: 0.9682 - auc: 0.9933 - val_loss: 0.0351 - val_accuracy: 0.9925 - val_auc: 0.9996 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "267/267 [==============================] - 61s 222ms/step - loss: 0.1020 - accuracy: 0.9682 - auc: 0.9933 - val_loss: 0.0351 - val_accuracy: 0.9925 - val_auc: 0.9996 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "267/267 [==============================] - 55s 204ms/step - loss: 0.0767 - accuracy: 0.9772 - auc: 0.9959 - val_loss: 0.0370 - val_accuracy: 0.9906 - val_auc: 0.9996 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "267/267 [==============================] - 55s 204ms/step - loss: 0.0767 - accuracy: 0.9772 - auc: 0.9959 - val_loss: 0.0370 - val_accuracy: 0.9906 - val_auc: 0.9996 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "267/267 [==============================] - 55s 206ms/step - loss: 0.0707 - accuracy: 0.9792 - auc: 0.9966 - val_loss: 0.0525 - val_accuracy: 0.9822 - val_auc: 0.9988 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "267/267 [==============================] - 55s 206ms/step - loss: 0.0707 - accuracy: 0.9792 - auc: 0.9966 - val_loss: 0.0525 - val_accuracy: 0.9822 - val_auc: 0.9988 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0604 - accuracy: 0.9828 - auc: 0.9977 - val_loss: 0.0319 - val_accuracy: 0.9925 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0604 - accuracy: 0.9828 - auc: 0.9977 - val_loss: 0.0319 - val_accuracy: 0.9925 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0638 - accuracy: 0.9809 - auc: 0.9973 - val_loss: 0.0328 - val_accuracy: 0.9925 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0638 - accuracy: 0.9809 - auc: 0.9973 - val_loss: 0.0328 - val_accuracy: 0.9925 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "267/267 [==============================] - 55s 204ms/step - loss: 0.0622 - accuracy: 0.9815 - auc: 0.9974 - val_loss: 0.0280 - val_accuracy: 0.9944 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "267/267 [==============================] - 55s 204ms/step - loss: 0.0622 - accuracy: 0.9815 - auc: 0.9974 - val_loss: 0.0280 - val_accuracy: 0.9944 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0589 - accuracy: 0.9827 - auc: 0.9977 - val_loss: 0.0436 - val_accuracy: 0.9860 - val_auc: 0.9988 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0589 - accuracy: 0.9827 - auc: 0.9977 - val_loss: 0.0436 - val_accuracy: 0.9860 - val_auc: 0.9988 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0569 - accuracy: 0.9823 - auc: 0.9981 - val_loss: 0.0518 - val_accuracy: 0.9860 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0569 - accuracy: 0.9823 - auc: 0.9981 - val_loss: 0.0518 - val_accuracy: 0.9860 - val_auc: 0.9997 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "267/267 [==============================] - 55s 206ms/step - loss: 0.0579 - accuracy: 0.9802 - auc: 0.9981 - val_loss: 0.0342 - val_accuracy: 0.9906 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "267/267 [==============================] - 55s 206ms/step - loss: 0.0579 - accuracy: 0.9802 - auc: 0.9981 - val_loss: 0.0342 - val_accuracy: 0.9906 - val_auc: 0.9999 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0525 - accuracy: 0.9838 - auc: 0.9984 - val_loss: 0.0443 - val_accuracy: 0.9906 - val_auc: 0.9988 - lr: 0.0010\n",
      "267/267 [==============================] - 55s 205ms/step - loss: 0.0525 - accuracy: 0.9838 - auc: 0.9984 - val_loss: 0.0443 - val_accuracy: 0.9906 - val_auc: 0.9988 - lr: 0.0010\n",
      "\n",
      "MobileNetV2 Baseline Results:\n",
      "Accuracy: 0.9906\n",
      "AUC: 0.9988\n",
      "Loss: 0.0443\n",
      "\n",
      "=== QUICK COMPARISON ===\n",
      "Custom CNN  - Accuracy: 0.7285, AUC: 0.9526\n",
      "MobileNetV2 - Accuracy: 0.9906, AUC: 0.9988\n",
      "Accuracy diff (MobileNet - Custom): +0.2622\n",
      "\n",
      "MobileNetV2 Baseline Results:\n",
      "Accuracy: 0.9906\n",
      "AUC: 0.9988\n",
      "Loss: 0.0443\n",
      "\n",
      "=== QUICK COMPARISON ===\n",
      "Custom CNN  - Accuracy: 0.7285, AUC: 0.9526\n",
      "MobileNetV2 - Accuracy: 0.9906, AUC: 0.9988\n",
      "Accuracy diff (MobileNet - Custom): +0.2622\n"
     ]
    }
   ],
   "source": [
    "# MobileNetV2 transfer-learning baseline confined to this cell\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "def create_mobilenetv2_model(reg_strength=1e-4, dropout_dense=0.4, dense_units=512, input_shape=(128,128,3), train_base=False):\n",
    "    \"\"\"Build a MobileNetV2-based binary classifier (transfer learning).\n",
    "    Set train_base=True to fine-tune the backbone.\"\"\"\n",
    "    base = MobileNetV2(include_top=False, weights='imagenet', input_shape=input_shape, pooling='avg')\n",
    "    base.trainable = train_base\n",
    "    x = base.output\n",
    "    x = Dense(dense_units, activation='relu', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_dense)(x)\n",
    "    x = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_strength))(x)\n",
    "    return Model(inputs=base.input, outputs=x)\n",
    "\n",
    "def train_and_evaluate_mobilenet(config, epochs=5):\n",
    "    \"\"\"Train MobileNetV2 using MobileNet preprocessing and return same metrics structure as the custom trainer.\"\"\"\n",
    "    try:\n",
    "        # Use MobileNet preprocess_input for consistency with pretrained weights\n",
    "        mn_train_gen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                          rotation_range=20, width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                          shear_range=0.2, zoom_range=0.2, horizontal_flip=True,\n",
    "                                          brightness_range=[0.8,1.2], fill_mode='nearest')\n",
    "        mn_val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "        mn_test_gen = ImageDataGenerator(preprocessing_function=preprocess_input) if INCLUDE_TEST else None\n",
    "\n",
    "        model = create_mobilenetv2_model(reg_strength=config.get('reg_strength', 1e-4),\n",
    "                                         dropout_dense=config.get('dropout_dense', 0.4),\n",
    "                                         dense_units=config.get('dense_units', 512),\n",
    "                                         train_base=False)\n",
    "\n",
    "        optimizer = Adam(learning_rate=config['learning_rate'], beta_1=config['beta_1'], beta_2=config['beta_2'])\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "\n",
    "        train_generator = mn_train_gen.flow_from_directory(TRAINING_DIR, target_size=(128,128),\n",
    "                                                          batch_size=config['batch_size'], class_mode='binary', shuffle=True)\n",
    "        validation_generator = mn_val_gen.flow_from_directory(VALIDATION_DIR, target_size=(128,128),\n",
    "                                                               batch_size=config['batch_size'], class_mode='binary', shuffle=False)\n",
    "\n",
    "        if INCLUDE_TEST:\n",
    "            test_generator = mn_test_gen.flow_from_directory(TESTING_DIR, target_size=(128,128),\n",
    "                                                            batch_size=config['batch_size'], class_mode='binary', shuffle=True)\n",
    "\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr=1e-8, verbose=1)\n",
    "        early_stop = EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1, mode='max')\n",
    "        callbacks = [reduce_lr, early_stop]\n",
    "\n",
    "        history = model.fit(train_generator, validation_data=validation_generator, epochs=epochs, verbose=1, callbacks=callbacks)\n",
    "\n",
    "        val_loss, val_accuracy, val_auc = model.evaluate(validation_generator, verbose=0)\n",
    "        results = {'val_accuracy': val_accuracy, 'val_auc': val_auc, 'val_loss': val_loss, 'history': history.history, 'batch_size_used': config['batch_size']}\n",
    "\n",
    "        if INCLUDE_TEST:\n",
    "            test_loss, test_accuracy, test_auc = model.evaluate(test_generator, verbose=0)\n",
    "            results.update({'test_accuracy': test_accuracy, 'test_auc': test_auc, 'test_loss': test_loss})\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'MobileNet training error: {e}')\n",
    "        return {'val_accuracy': 0.0, 'val_auc': 0.0, 'val_loss': float('inf'), 'error': str(e)}\n",
    "\n",
    "# Run MobileNetV2 baseline here (safe to run multiple times; checks for baseline_result)\n",
    "if 'baseline_result' in globals():\n",
    "    print('\\nTraining MobileNetV2 baseline (transfer learning)...')\n",
    "    mobilenet_config = BASELINE_CONFIG.copy()\n",
    "    mobilenet_result = train_and_evaluate_mobilenet(mobilenet_config, epochs=10)\n",
    "\n",
    "    print('\\nMobileNetV2 Baseline Results:')\n",
    "    print(f\"Accuracy: {mobilenet_result['val_accuracy']:.4f}\")\n",
    "    print(f\"AUC: {mobilenet_result['val_auc']:.4f}\")\n",
    "    print(f\"Loss: {mobilenet_result['val_loss']:.4f}\")\n",
    "\n",
    "    try:\n",
    "        print('\\n=== QUICK COMPARISON ===')\n",
    "        print(f\"Custom CNN  - Accuracy: {baseline_result['val_accuracy']:.4f}, AUC: {baseline_result['val_auc']:.4f}\")\n",
    "        print(f\"MobileNetV2 - Accuracy: {mobilenet_result['val_accuracy']:.4f}, AUC: {mobilenet_result['val_auc']:.4f}\")\n",
    "        acc_diff = mobilenet_result['val_accuracy'] - baseline_result['val_accuracy']\n",
    "        print(f\"Accuracy diff (MobileNet - Custom): {acc_diff:+.4f}\")\n",
    "    except Exception as e:\n",
    "        print('Comparison failed:', e)\n",
    "else:\n",
    "    print('baseline_result not found - run the custom baseline cell first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZ957ewtmzm4"
   },
   "outputs": [],
   "source": [
    "study = run_bayesian_optimization(n_trials=50)  # Increase to 100-200 for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQGbBXbwm04I"
   },
   "outputs": [],
   "source": [
    "# Analyze and Visualize Results\n",
    "def analyze_bayesian_results(study):\n",
    "    # Train final model with best parameters\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üî• TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    best_config = study.best_params\n",
    "\n",
    "    # Train with more epochs for final model\n",
    "    final_result = train_and_evaluate_model(\n",
    "        best_config,\n",
    "        epochs=60,  # More epochs for final training\n",
    "        param_name=\"bayesian_best\",\n",
    "        param_value=\"final\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüìä FINAL RESULTS COMPARISON:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Baseline  - Accuracy: {baseline_result['val_accuracy']:.4f}, AUC: {baseline_result['val_auc']:.4f}\")\n",
    "    print(f\"Bayesian  - Accuracy: {final_result['val_accuracy']:.4f}, AUC: {final_result['val_auc']:.4f}\")\n",
    "    print(f\"Improvement - Accuracy: {final_result['val_accuracy'] - baseline_result['val_accuracy']:+.4f}, AUC: {final_result['val_auc'] - baseline_result['val_auc']:+.4f}\")\n",
    "\n",
    "    # Plot optimization history\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # 1. Optimization history\n",
    "    trial_numbers = [trial.number for trial in study.trials]\n",
    "    trial_values = [trial.value if trial.value is not None else 0 for trial in study.trials]\n",
    "\n",
    "    axes[0, 0].plot(trial_numbers, trial_values, 'b-', alpha=0.6)\n",
    "    axes[0, 0].scatter(trial_numbers, trial_values, c=trial_values, cmap='viridis', s=30)\n",
    "    axes[0, 0].axhline(y=baseline_result['val_accuracy'], color='red', linestyle='--',\n",
    "                       label=f'Baseline ({baseline_result[\"val_accuracy\"]:.4f})')\n",
    "    axes[0, 0].set_xlabel('Trial Number')\n",
    "    axes[0, 0].set_ylabel('Validation Accuracy')\n",
    "    axes[0, 0].set_title('Optimization History')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Parameter importance (if available)\n",
    "    try:\n",
    "        importance = optuna.importance.get_param_importances(study)\n",
    "        params = list(importance.keys())\n",
    "        values = list(importance.values())\n",
    "\n",
    "        axes[0, 1].barh(params, values)\n",
    "        axes[0, 1].set_xlabel('Importance')\n",
    "        axes[0, 1].set_title('Hyperparameter Importance')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    except:\n",
    "        axes[0, 1].text(0.5, 0.5, 'Parameter importance\\nnot available\\n(need more trials)',\n",
    "                        ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        axes[0, 1].set_title('Hyperparameter Importance')\n",
    "\n",
    "    # 3. Best vs worst trials comparison\n",
    "    best_trials = sorted(study.trials, key=lambda t: t.value if t.value else 0, reverse=True)[:5]\n",
    "    worst_trials = sorted(study.trials, key=lambda t: t.value if t.value else 0)[:5]\n",
    "\n",
    "    best_values = [t.value for t in best_trials if t.value is not None]\n",
    "    worst_values = [t.value for t in worst_trials if t.value is not None]\n",
    "\n",
    "    axes[1, 0].bar(range(len(best_values)), best_values, color='green', alpha=0.7, label='Best 5 trials')\n",
    "    axes[1, 0].bar(range(len(best_values), len(best_values) + len(worst_values)),\n",
    "                   worst_values, color='red', alpha=0.7, label='Worst 5 trials')\n",
    "    axes[1, 0].set_ylabel('Validation Accuracy')\n",
    "    axes[1, 0].set_title('Best vs Worst Trials')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Learning rate vs accuracy scatter\n",
    "    lr_values = []\n",
    "    acc_values = []\n",
    "    for trial in study.trials:\n",
    "        if trial.value is not None and 'learning_rate' in trial.params:\n",
    "            lr_values.append(trial.params['learning_rate'])\n",
    "            acc_values.append(trial.value)\n",
    "\n",
    "    if lr_values:\n",
    "        axes[1, 1].scatter(lr_values, acc_values, alpha=0.6, c=acc_values, cmap='viridis')\n",
    "        axes[1, 1].set_xscale('log')\n",
    "        axes[1, 1].set_xlabel('Learning Rate')\n",
    "        axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "        axes[1, 1].set_title('Learning Rate vs Accuracy')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bayesian_optimization_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return final_result, best_config\n",
    "\n",
    "# Run analysis\n",
    "if 'study' in locals():\n",
    "    bayesian_final_result, bayesian_best_config = analyze_bayesian_results(study)\n",
    "    print(\"‚úÖ Analysis complete!\")\n",
    "else:\n",
    "    print(\"‚ùå Run Bayesian optimization first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwNKJpGdm1_w"
   },
   "outputs": [],
   "source": [
    "# Final Summary and Model Comparison\n",
    "def final_summary():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üéØ BAYESIAN OPTIMIZATION SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    if 'bayesian_final_result' in locals():\n",
    "        baseline_acc = baseline_result['val_accuracy']\n",
    "        optimized_acc = bayesian_final_result['val_accuracy']\n",
    "        improvement = optimized_acc - baseline_acc\n",
    "\n",
    "        print(f\"üìà Performance Improvement:\")\n",
    "        print(f\"  Baseline Accuracy:  {baseline_acc:.4f}\")\n",
    "        print(f\"  Optimized Accuracy: {optimized_acc:.4f}\")\n",
    "        print(f\"  Improvement:        {improvement:+.4f} ({improvement/baseline_acc*100:+.2f}%)\")\n",
    "\n",
    "        print(f\"\\nüèÜ Best Configuration Found:\")\n",
    "        for param, value in bayesian_best_config.items():\n",
    "            baseline_val = BASELINE_CONFIG.get(param, \"N/A\")\n",
    "            print(f\"  {param:<18}: {value:<10} (baseline: {baseline_val})\")\n",
    "\n",
    "        print(f\"\\nüîç Key Insights:\")\n",
    "        print(f\"  ‚Ä¢ Total trials run: {len(study.trials)}\")\n",
    "        print(f\"  ‚Ä¢ Best trial: #{study.best_trial.number}\")\n",
    "        print(f\"  ‚Ä¢ Search space explored efficiently using Bayesian optimization\")\n",
    "        print(f\"  ‚Ä¢ Model saved as: {bayesian_final_result['model_filename']}\")\n",
    "\n",
    "        # Determine if optimization was successful\n",
    "        if improvement > 0.01:  # 1% improvement threshold\n",
    "            print(f\"\\n‚úÖ Optimization SUCCESSFUL! Significant improvement achieved.\")\n",
    "        elif improvement > 0:\n",
    "            print(f\"\\n‚úÖ Optimization successful with modest improvement.\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Baseline was already quite good. Consider:\")\n",
    "            print(f\"     ‚Ä¢ Running more trials\")\n",
    "            print(f\"     ‚Ä¢ Expanding search space\")\n",
    "            print(f\"     ‚Ä¢ Different optimization objectives\")\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå Bayesian optimization results not available.\")\n",
    "\n",
    "    print(f\"\\nüöÄ Next Steps:\")\n",
    "    print(f\"  ‚Ä¢ Use the best model for production\")\n",
    "    print(f\"  ‚Ä¢ Consider ensemble methods\")\n",
    "    print(f\"  ‚Ä¢ Test on holdout data\")\n",
    "    print(f\"  ‚Ä¢ Monitor performance in production\")\n",
    "\n",
    "# Run final summary\n",
    "final_summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNZ4Q9S58aix/ISGyxtFBeP",
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
