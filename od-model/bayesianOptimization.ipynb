{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyNZ4Q9S58aix/ISGyxtFBeP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"oyxuWT1jmYYf"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import json\n","import optuna\n","from optuna.samplers import TPESampler\n","\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, BatchNormalization, Input, Dropout, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.regularizers import l2"]},{"cell_type":"code","source":["# Unzip dataset in Google Colab\n","import os\n","import zipfile\n","import shutil\n","\n","zip_file_path = \"/content/cats-v-non-cats.zip\"\n","extract_dir = \"/content/cats-v-non-cats/\"\n","\n","os.makedirs(extract_dir, exist_ok=True)\n","\n","try:\n","    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","        for file_info in zip_ref.infolist():\n","            # Extract only the files we want, avoiding the extra directory structure and __MACOSX\n","            if not file_info.filename.startswith('__MACOSX/'):\n","                # Remove the leading 'cats-v-dogs/' from the filename if it exists\n","                arcname = file_info.filename\n","                if arcname.startswith('cats-v-non-cats/'):\n","                    arcname = arcname[len('cats-v-non-cats/'):]\n","\n","                # Only extract if the filename is not empty after removing the prefix\n","                if arcname:\n","                    file_info.filename = arcname\n","                    zip_ref.extract(file_info, extract_dir)\n","\n","\n","    print(f\"File extracted to: {extract_dir}\")\n","\n","    # Remove the __MACOSX folder if it exists\n","    macosx_folder = os.path.join(\"/content/\", '__MACOSX')\n","    if os.path.exists(macosx_folder):\n","        shutil.rmtree(macosx_folder)\n","        print(f\"Folder '{macosx_folder}' deleted successfully.\")\n","\n","except Exception as e:\n","    print(f\"An error occurred during unzipping: {e}\")"],"metadata":{"id":"B0gBkmf8nuwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configuration and Data Paths\n","TRAINING_DIR = \"cats-v-non-cats/training/\"\n","VALIDATION_DIR = \"cats-v-non-cats/validation/\"\n","TESTING_DIR = \"cats-v-non-cats/test/\"\n","\n","# Define whether to include test split or not\n","INCLUDE_TEST = True\n","\n","# Baseline Configuration for comparison\n","BASELINE_CONFIG = {\n","    'learning_rate': 0.001,\n","    'reg_strength': 0.00001,\n","    'dropout_conv': 0.15,\n","    'dropout_dense': 0.4,\n","    'dense_units': 512,\n","    'filters_multiplier': 0.75,\n","    'batch_size': 32,\n","    'beta_1': 0.8,\n","    'beta_2': 0.99\n","}"],"metadata":{"id":"vvzFQ3vKmbqF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up data generators\n","train_gen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    brightness_range=[0.8, 1.2],\n","    fill_mode='nearest'\n",")\n","\n","validation_gen = ImageDataGenerator(rescale=1./255)\n","\n","if INCLUDE_TEST:\n","    test_gen = ImageDataGenerator(rescale=1./255)"],"metadata":{"id":"W0GBfK9DmgPg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model Architecture Function\n","def create_tuned_model(reg_strength=0.0001, dropout_conv=0.2, dropout_dense=0.4,\n","                      dense_units=512, filters_multiplier=1):\n","\n","    inputs = Input(shape=(128, 128, 3))\n","\n","    # Calculate filter sizes\n","    filters1 = int(32 * filters_multiplier)\n","    filters2 = int(64 * filters_multiplier)\n","    filters3 = int(128 * filters_multiplier)\n","\n","    # First block\n","    x = Conv2D(filters1, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(inputs)\n","    x = BatchNormalization()(x)\n","    x = Conv2D(filters1, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n","    x = MaxPooling2D(2, 2)(x)\n","    x = Dropout(dropout_conv)(x)\n","\n","    # Second block\n","    x = Conv2D(filters2, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n","    x = BatchNormalization()(x)\n","    x = Conv2D(filters2, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n","    x = MaxPooling2D(2, 2)(x)\n","    x = Dropout(dropout_conv)(x)\n","\n","    # Third block\n","    x = Conv2D(filters3, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n","    x = BatchNormalization()(x)\n","    x = Conv2D(filters3, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n","    x = MaxPooling2D(2, 2)(x)\n","    x = Dropout(dropout_conv)(x)\n","\n","    # Global pooling and dense layers\n","    x = GlobalAveragePooling2D()(x)\n","    x = Dense(dense_units, activation='relu', kernel_regularizer=l2(reg_strength))(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(dropout_dense)(x)\n","    x = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_strength))(x)\n","\n","    return Model(inputs=inputs, outputs=x)"],"metadata":{"id":"l3ApKndwmh9e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training and Evaluation Function\n","def train_and_evaluate_model(config, epochs=5, param_name=None, param_value=None):\n","    try:\n","        # Create model with current configuration\n","        model = create_tuned_model(\n","            reg_strength=config['reg_strength'],\n","            dropout_conv=config['dropout_conv'],\n","            dropout_dense=config['dropout_dense'],\n","            dense_units=config['dense_units'],\n","            filters_multiplier=config['filters_multiplier']\n","        )\n","\n","        # Compile model with current optimizer settings\n","        optimizer = Adam(\n","            learning_rate=config['learning_rate'],\n","            beta_1=config['beta_1'],\n","            beta_2=config['beta_2']\n","        )\n","\n","        model.compile(\n","            optimizer=optimizer,\n","            loss='binary_crossentropy',\n","            metrics=['accuracy', 'AUC']\n","        )\n","\n","        train_generator = train_gen.flow_from_directory(\n","            TRAINING_DIR,\n","            target_size=(128, 128),\n","            batch_size=config['batch_size'],\n","            class_mode='binary',\n","            shuffle=True\n","        )\n","\n","        validation_generator = validation_gen.flow_from_directory(\n","            VALIDATION_DIR,\n","            target_size=(128, 128),\n","            batch_size=config['batch_size'],\n","            class_mode='binary',\n","            shuffle=False\n","        )\n","\n","        if INCLUDE_TEST:\n","            test_generator = test_gen.flow_from_directory(\n","                TESTING_DIR,\n","                target_size=(128, 128),\n","                batch_size=config['batch_size'],\n","                class_mode='binary',\n","                shuffle=True\n","            )\n","\n","        # Define callbacks for training (no checkpoint saving)\n","        reduce_lr = ReduceLROnPlateau(\n","            monitor='val_accuracy',\n","            factor=0.2,\n","            patience=5,\n","            min_lr=1e-8,\n","            verbose=1\n","        )\n","\n","        early_stop = EarlyStopping(\n","            monitor='val_accuracy',\n","            patience=15,\n","            restore_best_weights=True,\n","            verbose=1,\n","            mode='max'\n","        )\n","\n","        # Only use learning rate reduction and early stopping - no checkpoint saving\n","        callbacks = [reduce_lr, early_stop]\n","\n","        # Train model\n","        history = model.fit(\n","            train_generator,\n","            validation_data=validation_generator,\n","            epochs=epochs,\n","            verbose=1,\n","            callbacks=callbacks\n","        )\n","\n","        # Get final validation metrics\n","        val_loss, val_accuracy, val_auc = model.evaluate(validation_generator, verbose=0)\n","\n","        # Get test metrics if test set is available\n","        test_results = {}\n","        if INCLUDE_TEST:\n","            test_loss, test_accuracy, test_auc = model.evaluate(test_generator, verbose=0)\n","            test_results = {\n","                'test_accuracy': test_accuracy,\n","                'test_auc': test_auc,\n","                'test_loss': test_loss\n","            }\n","\n","        results = {\n","            'val_accuracy': val_accuracy,\n","            'val_auc': val_auc,\n","            'val_loss': val_loss,\n","            'history': history.history,\n","            'batch_size_used': config['batch_size']\n","        }\n","\n","        # Add test results if available\n","        results.update(test_results)\n","\n","        return results\n","\n","    except Exception as e:\n","        print(f\"Error in training: {e}\")\n","        return {\n","            'val_accuracy': 0.0,\n","            'val_auc': 0.0,\n","            'val_loss': float('inf'),\n","            'error': str(e)\n","        }"],"metadata":{"id":"-DqKAzYYmk0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bayesian Optimization Setup\n","def objective(trial):\n","    config = {\n","        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True),\n","        'reg_strength': trial.suggest_float('reg_strength', 1e-6, 1e-2, log=True),\n","        'dropout_conv': trial.suggest_float('dropout_conv', 0.1, 0.4),\n","        'dropout_dense': trial.suggest_float('dropout_dense', 0.2, 0.7),\n","        'dense_units': trial.suggest_categorical('dense_units', [128, 256, 512, 1024, 2048]),\n","        'filters_multiplier': trial.suggest_float('filters_multiplier', 0.5, 2.0),\n","        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64, 128]),\n","        'beta_1': trial.suggest_float('beta_1', 0.7, 0.99),\n","        'beta_2': trial.suggest_float('beta_2', 0.9, 0.9999)\n","    }\n","\n","    try:\n","        result = train_and_evaluate_model(\n","            config,\n","            epochs=30,\n","            param_name=f\"trial_{trial.number}\",\n","            param_value=\"bayesian\"\n","        )\n","        accuracy = result['val_accuracy']\n","\n","        trial.set_user_attr('val_auc', result['val_auc'])\n","        trial.set_user_attr('val_loss', result['val_loss'])\n","\n","        print(f\"Trial {trial.number}: Accuracy = {accuracy:.4f}\")\n","\n","        return accuracy\n","\n","    except Exception as e:\n","        print(f\"Trial {trial.number} failed: {e}\")\n","        # Return a low value for failed trials\n","        return 0.0\n","\n","def run_bayesian_optimization(n_trials=50, timeout=None):\n","\n","    print(f\"Starting Bayesian Optimization with {n_trials} trials...\")\n","\n","    # Create study object\n","    study = optuna.create_study(\n","        direction='maximize',  # We want to maximize accuracy\n","        sampler=TPESampler(seed=42),  # Tree-structured Parzen Estimator\n","        study_name='cnn_hyperparameter_optimization'\n","    )\n","\n","    # Run optimization\n","    study.optimize(objective, n_trials=n_trials, timeout=timeout)\n","\n","    # Print results\n","    print(f\"\\n{'='*60}\")\n","    print(\"BAYESIAN OPTIMIZATION RESULTS\")\n","    print(f\"{'='*60}\")\n","\n","    print(f\"Number of finished trials: {len(study.trials)}\")\n","    print(f\"Best trial number: {study.best_trial.number}\")\n","    print(f\"Best validation accuracy: {study.best_value:.4f}\")\n","\n","    print(f\"\\nüèÜ Best hyperparameters:\")\n","    for key, value in study.best_params.items():\n","        print(f\"  {key}: {value}\")\n","\n","    # Get additional metrics for best trial\n","    best_trial = study.best_trial\n","    if 'val_auc' in best_trial.user_attrs:\n","        print(f\"\\nBest trial metrics:\")\n","        print(f\"  Validation AUC: {best_trial.user_attrs['val_auc']:.4f}\")\n","        print(f\"  Validation Loss: {best_trial.user_attrs['val_loss']:.4f}\")\n","\n","    return study"],"metadata":{"id":"FUWQKKmDmw_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training baseline model...\")\n","baseline_result = train_and_evaluate_model(BASELINE_CONFIG, epochs=40)\n","\n","print(\"\\nBaseline Results:\")\n","print(f\"Accuracy: {baseline_result['val_accuracy']:.4f}\")\n","print(f\"AUC: {baseline_result['val_auc']:.4f}\")\n","print(f\"Loss: {baseline_result['val_loss']:.4f}\")"],"metadata":{"id":"9HCDFSrxmycr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["study = run_bayesian_optimization(n_trials=50)  # Increase to 100-200 for production"],"metadata":{"id":"jZ957ewtmzm4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analyze and Visualize Results\n","def analyze_bayesian_results(study):\n","    # Train final model with best parameters\n","    print(f\"\\n{'='*60}\")\n","    print(\"üî• TRAINING FINAL MODEL WITH BEST PARAMETERS\")\n","    print(f\"{'='*60}\")\n","\n","    best_config = study.best_params\n","\n","    # Train with more epochs for final model\n","    final_result = train_and_evaluate_model(\n","        best_config,\n","        epochs=60,  # More epochs for final training\n","        param_name=\"bayesian_best\",\n","        param_value=\"final\"\n","    )\n","\n","    print(f\"\\nüìä FINAL RESULTS COMPARISON:\")\n","    print(f\"{'='*50}\")\n","    print(f\"Baseline  - Accuracy: {baseline_result['val_accuracy']:.4f}, AUC: {baseline_result['val_auc']:.4f}\")\n","    print(f\"Bayesian  - Accuracy: {final_result['val_accuracy']:.4f}, AUC: {final_result['val_auc']:.4f}\")\n","    print(f\"Improvement - Accuracy: {final_result['val_accuracy'] - baseline_result['val_accuracy']:+.4f}, AUC: {final_result['val_auc'] - baseline_result['val_auc']:+.4f}\")\n","\n","    # Plot optimization history\n","    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","\n","    # 1. Optimization history\n","    trial_numbers = [trial.number for trial in study.trials]\n","    trial_values = [trial.value if trial.value is not None else 0 for trial in study.trials]\n","\n","    axes[0, 0].plot(trial_numbers, trial_values, 'b-', alpha=0.6)\n","    axes[0, 0].scatter(trial_numbers, trial_values, c=trial_values, cmap='viridis', s=30)\n","    axes[0, 0].axhline(y=baseline_result['val_accuracy'], color='red', linestyle='--',\n","                       label=f'Baseline ({baseline_result[\"val_accuracy\"]:.4f})')\n","    axes[0, 0].set_xlabel('Trial Number')\n","    axes[0, 0].set_ylabel('Validation Accuracy')\n","    axes[0, 0].set_title('Optimization History')\n","    axes[0, 0].legend()\n","    axes[0, 0].grid(True, alpha=0.3)\n","\n","    # 2. Parameter importance (if available)\n","    try:\n","        importance = optuna.importance.get_param_importances(study)\n","        params = list(importance.keys())\n","        values = list(importance.values())\n","\n","        axes[0, 1].barh(params, values)\n","        axes[0, 1].set_xlabel('Importance')\n","        axes[0, 1].set_title('Hyperparameter Importance')\n","        axes[0, 1].grid(True, alpha=0.3)\n","    except:\n","        axes[0, 1].text(0.5, 0.5, 'Parameter importance\\nnot available\\n(need more trials)',\n","                        ha='center', va='center', transform=axes[0, 1].transAxes)\n","        axes[0, 1].set_title('Hyperparameter Importance')\n","\n","    # 3. Best vs worst trials comparison\n","    best_trials = sorted(study.trials, key=lambda t: t.value if t.value else 0, reverse=True)[:5]\n","    worst_trials = sorted(study.trials, key=lambda t: t.value if t.value else 0)[:5]\n","\n","    best_values = [t.value for t in best_trials if t.value is not None]\n","    worst_values = [t.value for t in worst_trials if t.value is not None]\n","\n","    axes[1, 0].bar(range(len(best_values)), best_values, color='green', alpha=0.7, label='Best 5 trials')\n","    axes[1, 0].bar(range(len(best_values), len(best_values) + len(worst_values)),\n","                   worst_values, color='red', alpha=0.7, label='Worst 5 trials')\n","    axes[1, 0].set_ylabel('Validation Accuracy')\n","    axes[1, 0].set_title('Best vs Worst Trials')\n","    axes[1, 0].legend()\n","    axes[1, 0].grid(True, alpha=0.3)\n","\n","    # 4. Learning rate vs accuracy scatter\n","    lr_values = []\n","    acc_values = []\n","    for trial in study.trials:\n","        if trial.value is not None and 'learning_rate' in trial.params:\n","            lr_values.append(trial.params['learning_rate'])\n","            acc_values.append(trial.value)\n","\n","    if lr_values:\n","        axes[1, 1].scatter(lr_values, acc_values, alpha=0.6, c=acc_values, cmap='viridis')\n","        axes[1, 1].set_xscale('log')\n","        axes[1, 1].set_xlabel('Learning Rate')\n","        axes[1, 1].set_ylabel('Validation Accuracy')\n","        axes[1, 1].set_title('Learning Rate vs Accuracy')\n","        axes[1, 1].grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.savefig('bayesian_optimization_analysis.png', dpi=300, bbox_inches='tight')\n","    plt.show()\n","\n","    return final_result, best_config\n","\n","# Run analysis\n","if 'study' in locals():\n","    bayesian_final_result, bayesian_best_config = analyze_bayesian_results(study)\n","    print(\"‚úÖ Analysis complete!\")\n","else:\n","    print(\"‚ùå Run Bayesian optimization first!\")"],"metadata":{"id":"EQGbBXbwm04I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Final Summary and Model Comparison\n","def final_summary():\n","    print(f\"\\n{'='*70}\")\n","    print(\"üéØ BAYESIAN OPTIMIZATION SUMMARY\")\n","    print(f\"{'='*70}\")\n","\n","    if 'bayesian_final_result' in locals():\n","        baseline_acc = baseline_result['val_accuracy']\n","        optimized_acc = bayesian_final_result['val_accuracy']\n","        improvement = optimized_acc - baseline_acc\n","\n","        print(f\"üìà Performance Improvement:\")\n","        print(f\"  Baseline Accuracy:  {baseline_acc:.4f}\")\n","        print(f\"  Optimized Accuracy: {optimized_acc:.4f}\")\n","        print(f\"  Improvement:        {improvement:+.4f} ({improvement/baseline_acc*100:+.2f}%)\")\n","\n","        print(f\"\\nüèÜ Best Configuration Found:\")\n","        for param, value in bayesian_best_config.items():\n","            baseline_val = BASELINE_CONFIG.get(param, \"N/A\")\n","            print(f\"  {param:<18}: {value:<10} (baseline: {baseline_val})\")\n","\n","        print(f\"\\nüîç Key Insights:\")\n","        print(f\"  ‚Ä¢ Total trials run: {len(study.trials)}\")\n","        print(f\"  ‚Ä¢ Best trial: #{study.best_trial.number}\")\n","        print(f\"  ‚Ä¢ Search space explored efficiently using Bayesian optimization\")\n","        print(f\"  ‚Ä¢ Model saved as: {bayesian_final_result['model_filename']}\")\n","\n","        # Determine if optimization was successful\n","        if improvement > 0.01:  # 1% improvement threshold\n","            print(f\"\\n‚úÖ Optimization SUCCESSFUL! Significant improvement achieved.\")\n","        elif improvement > 0:\n","            print(f\"\\n‚úÖ Optimization successful with modest improvement.\")\n","        else:\n","            print(f\"\\n‚ö†Ô∏è  Baseline was already quite good. Consider:\")\n","            print(f\"     ‚Ä¢ Running more trials\")\n","            print(f\"     ‚Ä¢ Expanding search space\")\n","            print(f\"     ‚Ä¢ Different optimization objectives\")\n","\n","    else:\n","        print(\"‚ùå Bayesian optimization results not available.\")\n","\n","    print(f\"\\nüöÄ Next Steps:\")\n","    print(f\"  ‚Ä¢ Use the best model for production\")\n","    print(f\"  ‚Ä¢ Consider ensemble methods\")\n","    print(f\"  ‚Ä¢ Test on holdout data\")\n","    print(f\"  ‚Ä¢ Monitor performance in production\")\n","\n","# Run final summary\n","final_summary()"],"metadata":{"id":"jwNKJpGdm1_w"},"execution_count":null,"outputs":[]}]}