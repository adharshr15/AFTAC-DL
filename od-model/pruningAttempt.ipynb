{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Pruning & Benchmarking for Cats vs Non-Cats CNN\n",
        "\n",
        "This notebook loads your trained `CustomCNN` from `checkpoints/final_best.pt`,\n",
        "applies unstructured pruning at various sparsity levels, and reports metrics on\n",
        "accuracy, size, sparsity, FLOPs, and throughput.\n",
        "\n",
        "**Assumed structure**:\n",
        "- `checkpoints/final_best.pt`\n",
        "- `cats-v-non-cats/validation/`\n",
        "- `cats-v-non-cats/test/`\n",
        "\n",
        "Adjust paths if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
        "\n",
        "CHECKPOINT_PATH = \"checkpoints/final_best.pt\"\n",
        "IMG_SIZE = 128\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "AMP = torch.cuda.is_available()\n",
        "\n",
        "VALIDATION_DIR = \"cats-v-non-cats/validation/\"\n",
        "TESTING_DIR = \"cats-v-non-cats/test/\"\n",
        "\n",
        "PRUNING_AMOUNTS = [0.2, 0.4, 0.6, 0.8]\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Mixed precision: {AMP}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Definition\n",
        "Must match your training notebook so the checkpoint loads correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, reg_strength: float, dropout_conv: float, dropout_dense: float,\n",
        "                 dense_units: int, filters_multiplier: float):\n",
        "        super().__init__()\n",
        "        f1 = max(8, int(32 * filters_multiplier))\n",
        "        f2 = max(16, int(64 * filters_multiplier))\n",
        "        f3 = max(32, int(128 * filters_multiplier))\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, f1, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(f1),\n",
        "            nn.Conv2d(f1, f1, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(dropout_conv)\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(f1, f2, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(f2),\n",
        "            nn.Conv2d(f2, f2, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(dropout_conv)\n",
        "        )\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(f2, f3, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(f3),\n",
        "            nn.Conv2d(f3, f3, 3, padding=1), nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(dropout_conv)\n",
        "        )\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(f3, dense_units), nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(dense_units),\n",
        "            nn.Dropout(dropout_dense),\n",
        "            nn.Linear(dense_units, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.gap(x)\n",
        "        return self.head(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n",
        "Validation + test loaders for evaluation only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_eval_loaders(batch_size=128):\n",
        "    val_tfms = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.CenterCrop(IMG_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "    ])\n",
        "\n",
        "    val_ds = datasets.ImageFolder(VALIDATION_DIR, transform=val_tfms)\n",
        "    test_ds = datasets.ImageFolder(TESTING_DIR, transform=val_tfms)\n",
        "\n",
        "    kwargs = dict(num_workers=4, pin_memory=True) if torch.cuda.is_available() else {}\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "    return val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "    total_time = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    loss_sum = 0.0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        yf = y.float().to(device, non_blocking=True)\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=AMP and device.type=='cuda'):\n",
        "            logits = model(x).squeeze(1)\n",
        "            loss = criterion(logits, yf)\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        total_time += (time.time() - start)\n",
        "        n_batches += 1\n",
        "\n",
        "        loss_sum += float(loss.item()) * x.size(0)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        all_probs.extend(probs.tolist())\n",
        "        all_labels.extend(y.cpu().numpy().tolist())\n",
        "\n",
        "    y_true = np.array(all_labels)\n",
        "    y_pred = (np.array(all_probs) >= 0.5).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': float((y_pred == y_true).mean()),\n",
        "        'loss': loss_sum / len(all_labels),\n",
        "        'auc': roc_auc_score(y_true, all_probs) if len(set(y_true)) > 1 else 0.0,\n",
        "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
        "        'avg_inference_time': total_time / max(n_batches, 1),\n",
        "        'throughput': len(all_labels) / total_time if total_time > 0 else 0.0\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "def measure_model_size(model):\n",
        "    param_size = 0\n",
        "    param_count = 0\n",
        "    buffer_size = 0\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param_count += param.nelement()\n",
        "        param_size += param.nelement() * param.element_size()\n",
        "\n",
        "    for buffer in model.buffers():\n",
        "        buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "    size_mb = (param_size + buffer_size) / 1024**2\n",
        "\n",
        "    return {\n",
        "        'size_mb': size_mb,\n",
        "        'param_count': param_count,\n",
        "        'param_size_mb': param_size / 1024**2,\n",
        "        'buffer_size_mb': buffer_size / 1024**2\n",
        "    }\n",
        "\n",
        "def count_zero_parameters(model):\n",
        "    zeros = 0\n",
        "    total = 0\n",
        "    for param in model.parameters():\n",
        "        if param is not None:\n",
        "            zeros += torch.sum(param == 0).item()\n",
        "            total += param.nelement()\n",
        "    sparsity = 100.0 * zeros / total if total > 0 else 0.0\n",
        "    return zeros, total, sparsity\n",
        "\n",
        "def estimate_flops(model, input_size=(1, 3, 128, 128)):\n",
        "    device = next(model.parameters()).device\n",
        "    dummy_input = torch.randn(input_size).to(device)\n",
        "    flops = 0\n",
        "\n",
        "    def conv_hook(module, input, output):\n",
        "        nonlocal flops\n",
        "        batch_size = input[0].size(0)\n",
        "        out_c = output.size(1)\n",
        "        out_h = output.size(2)\n",
        "        out_w = output.size(3)\n",
        "        k_h, k_w = module.kernel_size\n",
        "        in_c = module.in_channels\n",
        "        flops += batch_size * out_c * out_h * out_w * in_c * k_h * k_w\n",
        "\n",
        "    def linear_hook(module, input, output):\n",
        "        nonlocal flops\n",
        "        batch_size = input[0].size(0)\n",
        "        flops += batch_size * module.in_features * module.out_features\n",
        "\n",
        "    hooks = []\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            hooks.append(m.register_forward_hook(conv_hook))\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            hooks.append(m.register_forward_hook(linear_hook))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input)\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    return flops / 1e9  # GFLOPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pruning Functions\n",
        "Unstructured and structured pruning helpers. (Experiments below use unstructured.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_unstructured_pruning(model, amount=0.5):\n",
        "    pruned_model = copy.deepcopy(model)\n",
        "    for _, module in pruned_model.named_modules():\n",
        "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "            prune.remove(module, 'weight')\n",
        "    return pruned_model\n",
        "\n",
        "def apply_structured_pruning(model, amount=0.5):\n",
        "    pruned_model = copy.deepcopy(model)\n",
        "    for _, module in pruned_model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)\n",
        "            prune.remove(module, 'weight')\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)\n",
        "            prune.remove(module, 'weight')\n",
        "    return pruned_model\n",
        "\n",
        "def fine_tune_pruned_model(model, train_loader, val_loader, epochs=5, lr=1e-4):\n",
        "    model.to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.float().to(DEVICE, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=AMP):\n",
        "                logits = model(x).squeeze(1)\n",
        "                loss = criterion(logits, y)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        metrics = evaluate_model(model, val_loader, DEVICE)\n",
        "        if metrics['accuracy'] > best_acc:\n",
        "            best_acc = metrics['accuracy']\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Val Acc: {metrics['accuracy']:.4f}\")\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"LOADING TRAINED MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
        "config = ckpt[\"config\"]\n",
        "state = ckpt[\"state_dict\"]\n",
        "\n",
        "print(\"Model configuration:\")\n",
        "for k, v in config.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "original_model = CustomCNN(\n",
        "    config[\"reg_strength\"],\n",
        "    config[\"dropout_conv\"],\n",
        "    config[\"dropout_dense\"],\n",
        "    int(config[\"dense_units\"]),\n",
        "    float(config[\"filters_multiplier\"]),\n",
        ").to(DEVICE)\n",
        "\n",
        "original_model.load_state_dict(state, strict=True)\n",
        "original_model.eval()\n",
        "print(\"\\nâœ… Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"BASELINE MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "val_loader, test_loader = build_eval_loaders(batch_size=128)\n",
        "\n",
        "print(\"Evaluating baseline model on test set...\")\n",
        "baseline_metrics = evaluate_model(original_model, test_loader, DEVICE)\n",
        "baseline_size = measure_model_size(original_model)\n",
        "baseline_flops = estimate_flops(original_model)\n",
        "_, _, baseline_sparsity = count_zero_parameters(original_model)\n",
        "\n",
        "print(\"\\nðŸ“Š Baseline Performance:\")\n",
        "for k in [\"accuracy\", \"auc\", \"f1\", \"precision\", \"recall\", \"loss\"]:\n",
        "    print(f\"  {k.capitalize():<10}: {baseline_metrics[k]:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ’¾ Baseline Model Size:\")\n",
        "print(f\"  Total Size: {baseline_size['size_mb']:.2f} MB\")\n",
        "print(f\"  Parameters: {baseline_size['param_count']:,}\")\n",
        "print(f\"  Sparsity:   {baseline_sparsity:.2f}%\")\n",
        "\n",
        "print(\"\\nâš¡ Baseline Efficiency:\")\n",
        "print(f\"  Inference Time: {baseline_metrics['avg_inference_time']*1000:.2f} ms/batch\")\n",
        "print(f\"  Throughput:     {baseline_metrics['throughput']:.1f} samples/sec\")\n",
        "print(f\"  FLOPs:          {baseline_flops:.2f} GFLOPs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pruning Experiments\n",
        "Unstructured pruning at multiple sparsity levels; results recorded in `results`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"PRUNING EXPERIMENTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results = {\n",
        "    'baseline': {\n",
        "        'metrics': baseline_metrics,\n",
        "        'size': baseline_size,\n",
        "        'flops': baseline_flops,\n",
        "        'sparsity': baseline_sparsity\n",
        "    }\n",
        "}\n",
        "\n",
        "for prune_amount in PRUNING_AMOUNTS:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"UNSTRUCTURED PRUNING: {prune_amount*100:.0f}% sparsity\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    pruned_model = apply_unstructured_pruning(original_model, amount=prune_amount)\n",
        "    pruned_model.eval()\n",
        "\n",
        "    print(\"Evaluating pruned model (before fine-tuning)...\")\n",
        "    metrics = evaluate_model(pruned_model, test_loader, DEVICE)\n",
        "    size = measure_model_size(pruned_model)\n",
        "    flops = estimate_flops(pruned_model)\n",
        "    _, _, sparsity = count_zero_parameters(pruned_model)\n",
        "\n",
        "    print(f\"  Accuracy:  {metrics['accuracy']:.4f} (Î” {metrics['accuracy']-baseline_metrics['accuracy']:+.4f})\")\n",
        "    print(f\"  AUC:       {metrics['auc']:.4f} (Î” {metrics['auc']-baseline_metrics['auc']:+.4f})\")\n",
        "    print(f\"  F1:        {metrics['f1']:.4f}\")\n",
        "    print(f\"  Sparsity:  {sparsity:.2f}%\")\n",
        "    print(f\"  Size (MB): {size['size_mb']:.2f} (Î” {(1 - size['size_mb']/baseline_size['size_mb'])*100:.1f}% reduction)\")\n",
        "    print(f\"  FLOPs:     {flops:.2f} GFLOPs\")\n",
        "\n",
        "    results[f'unstructured_{prune_amount}'] = {\n",
        "        'metrics': metrics,\n",
        "        'size': size,\n",
        "        'flops': flops,\n",
        "        'sparsity': sparsity\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Generating visualizations...\")\n",
        "\n",
        "prune_levels = ['Baseline'] + [f'{int(p*100)}%' for p in PRUNING_AMOUNTS]\n",
        "accuracies = [baseline_metrics['accuracy']] + [results[f'unstructured_{p}']['metrics']['accuracy'] for p in PRUNING_AMOUNTS]\n",
        "sizes = [baseline_size['size_mb']] + [results[f'unstructured_{p}']['size']['size_mb'] for p in PRUNING_AMOUNTS]\n",
        "sparsities = [baseline_sparsity] + [results[f'unstructured_{p}']['sparsity'] for p in PRUNING_AMOUNTS]\n",
        "throughputs = [baseline_metrics['throughput']] + [results[f'unstructured_{p}']['metrics']['throughput'] for p in PRUNING_AMOUNTS]\n",
        "inference_times = [baseline_metrics['avg_inference_time']*1000] + [results[f'unstructured_{p}']['metrics']['avg_inference_time']*1000 for p in PRUNING_AMOUNTS]\n",
        "f1_scores = [baseline_metrics['f1']] + [results[f'unstructured_{p}']['metrics']['f1'] for p in PRUNING_AMOUNTS]\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('Model Pruning Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "axes[0, 0].plot(sparsities, accuracies, marker='o')\n",
        "axes[0, 0].set_xlabel('Sparsity (%)')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_title('Accuracy vs Sparsity')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].bar(range(len(sizes)), sizes)\n",
        "axes[0, 1].set_xticks(range(len(prune_levels)))\n",
        "axes[0, 1].set_xticklabels(prune_levels, rotation=45)\n",
        "axes[0, 1].set_ylabel('Size (MB)')\n",
        "axes[0, 1].set_title('Model Size Reduction')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[0, 2].plot(sparsities, throughputs, marker='s')\n",
        "axes[0, 2].set_xlabel('Sparsity (%)')\n",
        "axes[0, 2].set_ylabel('Throughput (samples/sec)')\n",
        "axes[0, 2].set_title('Inference Throughput')\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 0].bar(range(len(inference_times)), inference_times)\n",
        "axes[1, 0].set_xticks(range(len(prune_levels)))\n",
        "axes[1, 0].set_xticklabels(prune_levels, rotation=45)\n",
        "axes[1, 0].set_ylabel('Time (ms/batch)')\n",
        "axes[1, 0].set_title('Inference Time per Batch')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[1, 1].scatter(sizes, accuracies, s=80)\n",
        "for i, txt in enumerate(prune_levels):\n",
        "    axes[1, 1].annotate(txt, (sizes[i], accuracies[i]), fontsize=8, ha='center')\n",
        "axes[1, 1].set_xlabel('Model Size (MB)')\n",
        "axes[1, 1].set_ylabel('Accuracy')\n",
        "axes[1, 1].set_title('Accuracy vs Size')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 2].plot(sparsities, f1_scores, marker='D')\n",
        "axes[1, 2].set_xlabel('Sparsity (%)')\n",
        "axes[1, 2].set_ylabel('F1 Score')\n",
        "axes[1, 2].set_title('F1 Score vs Sparsity')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('pruning_analysis.png', dpi=300, bbox_inches='tight')\n",
        "print(\"âœ… Saved visualization to 'pruning_analysis.png'\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Table & Simple Recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"SUMMARY TABLE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n{:<15} {:<10} {:<10} {:<12} {:<12} {:<15}\".format(\n",
        "    \"Config\", \"Acc\", \"SizeMB\", \"Sparsity(%)\", \"Time(ms)\", \"Throughput\"))\n",
        "print(\"-\"*80)\n",
        "\n",
        "prune_levels = ['Baseline'] + [f'{int(p*100)}%' for p in PRUNING_AMOUNTS]\n",
        "\n",
        "for i, level in enumerate(prune_levels):\n",
        "    key = 'baseline' if i == 0 else f'unstructured_{PRUNING_AMOUNTS[i-1]}'\n",
        "    r = results[key]\n",
        "    print(\"{:<15} {:<10.4f} {:<10.2f} {:<12.2f} {:<12.2f} {:<15.1f}\".format(\n",
        "        level,\n",
        "        r['metrics']['accuracy'],\n",
        "        r['size']['size_mb'],\n",
        "        r['sparsity'],\n",
        "        r['metrics']['avg_inference_time'] * 1000,\n",
        "        r['metrics']['throughput']\n",
        "    ))\n",
        "\n",
        "print(\"\\nðŸŽ¯ Recommendation (heuristic):\")\n",
        "best_config = None\n",
        "for prune_amount in PRUNING_AMOUNTS:\n",
        "    key = f'unstructured_{prune_amount}'\n",
        "    acc_drop = baseline_metrics['accuracy'] - results[key]['metrics']['accuracy']\n",
        "    if acc_drop < 0.02:\n",
        "        best_config = (prune_amount, results[key])\n",
        "\n",
        "if best_config:\n",
        "    amount, res = best_config\n",
        "    print(f\"  Use {amount*100:.0f}% unstructured sparsity: <2% accuracy drop,\")\n",
        "    print(f\"  ~{(1 - res['size']['size_mb']/baseline_size['size_mb'])*100:.1f}% size reduction,\")\n",
        "    print(f\"  throughput change: {(res['metrics']['throughput']/baseline_metrics['throughput']-1)*100:+.1f}%\")\n",
        "else:\n",
        "    print(\"  All tested levels exceeded 2% accuracy drop. Try milder pruning or fine-tuning.\")\n",
        "\n",
        "print(\"\\n(For edge-device story, consider switching to structured channel pruning next.)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
