{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QqQthhNaJAk4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, BatchNormalization, Input, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4272\n",
      "4272\n",
      "534\n",
      "534\n",
      "535\n",
      "534\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir('cats-v-non-cats/training/cats')))\n",
    "print(len(os.listdir('cats-v-non-cats/training/non-cats')))\n",
    "\n",
    "print(len(os.listdir('cats-v-non-cats/validation/cats')))\n",
    "print(len(os.listdir('cats-v-non-cats/validation/non-cats')))\n",
    "\n",
    "print(len(os.listdir('cats-v-non-cats/test/cats')))\n",
    "print(len(os.listdir('cats-v-non-cats/test/non-cats')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ezw8Vtb-Je3L"
   },
   "outputs": [],
   "source": [
    "#Define data path\n",
    "TRAINING_DIR = \"cats-v-non-cats/training/\"\n",
    "VALIDATION_DIR = \"cats-v-non-cats/validation/\"\n",
    "TESTING_DIR = \"cats-v-non-cats/test/\"\n",
    "\n",
    "# Define whether to include test split or not\n",
    "INCLUDE_TEST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrMz3f2FLOPc"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total possible combinations: 26244\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning Configuration\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Define hyperparameter search space\n",
    "HYPERPARAMETER_SPACE = {\n",
    "    'learning_rate': [0.001, 0.0005, 0.0001],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'reg_strength': [0.0001, 0.0005, 0.001],\n",
    "    'dropout_conv': [0.2, 0.25, 0.3],\n",
    "    'dropout_dense': [0.4, 0.5, 0.6],\n",
    "    'dense_units': [256, 512, 1024],\n",
    "    'filters_multiplier': [1.25, 1.5, 2],\n",
    "    'beta_1': [0.8, 0.9, 0.95, 0.99],  # Adam momentum parameter\n",
    "    'beta_2': [0.99, 0.999, 0.9999]    # Adam variance parameter\n",
    "}\n",
    "\n",
    "# Track results\n",
    "tuning_results = []\n",
    "\n",
    "def create_tuned_model(reg_strength=0.0005, dropout_conv=0.25, dropout_dense=0.5, \n",
    "                       dense_units=512, filters_multiplier=1):\n",
    "\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    # Calculate filter sizes\n",
    "    filters1 = int(32 * filters_multiplier)\n",
    "    filters2 = int(64 * filters_multiplier)\n",
    "    filters3 = int(128 * filters_multiplier)\n",
    "\n",
    "    # First block\n",
    "    x = Conv2D(filters1, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters1, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Dropout(dropout_conv)(x)\n",
    "\n",
    "    # Second block\n",
    "    x = Conv2D(filters2, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters2, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Dropout(dropout_conv)(x)\n",
    "\n",
    "    # Third block\n",
    "    x = Conv2D(filters3, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters3, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    x = Dropout(dropout_conv)(x)\n",
    "\n",
    "    # Global pooling and dense layers\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(dense_units, activation='relu', kernel_regularizer=l2(reg_strength))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_dense)(x)\n",
    "    x = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_strength))(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=x)\n",
    "\n",
    "print(f\"Total possible combinations: {len(list(itertools.product(*HYPERPARAMETER_SPACE.values())))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart Hyperparameter Search Function\n",
    "def run_hyperparameter_search(max_trials=12, epochs_per_trial=50):\n",
    "\n",
    "    global tuning_results\n",
    "    \n",
    "    # Create directory for saved models\n",
    "    models_dir = \"hyperparameter_models\"\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    print(f\"Models will be saved in: {models_dir}/\")\n",
    "    \n",
    "    # Define strategic combinations to test (now including beta parameter variations)\n",
    "    strategic_combinations = [\n",
    "        # Baseline (current setup with default betas)\n",
    "        {'learning_rate': 0.0005, 'batch_size': 32, 'reg_strength': 0.0005, \n",
    "         'dropout_conv': 0.25, 'dropout_dense': 0.5, 'dense_units': 512, 'filters_multiplier': 1,\n",
    "         'beta_1': 0.9, 'beta_2': 0.999},\n",
    "        \n",
    "        # High momentum (higher beta_1) - good for noisy gradients\n",
    "        {'learning_rate': 0.001, 'batch_size': 32, 'reg_strength': 0.0001, \n",
    "         'dropout_conv': 0.2, 'dropout_dense': 0.4, 'dense_units': 512, 'filters_multiplier': 1,\n",
    "         'beta_1': 0.95, 'beta_2': 0.999},\n",
    "        \n",
    "        # Very high momentum - even smoother convergence\n",
    "        {'learning_rate': 0.0005, 'batch_size': 32, 'reg_strength': 0.0005, \n",
    "         'dropout_conv': 0.25, 'dropout_dense': 0.5, 'dense_units': 512, 'filters_multiplier': 1,\n",
    "         'beta_1': 0.99, 'beta_2': 0.999},\n",
    "        \n",
    "        # Low momentum (lower beta_1) - faster adaptation\n",
    "        {'learning_rate': 0.001, 'batch_size': 32, 'reg_strength': 0.0005, \n",
    "         'dropout_conv': 0.25, 'dropout_dense': 0.5, 'dense_units': 512, 'filters_multiplier': 1,\n",
    "         'beta_1': 0.8, 'beta_2': 0.999},\n",
    "        \n",
    "        # High variance adaptation (lower beta_2) - for dense gradients\n",
    "        {'learning_rate': 0.0005, 'batch_size': 32, 'reg_strength': 0.0005, \n",
    "         'dropout_conv': 0.25, 'dropout_dense': 0.5, 'dense_units': 512, 'filters_multiplier': 1,\n",
    "         'beta_1': 0.9, 'beta_2': 0.99},\n",
    "        \n",
    "        # Very high variance stability (higher beta_2) - for sparse gradients\n",
    "        {'learning_rate': 0.0001, 'batch_size': 32, 'reg_strength': 0.001, \n",
    "         'dropout_conv': 0.3, 'dropout_dense': 0.6, 'dense_units': 512, 'filters_multiplier': 1,\n",
    "         'beta_1': 0.9, 'beta_2': 0.9999},\n",
    "        \n",
    "        # Balanced high momentum + high variance stability\n",
    "        {'learning_rate': 0.0005, 'batch_size': 64, 'reg_strength': 0.0005, \n",
    "         'dropout_conv': 0.25, 'dropout_dense': 0.5, 'dense_units': 1024, 'filters_multiplier': 1.5,\n",
    "         'beta_1': 0.95, 'beta_2': 0.9999},\n",
    "        \n",
    "        # Fast adaptation setup (low momentum + low variance)\n",
    "        {'learning_rate': 0.001, 'batch_size': 16, 'reg_strength': 0.0001, \n",
    "         'dropout_conv': 0.2, 'dropout_dense': 0.4, 'dense_units': 512, 'filters_multiplier': 1,\n",
    "         'beta_1': 0.8, 'beta_2': 0.99},\n",
    "        \n",
    "        # Conservative setup with stable betas\n",
    "        {'learning_rate': 0.0001, 'batch_size': 64, 'reg_strength': 0.001, \n",
    "         'dropout_conv': 0.3, 'dropout_dense': 0.6, 'dense_units': 256, 'filters_multiplier': 1,\n",
    "         'beta_1': 0.95, 'beta_2': 0.9999},\n",
    "        \n",
    "        # High capacity with optimized betas\n",
    "        {'learning_rate': 0.0005, 'batch_size': 32, 'reg_strength': 0.0005, \n",
    "         'dropout_conv': 0.2, 'dropout_dense': 0.4, 'dense_units': 1024, 'filters_multiplier': 2,\n",
    "         'beta_1': 0.99, 'beta_2': 0.999},\n",
    "        \n",
    "        # Aggressive training with low momentum\n",
    "        {'learning_rate': 0.001, 'batch_size': 64, 'reg_strength': 0.0001, \n",
    "         'dropout_conv': 0.2, 'dropout_dense': 0.4, 'dense_units': 512, 'filters_multiplier': 1,\n",
    "         'beta_1': 0.8, 'beta_2': 0.99},\n",
    "        \n",
    "        # Ultra-stable training (highest betas)\n",
    "        {'learning_rate': 0.0001, 'batch_size': 32, 'reg_strength': 0.0005, \n",
    "         'dropout_conv': 0.25, 'dropout_dense': 0.5, 'dense_units': 512, 'filters_multiplier': 1,\n",
    "         'beta_1': 0.99, 'beta_2': 0.9999}\n",
    "    ]\n",
    "    \n",
    "    # Limit to max_trials\n",
    "    combinations_to_test = strategic_combinations[:max_trials]\n",
    "    \n",
    "    print(f\"Starting enhanced hyperparameter search with {len(combinations_to_test)} combinations...\")\n",
    "    print(f\"Each trial will run for {epochs_per_trial} epochs with early stopping.\")\n",
    "    print(\"Each best model will be saved with descriptive filename for later analysis.\")\n",
    "    \n",
    "    for trial_idx, params in enumerate(combinations_to_test):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TRIAL {trial_idx + 1}/{len(combinations_to_test)}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        print(f\"Adam Optimizer: lr={params['learning_rate']}, β₁={params['beta_1']}, β₂={params['beta_2']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Generate descriptive model filename\n",
    "        model_filename = f\"{models_dir}/trial_{trial_idx+1:02d}_lr{params['learning_rate']}_b1-{params['beta_1']}_b2-{params['beta_2']}_bs{params['batch_size']}_reg{params['reg_strength']}_du{params['dense_units']}.h5\"\n",
    "        \n",
    "        try:\n",
    "            # Create model with current parameters\n",
    "            model = create_tuned_model(\n",
    "                reg_strength=params['reg_strength'],\n",
    "                dropout_conv=params['dropout_conv'],\n",
    "                dropout_dense=params['dropout_dense'],\n",
    "                dense_units=params['dense_units'],\n",
    "                filters_multiplier=params['filters_multiplier']\n",
    "            )\n",
    "            \n",
    "            # Setup callbacks for this trial (with individual model saving)\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_accuracy',  # Focus on accuracy for tuning\n",
    "                factor=0.2,\n",
    "                patience=3,  # Shorter patience for tuning\n",
    "                min_lr=1e-8,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            early_stop = EarlyStopping(\n",
    "                monitor='val_accuracy',\n",
    "                patience=7,  # Shorter patience for tuning\n",
    "                restore_best_weights=True,\n",
    "                verbose=1,\n",
    "                mode='max'  # Maximize accuracy\n",
    "            )\n",
    "            \n",
    "            # Save best model for this trial\n",
    "            trial_checkpoint = ModelCheckpoint(\n",
    "                model_filename,\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1,\n",
    "                mode='max'\n",
    "            )\n",
    "            \n",
    "            callbacks = [reduce_lr, early_stop, trial_checkpoint]\n",
    "            \n",
    "            # Compile model with beta parameters\n",
    "            model.compile(\n",
    "                optimizer=Adam(\n",
    "                    learning_rate=params['learning_rate'], \n",
    "                    beta_1=params['beta_1'], \n",
    "                    beta_2=params['beta_2']\n",
    "                ),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', 'precision', 'recall', 'auc']\n",
    "            )\n",
    "            \n",
    "            # Create data generators with current batch size\n",
    "            train_generator = train_gen.flow_from_directory(\n",
    "                TRAINING_DIR,\n",
    "                target_size=(128, 128),\n",
    "                batch_size=params['batch_size'],\n",
    "                class_mode='binary',\n",
    "                shuffle=True\n",
    "            )\n",
    "            \n",
    "            validation_generator = validation_gen.flow_from_directory(\n",
    "                VALIDATION_DIR,\n",
    "                target_size=(128, 128),\n",
    "                batch_size=params['batch_size'],\n",
    "                class_mode='binary',\n",
    "                shuffle=False\n",
    "            )\n",
    "            \n",
    "            # Train model\n",
    "            history = model.fit(\n",
    "                train_generator,\n",
    "                epochs=epochs_per_trial,\n",
    "                validation_data=validation_generator,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Get best results\n",
    "            best_val_accuracy = max(history.history['val_accuracy'])\n",
    "            best_val_auc = max(history.history['val_auc'])\n",
    "            final_train_accuracy = history.history['accuracy'][-1]\n",
    "            final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "            \n",
    "            # Calculate training time\n",
    "            end_time = datetime.now()\n",
    "            training_time = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            # Store results with model filename\n",
    "            result = {\n",
    "                'trial': trial_idx + 1,\n",
    "                'params': params.copy(),\n",
    "                'best_val_accuracy': best_val_accuracy,\n",
    "                'best_val_auc': best_val_auc,\n",
    "                'final_train_accuracy': final_train_accuracy,\n",
    "                'final_val_accuracy': final_val_accuracy,\n",
    "                'epochs_run': len(history.history['accuracy']),\n",
    "                'training_time_seconds': training_time,\n",
    "                'model_filename': model_filename,\n",
    "                'timestamp': start_time.isoformat()\n",
    "            }\n",
    "            \n",
    "            tuning_results.append(result)\n",
    "            \n",
    "            print(f\"\\nTrial {trial_idx + 1} Results:\")\n",
    "            print(f\"Best Validation Accuracy: {best_val_accuracy:.4f}\")\n",
    "            print(f\"Best Validation AUC: {best_val_auc:.4f}\")\n",
    "            print(f\"Final Train Accuracy: {final_train_accuracy:.4f}\")\n",
    "            print(f\"Final Val Accuracy: {final_val_accuracy:.4f}\")\n",
    "            print(f\"Epochs Run: {len(history.history['accuracy'])}\")\n",
    "            print(f\"Training Time: {training_time:.1f} seconds\")\n",
    "            print(f\"Beta Parameters: β₁={params['beta_1']}, β₂={params['beta_2']}\")\n",
    "            print(f\"Model saved as: {model_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Trial {trial_idx + 1} failed with error: {str(e)}\")\n",
    "            result = {\n",
    "                'trial': trial_idx + 1,\n",
    "                'params': params.copy(),\n",
    "                'error': str(e),\n",
    "                'model_filename': None,\n",
    "                'timestamp': start_time.isoformat()\n",
    "            }\n",
    "            tuning_results.append(result)\n",
    "            continue\n",
    "    \n",
    "    # Print summary of all saved models\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAVED MODELS SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "    successful_trials = [r for r in tuning_results if 'error' not in r and 'model_filename' in r]\n",
    "    for result in successful_trials:\n",
    "        print(f\"Trial {result['trial']}: {result['model_filename']} (Acc: {result['best_val_accuracy']:.4f})\")\n",
    "    \n",
    "    return tuning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced analysis functions with model management ready!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Results Analysis and Visualization (with Beta Parameters and Model Info)\n",
    "def analyze_tuning_results():\n",
    "\n",
    "    if not tuning_results:\n",
    "        print(\"No tuning results available. Run the search first!\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame([r for r in tuning_results if 'error' not in r])\n",
    "    \n",
    "    if results_df.empty:\n",
    "        print(\"All trials failed. Check your setup and try again.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract parameter columns\n",
    "    param_columns = list(HYPERPARAMETER_SPACE.keys())\n",
    "    for param in param_columns:\n",
    "        results_df[param] = results_df['params'].apply(lambda x: x[param])\n",
    "    \n",
    "    # Sort by best validation accuracy\n",
    "    results_df_sorted = results_df.sort_values('best_val_accuracy', ascending=False)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ENHANCED HYPERPARAMETER TUNING RESULTS (with Adam Beta Analysis)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display top 5 results with beta parameter info and model filenames\n",
    "    print(\"\\nTOP 5 PERFORMING CONFIGURATIONS:\")\n",
    "    print(\"-\" * 80)\n",
    "    for idx, (_, row) in enumerate(results_df_sorted.head().iterrows()):\n",
    "        print(f\"\\nRank {idx + 1}: Trial {int(row['trial'])}\")\n",
    "        print(f\"Validation Accuracy: {row['best_val_accuracy']:.4f}\")\n",
    "        print(f\"Validation AUC: {row['best_val_auc']:.4f}\")\n",
    "        print(f\"Adam Betas: β₁={row['beta_1']}, β₂={row['beta_2']}\")\n",
    "        print(f\"Learning Rate: {row['learning_rate']}\")\n",
    "        if 'model_filename' in row and row['model_filename']:\n",
    "            print(f\"Saved Model: {row['model_filename']}\")\n",
    "        print(f\"Full Parameters: {row['params']}\")\n",
    "    \n",
    "    # Find best configuration\n",
    "    best_config = results_df_sorted.iloc[0]\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST CONFIGURATION:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Trial: {int(best_config['trial'])}\")\n",
    "    print(f\"Best Validation Accuracy: {best_config['best_val_accuracy']:.4f}\")\n",
    "    print(f\"Best Validation AUC: {best_config['best_val_auc']:.4f}\")\n",
    "    print(f\"Training Time: {best_config['training_time_seconds']:.1f} seconds\")\n",
    "    print(f\"Optimal Adam Parameters:\")\n",
    "    print(f\"  Learning Rate: {best_config['learning_rate']}\")\n",
    "    print(f\"  Beta_1 (momentum): {best_config['beta_1']}\")\n",
    "    print(f\"  Beta_2 (variance): {best_config['beta_2']}\")\n",
    "    if 'model_filename' in best_config and best_config['model_filename']:\n",
    "        print(f\"Best Model File: {best_config['model_filename']}\")\n",
    "    \n",
    "    # Beta parameter analysis\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"BETA PARAMETER ANALYSIS:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Group by beta values and show average performance\n",
    "    beta1_performance = results_df.groupby('beta_1')['best_val_accuracy'].agg(['mean', 'std', 'count'])\n",
    "    beta2_performance = results_df.groupby('beta_2')['best_val_accuracy'].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    print(f\"\\nBeta_1 (Momentum) Performance:\")\n",
    "    for beta1, stats in beta1_performance.iterrows():\n",
    "        print(f\"  β₁={beta1}: Avg Accuracy={stats['mean']:.4f} ±{stats['std']:.4f} (n={int(stats['count'])})\")\n",
    "    \n",
    "    print(f\"\\nBeta_2 (Variance) Performance:\")\n",
    "    for beta2, stats in beta2_performance.iterrows():\n",
    "        print(f\"  β₂={beta2}: Avg Accuracy={stats['mean']:.4f} ±{stats['std']:.4f} (n={int(stats['count'])})\")\n",
    "    \n",
    "    # Saved models summary\n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(\"SAVED MODELS SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "    models_info = results_df_sorted[['trial', 'best_val_accuracy', 'best_val_auc', 'model_filename']].copy()\n",
    "    models_info = models_info[models_info['model_filename'].notna()]\n",
    "    \n",
    "    print(f\"Total models saved: {len(models_info)}\")\n",
    "    print(\"\\nAll saved models (sorted by performance):\")\n",
    "    for _, row in models_info.iterrows():\n",
    "        filename = os.path.basename(row['model_filename']) if row['model_filename'] else 'N/A'\n",
    "        print(f\"  Trial {int(row['trial'])}: {filename}\")\n",
    "        print(f\"    Accuracy: {row['best_val_accuracy']:.4f}, AUC: {row['best_val_auc']:.4f}\")\n",
    "    \n",
    "    # Create enhanced visualizations\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.suptitle('Enhanced Hyperparameter Tuning Analysis (with Adam Beta Parameters)', fontsize=16)\n",
    "    \n",
    "    # 1. Accuracy vs Trial\n",
    "    axes[0,0].scatter(results_df['trial'], results_df['best_val_accuracy'], alpha=0.7)\n",
    "    axes[0,0].set_xlabel('Trial Number')\n",
    "    axes[0,0].set_ylabel('Best Validation Accuracy')\n",
    "    axes[0,0].set_title('Accuracy by Trial')\n",
    "    axes[0,0].grid(True)\n",
    "    \n",
    "    # 2. Learning Rate Impact\n",
    "    lr_groups = results_df.groupby('learning_rate')['best_val_accuracy'].mean()\n",
    "    axes[0,1].bar(range(len(lr_groups)), lr_groups.values)\n",
    "    axes[0,1].set_xticks(range(len(lr_groups)))\n",
    "    axes[0,1].set_xticklabels([f'{lr:.4f}' for lr in lr_groups.index])\n",
    "    axes[0,1].set_xlabel('Learning Rate')\n",
    "    axes[0,1].set_ylabel('Mean Validation Accuracy')\n",
    "    axes[0,1].set_title('Learning Rate Impact')\n",
    "    axes[0,1].grid(True)\n",
    "    \n",
    "    # 3. Beta_1 Impact\n",
    "    beta1_groups = results_df.groupby('beta_1')['best_val_accuracy'].mean()\n",
    "    axes[0,2].bar(range(len(beta1_groups)), beta1_groups.values)\n",
    "    axes[0,2].set_xticks(range(len(beta1_groups)))\n",
    "    axes[0,2].set_xticklabels([f'{b1:.2f}' for b1 in beta1_groups.index])\n",
    "    axes[0,2].set_xlabel('Beta_1 (Momentum)')\n",
    "    axes[0,2].set_ylabel('Mean Validation Accuracy')\n",
    "    axes[0,2].set_title('Beta_1 Impact')\n",
    "    axes[0,2].grid(True)\n",
    "    \n",
    "    # 4. Beta_2 Impact\n",
    "    beta2_groups = results_df.groupby('beta_2')['best_val_accuracy'].mean()\n",
    "    axes[1,0].bar(range(len(beta2_groups)), beta2_groups.values)\n",
    "    axes[1,0].set_xticks(range(len(beta2_groups)))\n",
    "    axes[1,0].set_xticklabels([f'{b2:.3f}' for b2 in beta2_groups.index])\n",
    "    axes[1,0].set_xlabel('Beta_2 (Variance)')\n",
    "    axes[1,0].set_ylabel('Mean Validation Accuracy')\n",
    "    axes[1,0].set_title('Beta_2 Impact')\n",
    "    axes[1,0].grid(True)\n",
    "    \n",
    "    # 5. Batch Size Impact\n",
    "    batch_groups = results_df.groupby('batch_size')['best_val_accuracy'].mean()\n",
    "    axes[1,1].bar(range(len(batch_groups)), batch_groups.values)\n",
    "    axes[1,1].set_xticks(range(len(batch_groups)))\n",
    "    axes[1,1].set_xticklabels(batch_groups.index)\n",
    "    axes[1,1].set_xlabel('Batch Size')\n",
    "    axes[1,1].set_ylabel('Mean Validation Accuracy')\n",
    "    axes[1,1].set_title('Batch Size Impact')\n",
    "    axes[1,1].grid(True)\n",
    "    \n",
    "    # 6. Beta_1 vs Beta_2 Scatter\n",
    "    scatter = axes[1,2].scatter(results_df['beta_1'], results_df['beta_2'], \n",
    "                               c=results_df['best_val_accuracy'], cmap='viridis', alpha=0.7)\n",
    "    axes[1,2].set_xlabel('Beta_1 (Momentum)')\n",
    "    axes[1,2].set_ylabel('Beta_2 (Variance)')\n",
    "    axes[1,2].set_title('Beta_1 vs Beta_2 (Color = Accuracy)')\n",
    "    plt.colorbar(scatter, ax=axes[1,2])\n",
    "    \n",
    "    # 7. Regularization Impact\n",
    "    reg_groups = results_df.groupby('reg_strength')['best_val_accuracy'].mean()\n",
    "    axes[2,0].bar(range(len(reg_groups)), reg_groups.values)\n",
    "    axes[2,0].set_xticks(range(len(reg_groups)))\n",
    "    axes[2,0].set_xticklabels([f'{reg:.4f}' for reg in reg_groups.index])\n",
    "    axes[2,0].set_xlabel('L2 Regularization')\n",
    "    axes[2,0].set_ylabel('Mean Validation Accuracy')\n",
    "    axes[2,0].set_title('Regularization Impact')\n",
    "    axes[2,0].grid(True)\n",
    "    \n",
    "    # 8. Dense Units Impact\n",
    "    dense_groups = results_df.groupby('dense_units')['best_val_accuracy'].mean()\n",
    "    axes[2,1].bar(range(len(dense_groups)), dense_groups.values)\n",
    "    axes[2,1].set_xticks(range(len(dense_groups)))\n",
    "    axes[2,1].set_xticklabels(dense_groups.index)\n",
    "    axes[2,1].set_xlabel('Dense Layer Units')\n",
    "    axes[2,1].set_ylabel('Mean Validation Accuracy')\n",
    "    axes[2,1].set_title('Dense Units Impact')\n",
    "    axes[2,1].grid(True)\n",
    "    \n",
    "    # 9. Accuracy vs AUC\n",
    "    axes[2,2].scatter(results_df['best_val_accuracy'], results_df['best_val_auc'], alpha=0.7)\n",
    "    axes[2,2].set_xlabel('Best Validation Accuracy')\n",
    "    axes[2,2].set_ylabel('Best Validation AUC')\n",
    "    axes[2,2].set_title('Accuracy vs AUC')\n",
    "    axes[2,2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_config, results_df_sorted\n",
    "\n",
    "def save_results_to_file(filename='hyperparameter_results_with_betas.json'):\n",
    "    \"\"\"Save tuning results to file including model filenames\"\"\"\n",
    "    if tuning_results:\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(tuning_results, f, indent=2)\n",
    "        print(f\"Enhanced results (with beta parameters and model filenames) saved to {filename}\")\n",
    "    else:\n",
    "        print(\"No results to save!\")\n",
    "\n",
    "def load_best_models(top_n=3):\n",
    "    \"\"\"Load the top N performing models for ensemble or comparison\"\"\"\n",
    "    if not tuning_results:\n",
    "        print(\"No tuning results available!\")\n",
    "        return []\n",
    "    \n",
    "    # Get successful results sorted by accuracy\n",
    "    successful_results = [r for r in tuning_results if 'error' not in r and 'model_filename' in r]\n",
    "    successful_results.sort(key=lambda x: x['best_val_accuracy'], reverse=True)\n",
    "    \n",
    "    loaded_models = []\n",
    "    print(f\"\\nLoading top {top_n} models...\")\n",
    "    \n",
    "    for i, result in enumerate(successful_results[:top_n]):\n",
    "        try:\n",
    "            model = load_model(result['model_filename'])\n",
    "            loaded_models.append({\n",
    "                'model': model,\n",
    "                'trial': result['trial'],\n",
    "                'accuracy': result['best_val_accuracy'],\n",
    "                'auc': result['best_val_auc'],\n",
    "                'params': result['params'],\n",
    "                'filename': result['model_filename']\n",
    "            })\n",
    "            print(f\"✓ Loaded Trial {result['trial']}: {os.path.basename(result['model_filename'])} (Acc: {result['best_val_accuracy']:.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to load Trial {result['trial']}: {str(e)}\")\n",
    "    \n",
    "    return loaded_models\n",
    "\n",
    "print(\"Enhanced analysis functions with model management ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ENHANCED hyperparameter search with 12 trials, 50 epochs each...\n",
      "Now testing Adam optimizer beta parameters (β₁ and β₂) for optimization improvements!\n",
      "This may take 45-90 minutes depending on your hardware.\n",
      "\n",
      "Beta Parameter Testing Strategy:\n",
      "• β₁ (momentum): Testing 0.8, 0.9, 0.95, 0.99\n",
      "• β₂ (variance): Testing 0.99, 0.999, 0.9999\n",
      "• Looking for optimal momentum vs stability balance\n",
      "\n",
      "Setting up data generators...\n",
      "\n",
      "======================================================================\n",
      "BETA PARAMETER IMPACT EXPLANATION:\n",
      "======================================================================\n",
      "Higher β₁ (0.95-0.99): More momentum, smoother convergence\n",
      "Lower β₁ (0.8-0.9): Less momentum, faster adaptation to changes\n",
      "Higher β₂ (0.999-0.9999): More stable, better for sparse gradients\n",
      "Lower β₂ (0.99-0.995): Faster adaptation, better for dense gradients\n",
      "======================================================================\n",
      "Models will be saved in: hyperparameter_models/\n",
      "Starting enhanced hyperparameter search with 12 combinations...\n",
      "Each trial will run for 50 epochs with early stopping.\n",
      "Each best model will be saved with descriptive filename for later analysis.\n",
      "\n",
      "======================================================================\n",
      "TRIAL 1/12\n",
      "Parameters: {'learning_rate': 0.0005, 'batch_size': 32, 'reg_strength': 0.0005, 'dropout_conv': 0.25, 'dropout_dense': 0.5, 'dense_units': 512, 'filters_multiplier': 1, 'beta_1': 0.9, 'beta_2': 0.999}\n",
      "Adam Optimizer: lr=0.0005, β₁=0.9, β₂=0.999\n",
      "======================================================================\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ENHANCED hyperparameter search with 12 trials, 50 epochs each...\n",
      "Now testing Adam optimizer beta parameters (β₁ and β₂) for optimization improvements!\n",
      "This may take 45-90 minutes depending on your hardware.\n",
      "\n",
      "Beta Parameter Testing Strategy:\n",
      "• β₁ (momentum): Testing 0.8, 0.9, 0.95, 0.99\n",
      "• β₂ (variance): Testing 0.99, 0.999, 0.9999\n",
      "• Looking for optimal momentum vs stability balance\n",
      "\n",
      "Setting up data generators...\n",
      "\n",
      "======================================================================\n",
      "BETA PARAMETER IMPACT EXPLANATION:\n",
      "======================================================================\n",
      "Higher β₁ (0.95-0.99): More momentum, smoother convergence\n",
      "Lower β₁ (0.8-0.9): Less momentum, faster adaptation to changes\n",
      "Higher β₂ (0.999-0.9999): More stable, better for sparse gradients\n",
      "Lower β₂ (0.99-0.995): Faster adaptation, better for dense gradients\n",
      "======================================================================\n",
      "Models will be saved in: hyperparameter_models/\n",
      "Starting enhanced hyperparameter search with 12 combinations...\n",
      "Each trial will run for 50 epochs with early stopping.\n",
      "Each best model will be saved with descriptive filename for later analysis.\n",
      "\n",
      "======================================================================\n",
      "TRIAL 1/12\n",
      "Parameters: {'learning_rate': 0.0005, 'batch_size': 32, 'reg_strength': 0.0005, 'dropout_conv': 0.25, 'dropout_dense': 0.5, 'dense_units': 512, 'filters_multiplier': 1, 'beta_1': 0.9, 'beta_2': 0.999}\n",
      "Adam Optimizer: lr=0.0005, β₁=0.9, β₂=0.999\n",
      "======================================================================\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linhnguyen/Library/Python/3.10/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ENHANCED hyperparameter search with 12 trials, 50 epochs each...\n",
      "Now testing Adam optimizer beta parameters (β₁ and β₂) for optimization improvements!\n",
      "This may take 45-90 minutes depending on your hardware.\n",
      "\n",
      "Beta Parameter Testing Strategy:\n",
      "• β₁ (momentum): Testing 0.8, 0.9, 0.95, 0.99\n",
      "• β₂ (variance): Testing 0.99, 0.999, 0.9999\n",
      "• Looking for optimal momentum vs stability balance\n",
      "\n",
      "Setting up data generators...\n",
      "\n",
      "======================================================================\n",
      "BETA PARAMETER IMPACT EXPLANATION:\n",
      "======================================================================\n",
      "Higher β₁ (0.95-0.99): More momentum, smoother convergence\n",
      "Lower β₁ (0.8-0.9): Less momentum, faster adaptation to changes\n",
      "Higher β₂ (0.999-0.9999): More stable, better for sparse gradients\n",
      "Lower β₂ (0.99-0.995): Faster adaptation, better for dense gradients\n",
      "======================================================================\n",
      "Models will be saved in: hyperparameter_models/\n",
      "Starting enhanced hyperparameter search with 12 combinations...\n",
      "Each trial will run for 50 epochs with early stopping.\n",
      "Each best model will be saved with descriptive filename for later analysis.\n",
      "\n",
      "======================================================================\n",
      "TRIAL 1/12\n",
      "Parameters: {'learning_rate': 0.0005, 'batch_size': 32, 'reg_strength': 0.0005, 'dropout_conv': 0.25, 'dropout_dense': 0.5, 'dense_units': 512, 'filters_multiplier': 1, 'beta_1': 0.9, 'beta_2': 0.999}\n",
      "Adam Optimizer: lr=0.0005, β₁=0.9, β₂=0.999\n",
      "======================================================================\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linhnguyen/Library/Python/3.10/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m 44/267\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:46\u001b[0m 2s/step - accuracy: 0.6439 - auc: 0.7090 - loss: 1.0253 - precision: 0.6480 - recall: 0.5659"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ENHANCED hyperparameter search with 12 trials, 50 epochs each...\n",
      "Now testing Adam optimizer beta parameters (β₁ and β₂) for optimization improvements!\n",
      "This may take 45-90 minutes depending on your hardware.\n",
      "\n",
      "Beta Parameter Testing Strategy:\n",
      "• β₁ (momentum): Testing 0.8, 0.9, 0.95, 0.99\n",
      "• β₂ (variance): Testing 0.99, 0.999, 0.9999\n",
      "• Looking for optimal momentum vs stability balance\n",
      "\n",
      "Setting up data generators...\n",
      "\n",
      "======================================================================\n",
      "BETA PARAMETER IMPACT EXPLANATION:\n",
      "======================================================================\n",
      "Higher β₁ (0.95-0.99): More momentum, smoother convergence\n",
      "Lower β₁ (0.8-0.9): Less momentum, faster adaptation to changes\n",
      "Higher β₂ (0.999-0.9999): More stable, better for sparse gradients\n",
      "Lower β₂ (0.99-0.995): Faster adaptation, better for dense gradients\n",
      "======================================================================\n",
      "Models will be saved in: hyperparameter_models/\n",
      "Starting enhanced hyperparameter search with 12 combinations...\n",
      "Each trial will run for 50 epochs with early stopping.\n",
      "Each best model will be saved with descriptive filename for later analysis.\n",
      "\n",
      "======================================================================\n",
      "TRIAL 1/12\n",
      "Parameters: {'learning_rate': 0.0005, 'batch_size': 32, 'reg_strength': 0.0005, 'dropout_conv': 0.25, 'dropout_dense': 0.5, 'dense_units': 512, 'filters_multiplier': 1, 'beta_1': 0.9, 'beta_2': 0.999}\n",
      "Adam Optimizer: lr=0.0005, β₁=0.9, β₂=0.999\n",
      "======================================================================\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 8544 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n",
      "Found 1068 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linhnguyen/Library/Python/3.10/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m 44/267\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:46\u001b[0m 2s/step - accuracy: 0.6439 - auc: 0.7090 - loss: 1.0253 - precision: 0.6480 - recall: 0.5659"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Run the enhanced search\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m search_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_hyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_TRIALS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS_PER_TRIAL\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 161\u001b[0m, in \u001b[0;36mrun_hyperparameter_search\u001b[0;34m(max_trials, epochs_per_trial)\u001b[0m\n\u001b[1;32m    152\u001b[0m validation_generator \u001b[38;5;241m=\u001b[39m validation_gen\u001b[38;5;241m.\u001b[39mflow_from_directory(\n\u001b[1;32m    153\u001b[0m     VALIDATION_DIR,\n\u001b[1;32m    154\u001b[0m     target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_per_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Get best results\u001b[39;00m\n\u001b[1;32m    170\u001b[0m best_val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[0;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    219\u001b[0m     ):\n\u001b[0;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Execute Enhanced Hyperparameter Search (with Adam Beta Parameters)\n",
    "# WARNING: This will take significant time depending on max_trials and epochs_per_trial\n",
    "\n",
    "# Configuration for the search\n",
    "MAX_TRIALS = 12  # Increased to test more beta combinations\n",
    "EPOCHS_PER_TRIAL = 50  # Slightly more epochs to see beta effects\n",
    "\n",
    "print(f\"Starting ENHANCED hyperparameter search with {MAX_TRIALS} trials, {EPOCHS_PER_TRIAL} epochs each...\")\n",
    "print(\"\\nBeta Parameter Testing Strategy:\")\n",
    "print(\"• β₁ (momentum): Testing 0.8, 0.9, 0.95, 0.99\")\n",
    "print(\"• β₂ (variance): Testing 0.99, 0.999, 0.9999\")\n",
    "print(\"• Looking for optimal momentum vs stability balance\")\n",
    "\n",
    "# Ensure we have the data generators\n",
    "if 'train_gen' not in locals():\n",
    "    print(\"\\nSetting up data generators...\")\n",
    "    # Data Augmentation for Better Generalization\n",
    "    train_gen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    validation_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BETA PARAMETER IMPACT EXPLANATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Higher β₁ (0.95-0.99): More momentum, smoother convergence\")\n",
    "print(\"Lower β₁ (0.8-0.9): Less momentum, faster adaptation to changes\")\n",
    "print(\"Higher β₂ (0.999-0.9999): More stable, better for sparse gradients\")  \n",
    "print(\"Lower β₂ (0.99-0.995): Faster adaptation, better for dense gradients\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run the enhanced search\n",
    "search_results = run_hyperparameter_search(max_trials=MAX_TRIALS, epochs_per_trial=EPOCHS_PER_TRIAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Results and Train Best Model (with Optimized Beta Parameters)\n",
    "print(\"Analyzing enhanced hyperparameter search results (including Adam beta parameters)...\")\n",
    "\n",
    "# Analyze the results\n",
    "best_config, all_results = analyze_tuning_results()\n",
    "\n",
    "# Save enhanced results for future reference\n",
    "save_results_to_file('hyperparameter_tuning_results_with_betas.json')\n",
    "\n",
    "if best_config is not None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING FINAL MODEL WITH OPTIMIZED ADAM BETA PARAMETERS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get best parameters including betas\n",
    "    best_params = best_config['params']\n",
    "    print(f\"Using optimized parameters: {best_params}\")\n",
    "    print(f\"\\nOptimized Adam Configuration:\")\n",
    "    print(f\"  Learning Rate: {best_params['learning_rate']}\")\n",
    "    print(f\"  Beta_1 (momentum): {best_params['beta_1']}\")\n",
    "    print(f\"  Beta_2 (variance): {best_params['beta_2']}\")\n",
    "    \n",
    "    # Create the best model\n",
    "    best_model = create_tuned_model(\n",
    "        reg_strength=best_params['reg_strength'],\n",
    "        dropout_conv=best_params['dropout_conv'],\n",
    "        dropout_dense=best_params['dropout_dense'],\n",
    "        dense_units=best_params['dense_units'],\n",
    "        filters_multiplier=best_params['filters_multiplier']\n",
    "    )\n",
    "    \n",
    "    # Setup callbacks for final training (longer patience)\n",
    "    reduce_lr_final = ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stop_final = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    checkpoint_final = ModelCheckpoint(\n",
    "        'best_tuned_model_with_betas.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    callbacks_final = [reduce_lr_final, early_stop_final, checkpoint_final]\n",
    "    \n",
    "    # Compile with optimized Adam parameters (including betas!)\n",
    "    best_model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=best_params['learning_rate'],\n",
    "            beta_1=best_params['beta_1'],  # Optimized momentum\n",
    "            beta_2=best_params['beta_2']   # Optimized variance\n",
    "        ),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall', 'auc']\n",
    "    )\n",
    "    \n",
    "    # Create data generators with best batch size\n",
    "    final_train_generator = train_gen.flow_from_directory(\n",
    "        TRAINING_DIR,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=best_params['batch_size'],\n",
    "        class_mode='binary',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    final_validation_generator = validation_gen.flow_from_directory(\n",
    "        VALIDATION_DIR,\n",
    "        target_size=(128, 128),\n",
    "        batch_size=best_params['batch_size'],\n",
    "        class_mode='binary',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nStarting final training with optimized Adam beta parameters...\")\n",
    "    print(f\"Using β₁={best_params['beta_1']} (momentum) and β₂={best_params['beta_2']} (variance)\")\n",
    "    \n",
    "    # Train the final model with more epochs\n",
    "    final_history = best_model.fit(\n",
    "        final_train_generator,\n",
    "        epochs=50,  # Full training\n",
    "        validation_data=final_validation_generator,\n",
    "        callbacks=callbacks_final,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENHANCED HYPERPARAMETER TUNING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Best model saved as 'best_tuned_model_with_betas.h5'\")\n",
    "    print(f\"Final validation accuracy: {max(final_history.history['val_accuracy']):.4f}\")\n",
    "    print(f\"Final validation AUC: {max(final_history.history['val_auc']):.4f}\")\n",
    "    print(f\"\\nOptimal Adam Configuration Found:\")\n",
    "    print(f\"• Learning Rate: {best_params['learning_rate']}\")\n",
    "    print(f\"• Beta_1 (momentum): {best_params['beta_1']}\")\n",
    "    print(f\"• Beta_2 (variance): {best_params['beta_2']}\")\n",
    "    \n",
    "    # Compare with default Adam parameters\n",
    "    print(f\"\\nComparison with Default Adam (lr=0.001, β₁=0.9, β₂=0.999):\")\n",
    "    if best_params['beta_1'] > 0.9:\n",
    "        print(f\"✓ Higher momentum (β₁={best_params['beta_1']}) → Smoother convergence\")\n",
    "    elif best_params['beta_1'] < 0.9:\n",
    "        print(f\"✓ Lower momentum (β₁={best_params['beta_1']}) → Faster adaptation\")\n",
    "    \n",
    "    if best_params['beta_2'] > 0.999:\n",
    "        print(f\"✓ Higher variance stability (β₂={best_params['beta_2']}) → Better for sparse gradients\")\n",
    "    elif best_params['beta_2'] < 0.999:\n",
    "        print(f\"✓ Lower variance adaptation (β₂={best_params['beta_2']}) → Better for dense gradients\")\n",
    "        \n",
    "else:\n",
    "    print(\"Could not find best configuration. Check the search results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Ensemble and Comparison Functions\n",
    "def create_ensemble_predictions(models_info, test_generator, ensemble_method='average'):\n",
    "    \"\"\"\n",
    "    Create ensemble predictions from multiple trained models\n",
    "    \n",
    "    Args:\n",
    "        models_info: List of model info dictionaries from load_best_models()\n",
    "        test_generator: Test data generator\n",
    "        ensemble_method: 'average', 'weighted', or 'voting'\n",
    "    \"\"\"\n",
    "    if not models_info:\n",
    "        print(\"No models provided for ensemble!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nCreating ensemble predictions using {len(models_info)} models...\")\n",
    "    print(f\"Ensemble method: {ensemble_method}\")\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    all_predictions = []\n",
    "    model_weights = []\n",
    "    \n",
    "    test_generator.reset()\n",
    "    \n",
    "    for i, model_info in enumerate(models_info):\n",
    "        print(f\"Getting predictions from Trial {model_info['trial']} (Acc: {model_info['accuracy']:.4f})...\")\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model_info['model'].predict(test_generator, verbose=0)\n",
    "        all_predictions.append(predictions)\n",
    "        \n",
    "        # Weight by validation accuracy for weighted ensemble\n",
    "        model_weights.append(model_info['accuracy'])\n",
    "        \n",
    "        test_generator.reset()  # Reset for next model\n",
    "    \n",
    "    # Create ensemble predictions\n",
    "    all_predictions = np.array(all_predictions)  # Shape: (n_models, n_samples, 1)\n",
    "    \n",
    "    if ensemble_method == 'average':\n",
    "        ensemble_pred = np.mean(all_predictions, axis=0)\n",
    "    elif ensemble_method == 'weighted':\n",
    "        weights = np.array(model_weights) / sum(model_weights)\n",
    "        ensemble_pred = np.average(all_predictions, axis=0, weights=weights)\n",
    "    elif ensemble_method == 'voting':\n",
    "        # Convert to binary predictions and take majority vote\n",
    "        binary_preds = (all_predictions > 0.5).astype(int)\n",
    "        ensemble_pred = (np.mean(binary_preds, axis=0) > 0.5).astype(float)\n",
    "    else:\n",
    "        raise ValueError(\"ensemble_method must be 'average', 'weighted', or 'voting'\")\n",
    "    \n",
    "    print(f\"Ensemble predictions created using {ensemble_method} method.\")\n",
    "    return ensemble_pred\n",
    "\n",
    "def compare_individual_vs_ensemble(models_info, test_generator):\n",
    "    \"\"\"Compare individual model performance vs ensemble\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INDIVIDUAL MODELS vs ENSEMBLE COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Get true labels\n",
    "    test_generator.reset()\n",
    "    y_true = []\n",
    "    for i in range(len(test_generator)):\n",
    "        batch_x, batch_y = next(test_generator)\n",
    "        y_true.extend(batch_y)\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # Individual model results\n",
    "    print(\"\\nINDIVIDUAL MODEL PERFORMANCE:\")\n",
    "    individual_accuracies = []\n",
    "    for model_info in models_info:\n",
    "        test_generator.reset()\n",
    "        predictions = model_info['model'].predict(test_generator, verbose=0)\n",
    "        binary_preds = (predictions > 0.5).astype(int).flatten()\n",
    "        \n",
    "        accuracy = np.mean(binary_preds == y_true)\n",
    "        individual_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Trial {model_info['trial']}: {accuracy:.4f} (Val: {model_info['accuracy']:.4f})\")\n",
    "    \n",
    "    # Ensemble results\n",
    "    print(f\"\\nENSEMBLE PERFORMANCE:\")\n",
    "    ensemble_methods = ['average', 'weighted', 'voting']\n",
    "    \n",
    "    ensemble_results = {}\n",
    "    for method in ensemble_methods:\n",
    "        ensemble_pred = create_ensemble_predictions(models_info, test_generator, method)\n",
    "        binary_ensemble = (ensemble_pred > 0.5).astype(int).flatten()\n",
    "        ensemble_accuracy = np.mean(binary_ensemble == y_true)\n",
    "        ensemble_results[method] = ensemble_accuracy\n",
    "        \n",
    "        print(f\"{method.capitalize()} Ensemble: {ensemble_accuracy:.4f}\")\n",
    "    \n",
    "    # Summary\n",
    "    best_individual = max(individual_accuracies)\n",
    "    best_ensemble = max(ensemble_results.values())\n",
    "    best_method = max(ensemble_results.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*50)\n",
    "    print(f\"Best Individual Model: {best_individual:.4f}\")\n",
    "    print(f\"Best Ensemble ({best_method[0]}): {best_method[1]:.4f}\")\n",
    "    \n",
    "    if best_ensemble > best_individual:\n",
    "        improvement = best_ensemble - best_individual\n",
    "        print(f\"✓ Ensemble improves by {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "    else:\n",
    "        print(\"✗ Ensemble does not improve over best individual model\")\n",
    "    \n",
    "    return ensemble_results\n",
    "\n",
    "def save_model_comparison_report():\n",
    "    \"\"\"Save a comprehensive comparison report of all models\"\"\"\n",
    "    if not tuning_results:\n",
    "        print(\"No results to save!\")\n",
    "        return\n",
    "    \n",
    "    successful_results = [r for r in tuning_results if 'error' not in r]\n",
    "    \n",
    "    # Create detailed report\n",
    "    report = {\n",
    "        'experiment_info': {\n",
    "            'total_trials': len(tuning_results),\n",
    "            'successful_trials': len(successful_results),\n",
    "            'failed_trials': len(tuning_results) - len(successful_results),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        },\n",
    "        'hyperparameter_space': HYPERPARAMETER_SPACE,\n",
    "        'results': successful_results,\n",
    "        'best_models': sorted(successful_results, key=lambda x: x['best_val_accuracy'], reverse=True)[:5]\n",
    "    }\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open('model_comparison_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    # Create markdown report\n",
    "    with open('model_comparison_report.md', 'w') as f:\n",
    "        f.write(\"# Hyperparameter Tuning Results Report\\n\\n\")\n",
    "        f.write(f\"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Experiment Overview\\n\")\n",
    "        f.write(f\"- Total Trials: {len(tuning_results)}\\n\")\n",
    "        f.write(f\"- Successful Trials: {len(successful_results)}\\n\")\n",
    "        f.write(f\"- Failed Trials: {len(tuning_results) - len(successful_results)}\\n\\n\")\n",
    "        \n",
    "        f.write(\"## Top 5 Performing Models\\n\")\n",
    "        for i, result in enumerate(report['best_models'][:5]):\n",
    "            f.write(f\"\\n### Rank {i+1}: Trial {result['trial']}\\n\")\n",
    "            f.write(f\"- **Validation Accuracy**: {result['best_val_accuracy']:.4f}\\n\")\n",
    "            f.write(f\"- **Validation AUC**: {result['best_val_auc']:.4f}\\n\")\n",
    "            f.write(f\"- **Model File**: `{os.path.basename(result['model_filename'])}`\\n\")\n",
    "            f.write(f\"- **Parameters**:\\n\")\n",
    "            for param, value in result['params'].items():\n",
    "                f.write(f\"  - {param}: {value}\\n\")\n",
    "        \n",
    "        f.write(f\"\\n## All Model Files\\n\")\n",
    "        for result in successful_results:\n",
    "            filename = os.path.basename(result['model_filename']) if result['model_filename'] else 'N/A'\n",
    "            f.write(f\"- Trial {result['trial']}: `{filename}` (Acc: {result['best_val_accuracy']:.4f})\\n\")\n",
    "    \n",
    "    print(\"Comprehensive model comparison report saved:\")\n",
    "    print(\"- model_comparison_report.json (detailed data)\")\n",
    "    print(\"- model_comparison_report.md (readable report)\")\n",
    "\n",
    "print(\"Model ensemble and comparison functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyZlq570nuMf"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Plot AUC\n",
    "    ax3.plot(history.history['auc'], label='Training AUC')\n",
    "    ax3.plot(history.history['val_auc'], label='Validation AUC')\n",
    "    ax3.set_title('Model AUC')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('AUC')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # Plot Precision and Recall\n",
    "    ax4.plot(history.history['precision'], label='Training Precision')\n",
    "    ax4.plot(history.history['val_precision'], label='Validation Precision')\n",
    "    ax4.plot(history.history['recall'], label='Training Recall')\n",
    "    ax4.plot(history.history['val_recall'], label='Validation Recall')\n",
    "    ax4.set_title('Precision and Recall')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Score')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print final metrics\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    final_train_auc = history.history['auc'][-1]\n",
    "    final_val_auc = history.history['val_auc'][-1]\n",
    "    \n",
    "    print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n",
    "    print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "    print(f\"Final Training AUC: {final_train_auc:.4f}\")\n",
    "    print(f\"Final Validation AUC: {final_val_auc:.4f}\")\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b93058f"
   },
   "outputs": [],
   "source": [
    "\n",
    "best_model = load_model('best_cats_dogs_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model analysis functions ready!\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Model Analysis\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_all_saved_models():\n",
    "    \"\"\"Comprehensively analyze all saved models on test data\"\"\"\n",
    "    \n",
    "    # Get all model files\n",
    "    models_dir = \"/Users/linhnguyen/Documents/fall2025/csce482/project/models\"\n",
    "    model_files = [f for f in os.listdir(models_dir) if f.endswith('.h5')]\n",
    "    model_files.sort()  # Sort for consistent ordering\n",
    "    \n",
    "    print(f\"Found {len(model_files)} model files to analyze:\")\n",
    "    for i, filename in enumerate(model_files, 1):\n",
    "        print(f\"  {i}. {filename}\")\n",
    "    \n",
    "    print(f\"\\nAnalyzing models on {test_test_generator.samples} test samples...\")\n",
    "    \n",
    "    # Store results for all models\n",
    "    all_results = []\n",
    "    \n",
    "    # Get true labels once\n",
    "    test_test_generator.reset()\n",
    "    y_true = []\n",
    "    for i in range(len(test_test_generator)):\n",
    "        batch_x, batch_y = next(test_test_generator)\n",
    "        y_true.extend(batch_y)\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    print(f\"\\nTest set distribution:\")\n",
    "    print(f\"  Cats (class 0): {np.sum(y_true == 0)} samples\")\n",
    "    print(f\"  Non-cats (class 1): {np.sum(y_true == 1)} samples\")\n",
    "    \n",
    "    # Analyze each model\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"INDIVIDUAL MODEL ANALYSIS\")\n",
    "    print('='*100)\n",
    "    \n",
    "    for i, filename in enumerate(model_files, 1):\n",
    "        try:\n",
    "            print(f\"\\n[{i}/{len(model_files)}] Analyzing: {filename}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Load model\n",
    "            model_path = os.path.join(models_dir, filename)\n",
    "            model = load_model(model_path)\n",
    "            \n",
    "            # Get predictions\n",
    "            test_test_generator.reset()\n",
    "            predictions = model.predict(test_test_generator, verbose=0)\n",
    "            pred_probs = predictions.flatten()\n",
    "            pred_binary = (pred_probs > 0.5).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = np.mean(pred_binary == y_true)\n",
    "            auc_score = roc_auc_score(y_true, pred_probs)\n",
    "            \n",
    "            # Classification report\n",
    "            class_report = classification_report(y_true, pred_binary, \n",
    "                                               target_names=['Cats', 'Non-cats'], \n",
    "                                               output_dict=True)\n",
    "            \n",
    "            # Confusion matrix\n",
    "            cm = confusion_matrix(y_true, pred_binary)\n",
    "            \n",
    "            # Extract model info from filename\n",
    "            model_info = parse_model_filename(filename)\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'filename': filename,\n",
    "                'model_info': model_info,\n",
    "                'accuracy': accuracy,\n",
    "                'auc': auc_score,\n",
    "                'predictions': pred_probs,\n",
    "                'binary_predictions': pred_binary,\n",
    "                'classification_report': class_report,\n",
    "                'confusion_matrix': cm\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            \n",
    "            # Print key metrics\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"AUC: {auc_score:.4f}\")\n",
    "            print(f\"Precision (Cats): {class_report['Cats']['precision']:.4f}\")\n",
    "            print(f\"Recall (Cats): {class_report['Cats']['recall']:.4f}\")\n",
    "            print(f\"Precision (Non-cats): {class_report['Non-cats']['precision']:.4f}\")\n",
    "            print(f\"Recall (Non-cats): {class_report['Non-cats']['recall']:.4f}\")\n",
    "            \n",
    "            if model_info:\n",
    "                print(f\"\\nModel Parameters:\")\n",
    "                print(f\"  Learning Rate: {model_info.get('lr', 'N/A')}\")\n",
    "                print(f\"  Beta1: {model_info.get('b1', 'N/A')}\")\n",
    "                print(f\"  Beta2: {model_info.get('b2', 'N/A')}\")\n",
    "                print(f\"  Batch Size: {model_info.get('bs', 'N/A')}\")\n",
    "                print(f\"  Regularization: {model_info.get('reg', 'N/A')}\")\n",
    "                print(f\"  Dense Units: {model_info.get('du', 'N/A')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error analyzing {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort results by accuracy\n",
    "    all_results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"PERFORMANCE RANKING\")\n",
    "    print('='*100)\n",
    "    \n",
    "    for i, result in enumerate(all_results, 1):\n",
    "        print(f\"{i:2d}. {result['filename'][:60]:<60} | Acc: {result['accuracy']:.4f} | AUC: {result['auc']:.4f}\")\n",
    "    \n",
    "    return all_results, y_true\n",
    "\n",
    "def parse_model_filename(filename):\n",
    "    \"\"\"Extract parameters from model filename\"\"\"\n",
    "    try:\n",
    "        if filename.startswith('trial_'):\n",
    "            # Parse trial files: trial_01_lr0.0005_b1-0.9_b2-0.999_bs32_reg0.0005_du512.h5\n",
    "            parts = filename.replace('.h5', '').split('_')\n",
    "            info = {}\n",
    "            for part in parts:\n",
    "                if part.startswith('lr'):\n",
    "                    info['lr'] = float(part[2:])\n",
    "                elif part.startswith('b1-'):\n",
    "                    info['b1'] = float(part[3:])\n",
    "                elif part.startswith('b2-'):\n",
    "                    info['b2'] = float(part[3:])\n",
    "                elif part.startswith('bs'):\n",
    "                    info['bs'] = int(part[2:])\n",
    "                elif part.startswith('reg'):\n",
    "                    info['reg'] = float(part[3:])\n",
    "                elif part.startswith('du'):\n",
    "                    info['du'] = int(part[2:])\n",
    "            return info\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "print(\"Model analysis functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 RUNNING SIMPLIFIED MODEL ANALYSIS\n",
      "================================================================================\n",
      "Found 13 model files to analyze:\n",
      "  1. best_tuned_model_with_betas.h5\n",
      "  2. trial_01_lr0.0005_b1-0.9_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "  3. trial_02_lr0.001_b1-0.95_b2-0.999_bs32_reg0.0001_du512.h5\n",
      "  4. trial_03_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "  5. trial_04_lr0.001_b1-0.8_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "  6. trial_05_lr0.0005_b1-0.9_b2-0.99_bs32_reg0.0005_du512.h5\n",
      "  7. trial_06_lr0.0001_b1-0.9_b2-0.9999_bs32_reg0.001_du512.h5\n",
      "  8. trial_07_lr0.0005_b1-0.95_b2-0.9999_bs64_reg0.0005_du1024.h5\n",
      "  9. trial_08_lr0.001_b1-0.8_b2-0.99_bs16_reg0.0001_du512.h5\n",
      "  10. trial_09_lr0.0001_b1-0.95_b2-0.9999_bs64_reg0.001_du256.h5\n",
      "  11. trial_10_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du1024.h5\n",
      "  12. trial_11_lr0.001_b1-0.8_b2-0.99_bs64_reg0.0001_du512.h5\n",
      "  13. trial_12_lr0.0001_b1-0.99_b2-0.9999_bs32_reg0.0005_du512.h5\n",
      "\n",
      "Analyzing models on 1068 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set distribution:\n",
      "  Cats (class 0): 534 samples\n",
      "  Non-cats (class 1): 534 samples\n",
      "\n",
      "====================================================================================================\n",
      "INDIVIDUAL MODEL ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "[1/13] Analyzing: best_tuned_model_with_betas.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9625\n",
      "AUC: 0.9930\n",
      "Precision (Cats): 0.9458\n",
      "Recall (Cats): 0.9813\n",
      "Precision (Non-cats): 0.9805\n",
      "Recall (Non-cats): 0.9438\n",
      "\n",
      "[2/13] Analyzing: trial_01_lr0.0005_b1-0.9_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9213\n",
      "AUC: 0.9764\n",
      "Precision (Cats): 0.9518\n",
      "Recall (Cats): 0.8876\n",
      "Precision (Non-cats): 0.8947\n",
      "Recall (Non-cats): 0.9551\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0005\n",
      "  Beta1: 0.9\n",
      "  Beta2: 0.999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 512\n",
      "\n",
      "[3/13] Analyzing: trial_02_lr0.001_b1-0.95_b2-0.999_bs32_reg0.0001_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8886\n",
      "AUC: 0.9744\n",
      "Precision (Cats): 0.9621\n",
      "Recall (Cats): 0.8090\n",
      "Precision (Non-cats): 0.8352\n",
      "Recall (Non-cats): 0.9682\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.001\n",
      "  Beta1: 0.95\n",
      "  Beta2: 0.999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0001\n",
      "  Dense Units: 512\n",
      "\n",
      "[4/13] Analyzing: trial_03_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9073\n",
      "AUC: 0.9773\n",
      "Precision (Cats): 0.9718\n",
      "Recall (Cats): 0.8390\n",
      "Precision (Non-cats): 0.8583\n",
      "Recall (Non-cats): 0.9757\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0005\n",
      "  Beta1: 0.99\n",
      "  Beta2: 0.999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 512\n",
      "\n",
      "[5/13] Analyzing: trial_04_lr0.001_b1-0.8_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8895\n",
      "AUC: 0.9811\n",
      "Precision (Cats): 0.9860\n",
      "Recall (Cats): 0.7903\n",
      "Precision (Non-cats): 0.8250\n",
      "Recall (Non-cats): 0.9888\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.001\n",
      "  Beta1: 0.8\n",
      "  Beta2: 0.999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 512\n",
      "\n",
      "[6/13] Analyzing: trial_05_lr0.0005_b1-0.9_b2-0.99_bs32_reg0.0005_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9232\n",
      "AUC: 0.9825\n",
      "Precision (Cats): 0.9728\n",
      "Recall (Cats): 0.8708\n",
      "Precision (Non-cats): 0.8831\n",
      "Recall (Non-cats): 0.9757\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0005\n",
      "  Beta1: 0.9\n",
      "  Beta2: 0.99\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 512\n",
      "\n",
      "[7/13] Analyzing: trial_06_lr0.0001_b1-0.9_b2-0.9999_bs32_reg0.001_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7116\n",
      "AUC: 0.8789\n",
      "Precision (Cats): 0.9007\n",
      "Recall (Cats): 0.4757\n",
      "Precision (Non-cats): 0.6438\n",
      "Recall (Non-cats): 0.9476\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0001\n",
      "  Beta1: 0.9\n",
      "  Beta2: 0.9999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.001\n",
      "  Dense Units: 512\n",
      "\n",
      "[8/13] Analyzing: trial_07_lr0.0005_b1-0.95_b2-0.9999_bs64_reg0.0005_du1024.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9204\n",
      "AUC: 0.9778\n",
      "Precision (Cats): 0.9535\n",
      "Recall (Cats): 0.8839\n",
      "Precision (Non-cats): 0.8918\n",
      "Recall (Non-cats): 0.9569\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0005\n",
      "  Beta1: 0.95\n",
      "  Beta2: 0.9999\n",
      "  Batch Size: 64\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 1024\n",
      "\n",
      "[9/13] Analyzing: trial_08_lr0.001_b1-0.8_b2-0.99_bs16_reg0.0001_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9176\n",
      "AUC: 0.9837\n",
      "Precision (Cats): 0.9785\n",
      "Recall (Cats): 0.8539\n",
      "Precision (Non-cats): 0.8704\n",
      "Recall (Non-cats): 0.9813\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.001\n",
      "  Beta1: 0.8\n",
      "  Beta2: 0.99\n",
      "  Batch Size: 16\n",
      "  Regularization: 0.0001\n",
      "  Dense Units: 512\n",
      "\n",
      "[10/13] Analyzing: trial_09_lr0.0001_b1-0.95_b2-0.9999_bs64_reg0.001_du256.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7303\n",
      "AUC: 0.8076\n",
      "Precision (Cats): 0.7236\n",
      "Recall (Cats): 0.7453\n",
      "Precision (Non-cats): 0.7375\n",
      "Recall (Non-cats): 0.7154\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0001\n",
      "  Beta1: 0.95\n",
      "  Beta2: 0.9999\n",
      "  Batch Size: 64\n",
      "  Regularization: 0.001\n",
      "  Dense Units: 256\n",
      "\n",
      "[11/13] Analyzing: trial_10_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du1024.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9232\n",
      "AUC: 0.9809\n",
      "Precision (Cats): 0.9538\n",
      "Recall (Cats): 0.8895\n",
      "Precision (Non-cats): 0.8965\n",
      "Recall (Non-cats): 0.9569\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0005\n",
      "  Beta1: 0.99\n",
      "  Beta2: 0.999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 1024\n",
      "\n",
      "[12/13] Analyzing: trial_11_lr0.001_b1-0.8_b2-0.99_bs64_reg0.0001_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9335\n",
      "AUC: 0.9872\n",
      "Precision (Cats): 0.9443\n",
      "Recall (Cats): 0.9213\n",
      "Precision (Non-cats): 0.9232\n",
      "Recall (Non-cats): 0.9457\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.001\n",
      "  Beta1: 0.8\n",
      "  Beta2: 0.99\n",
      "  Batch Size: 64\n",
      "  Regularization: 0.0001\n",
      "  Dense Units: 512\n",
      "\n",
      "[13/13] Analyzing: trial_12_lr0.0001_b1-0.99_b2-0.9999_bs32_reg0.0005_du512.h5\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 0.8146\n",
      "AUC: 0.8854\n",
      "Precision (Cats): 0.8032\n",
      "Recall (Cats): 0.8333\n",
      "Precision (Non-cats): 0.8268\n",
      "Recall (Non-cats): 0.7959\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0001\n",
      "  Beta1: 0.99\n",
      "  Beta2: 0.9999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 512\n",
      "\n",
      "====================================================================================================\n",
      "PERFORMANCE RANKING\n",
      "====================================================================================================\n",
      " 1. best_tuned_model_with_betas.h5                               | Acc: 0.9625 | AUC: 0.9930\n",
      " 2. trial_11_lr0.001_b1-0.8_b2-0.99_bs64_reg0.0001_du512.h5      | Acc: 0.9335 | AUC: 0.9872\n",
      " 3. trial_05_lr0.0005_b1-0.9_b2-0.99_bs32_reg0.0005_du512.h5     | Acc: 0.9232 | AUC: 0.9825\n",
      " 4. trial_10_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du1024.h5  | Acc: 0.9232 | AUC: 0.9809\n",
      " 5. trial_01_lr0.0005_b1-0.9_b2-0.999_bs32_reg0.0005_du512.h5    | Acc: 0.9213 | AUC: 0.9764\n",
      " 6. trial_07_lr0.0005_b1-0.95_b2-0.9999_bs64_reg0.0005_du1024.h5 | Acc: 0.9204 | AUC: 0.9778\n",
      " 7. trial_08_lr0.001_b1-0.8_b2-0.99_bs16_reg0.0001_du512.h5      | Acc: 0.9176 | AUC: 0.9837\n",
      " 8. trial_03_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du512.h5   | Acc: 0.9073 | AUC: 0.9773\n",
      " 9. trial_04_lr0.001_b1-0.8_b2-0.999_bs32_reg0.0005_du512.h5     | Acc: 0.8895 | AUC: 0.9811\n",
      "10. trial_02_lr0.001_b1-0.95_b2-0.999_bs32_reg0.0001_du512.h5    | Acc: 0.8886 | AUC: 0.9744\n",
      "11. trial_12_lr0.0001_b1-0.99_b2-0.9999_bs32_reg0.0005_du512.h5  | Acc: 0.8146 | AUC: 0.8854\n",
      "12. trial_09_lr0.0001_b1-0.95_b2-0.9999_bs64_reg0.001_du256.h5   | Acc: 0.7303 | AUC: 0.8076\n",
      "13. trial_06_lr0.0001_b1-0.9_b2-0.9999_bs32_reg0.001_du512.h5    | Acc: 0.7116 | AUC: 0.8789\n",
      "\n",
      "📊 SUMMARY STATISTICS:\n",
      "--------------------------------------------------\n",
      "Number of models: 13\n",
      "Test samples: 1068\n",
      "Accuracy - Mean: 0.8803 ± 0.0755\n",
      "Accuracy - Range: 0.7116 to 0.9625\n",
      "AUC - Mean: 0.9528 ± 0.0552\n",
      "AUC - Range: 0.8076 to 0.9930\n",
      "\n",
      "🏆 TOP 5 MODELS:\n",
      "--------------------------------------------------\n",
      "1. best_tuned_model_with_betas.h5\n",
      "   Accuracy: 0.9625 | AUC: 0.9930\n",
      "2. trial_11_lr0.001_b1-0.8_b2-0.99_bs64_reg0.0001_du512.h5\n",
      "   Accuracy: 0.9335 | AUC: 0.9872\n",
      "   LR: 0.001 | β₁: 0.8 | β₂: 0.99\n",
      "3. trial_05_lr0.0005_b1-0.9_b2-0.99_bs32_reg0.0005_du512.h5\n",
      "   Accuracy: 0.9232 | AUC: 0.9825\n",
      "   LR: 0.0005 | β₁: 0.9 | β₂: 0.99\n",
      "4. trial_10_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du1024.h5\n",
      "   Accuracy: 0.9232 | AUC: 0.9809\n",
      "   LR: 0.0005 | β₁: 0.99 | β₂: 0.999\n",
      "5. trial_01_lr0.0005_b1-0.9_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "   Accuracy: 0.9213 | AUC: 0.9764\n",
      "   LR: 0.0005 | β₁: 0.9 | β₂: 0.999\n",
      "\n",
      "✅ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Simple Model Analysis (without complex visualizations)\n",
    "print(\"🔍 RUNNING SIMPLIFIED MODEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run just the analysis part\n",
    "all_results, y_true = analyze_all_saved_models()\n",
    "\n",
    "print(f\"\\n📊 SUMMARY STATISTICS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "accuracies = [r['accuracy'] for r in all_results]\n",
    "aucs = [r['auc'] for r in all_results]\n",
    "\n",
    "print(f\"Number of models: {len(all_results)}\")\n",
    "print(f\"Test samples: {len(y_true)}\")\n",
    "print(f\"Accuracy - Mean: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "print(f\"Accuracy - Range: {np.min(accuracies):.4f} to {np.max(accuracies):.4f}\")\n",
    "print(f\"AUC - Mean: {np.mean(aucs):.4f} ± {np.std(aucs):.4f}\")\n",
    "print(f\"AUC - Range: {np.min(aucs):.4f} to {np.max(aucs):.4f}\")\n",
    "\n",
    "print(f\"\\n🏆 TOP 5 MODELS:\")\n",
    "print(\"-\" * 50)\n",
    "for i, result in enumerate(all_results[:5], 1):\n",
    "    print(f\"{i}. {result['filename']}\")\n",
    "    print(f\"   Accuracy: {result['accuracy']:.4f} | AUC: {result['auc']:.4f}\")\n",
    "    if result['model_info']:\n",
    "        info = result['model_info']\n",
    "        print(f\"   LR: {info.get('lr', 'N/A')} | β₁: {info.get('b1', 'N/A')} | β₂: {info.get('b2', 'N/A')}\")\n",
    "\n",
    "print(f\"\\n✅ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization and reporting functions ready!\n"
     ]
    }
   ],
   "source": [
    "def create_model_comparison_visualizations(all_results, y_true):\n",
    "    \"\"\"Create comprehensive visualizations comparing all models\"\"\"\n",
    "    \n",
    "    n_models = len(all_results)\n",
    "    \n",
    "    # Create a large figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Accuracy comparison bar plot\n",
    "    plt.subplot(3, 3, 1)\n",
    "    accuracies = [r['accuracy'] for r in all_results]\n",
    "    model_names = [r['filename'][:20] + '...' if len(r['filename']) > 20 else r['filename'] for r in all_results]\n",
    "    \n",
    "    bars = plt.bar(range(len(accuracies)), accuracies)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title('Test Accuracy Comparison')\n",
    "    plt.xticks(range(len(accuracies)), [f'M{i+1}' for i in range(len(accuracies))], rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Color bars by performance\n",
    "    max_acc = max(accuracies)\n",
    "    for i, bar in enumerate(bars):\n",
    "        if accuracies[i] == max_acc:\n",
    "            bar.set_color('gold')\n",
    "        elif accuracies[i] >= max_acc - 0.01:\n",
    "            bar.set_color('lightgreen')\n",
    "        else:\n",
    "            bar.set_color('lightblue')\n",
    "    \n",
    "    # 2. AUC comparison\n",
    "    plt.subplot(3, 3, 2)\n",
    "    aucs = [r['auc'] for r in all_results]\n",
    "    bars = plt.bar(range(len(aucs)), aucs)\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('AUC Comparison')\n",
    "    plt.xticks(range(len(aucs)), [f'M{i+1}' for i in range(len(aucs))], rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 3. ROC Curves for top 5 models\n",
    "    plt.subplot(3, 3, 3)\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, min(5, len(all_results))))\n",
    "    \n",
    "    for i, result in enumerate(all_results[:5]):\n",
    "        fpr, tpr, _ = roc_curve(y_true, result['predictions'])\n",
    "        plt.plot(fpr, tpr, color=colors[i], \n",
    "                label=f\"M{i+1} (AUC={result['auc']:.3f})\", linewidth=2)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves (Top 5 Models)')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 4. Accuracy vs AUC scatter\n",
    "    plt.subplot(3, 3, 4)\n",
    "    plt.scatter(accuracies, aucs, alpha=0.7, s=100)\n",
    "    for i, (acc, auc) in enumerate(zip(accuracies, aucs)):\n",
    "        plt.annotate(f'M{i+1}', (acc, auc), xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    plt.xlabel('Test Accuracy')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('Accuracy vs AUC')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 5. Parameter analysis (if available)\n",
    "    plt.subplot(3, 3, 5)\n",
    "    trial_results = [r for r in all_results if r['model_info'] is not None]\n",
    "    if trial_results:\n",
    "        learning_rates = [r['model_info']['lr'] for r in trial_results]\n",
    "        lr_accuracies = [r['accuracy'] for r in trial_results]\n",
    "        plt.scatter(learning_rates, lr_accuracies, alpha=0.7, s=100)\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.title('Learning Rate vs Accuracy')\n",
    "        plt.xscale('log')\n",
    "        plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 6. Beta parameter analysis\n",
    "    plt.subplot(3, 3, 6)\n",
    "    if trial_results:\n",
    "        beta1_values = [r['model_info']['b1'] for r in trial_results]\n",
    "        beta1_accuracies = [r['accuracy'] for r in trial_results]\n",
    "        plt.scatter(beta1_values, beta1_accuracies, alpha=0.7, s=100)\n",
    "        plt.xlabel('Beta1 (Momentum)')\n",
    "        plt.ylabel('Test Accuracy')\n",
    "        plt.title('Beta1 vs Accuracy')\n",
    "        plt.grid(alpha=0.3)\n",
    "    \n",
    "    # 7. Confusion matrix for best model\n",
    "    plt.subplot(3, 3, 7)\n",
    "    best_model = all_results[0]\n",
    "    cm = best_model['confusion_matrix']\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Cats', 'Non-cats'], \n",
    "                yticklabels=['Cats', 'Non-cats'])\n",
    "    plt.title(f'Best Model Confusion Matrix\\n({best_model[\"filename\"][:30]}...)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # 8. Performance distribution\n",
    "    plt.subplot(3, 3, 8)\n",
    "    plt.hist(accuracies, bins=10, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(np.mean(accuracies), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(accuracies):.3f}')\n",
    "    plt.axvline(np.median(accuracies), color='orange', linestyle='--', \n",
    "                label=f'Median: {np.median(accuracies):.3f}')\n",
    "    plt.xlabel('Test Accuracy')\n",
    "    plt.ylabel('Number of Models')\n",
    "    plt.title('Accuracy Distribution')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 9. Top vs Bottom models comparison\n",
    "    plt.subplot(3, 3, 9)\n",
    "    top_3_acc = np.mean([r['accuracy'] for r in all_results[:3]])\n",
    "    bottom_3_acc = np.mean([r['accuracy'] for r in all_results[-3:]])\n",
    "    top_3_auc = np.mean([r['auc'] for r in all_results[:3]])\n",
    "    bottom_3_auc = np.mean([r['auc'] for r in all_results[-3:]])\n",
    "    \n",
    "    categories = ['Top 3', 'Bottom 3']\n",
    "    acc_values = [top_3_acc, bottom_3_acc]\n",
    "    auc_values = [top_3_auc, bottom_3_auc]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, acc_values, width, label='Accuracy', alpha=0.8)\n",
    "    plt.bar(x + width/2, auc_values, width, label='AUC', alpha=0.8)\n",
    "    plt.xlabel('Model Groups')\n",
    "    plt.ylabel('Performance')\n",
    "    plt.title('Top vs Bottom Models')\n",
    "    plt.xticks(x, categories)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def generate_detailed_report(all_results, y_true):\n",
    "    \"\"\"Generate a detailed text report of all models\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*120}\")\n",
    "    print(\"DETAILED MODEL ANALYSIS REPORT\")\n",
    "    print('='*120)\n",
    "    \n",
    "    # Overall statistics\n",
    "    accuracies = [r['accuracy'] for r in all_results]\n",
    "    aucs = [r['auc'] for r in all_results]\n",
    "    \n",
    "    print(f\"\\nOVERALL STATISTICS:\")\n",
    "    print(f\"  Number of models analyzed: {len(all_results)}\")\n",
    "    print(f\"  Test samples: {len(y_true)}\")\n",
    "    print(f\"  Accuracy - Mean: {np.mean(accuracies):.4f}, Std: {np.std(accuracies):.4f}\")\n",
    "    print(f\"  Accuracy - Min: {np.min(accuracies):.4f}, Max: {np.max(accuracies):.4f}\")\n",
    "    print(f\"  AUC - Mean: {np.mean(aucs):.4f}, Std: {np.std(aucs):.4f}\")\n",
    "    print(f\"  AUC - Min: {np.min(aucs):.4f}, Max: {np.max(aucs):.4f}\")\n",
    "    \n",
    "    # Top 3 models detailed analysis\n",
    "    print(f\"\\nTOP 3 MODELS DETAILED ANALYSIS:\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    for i, result in enumerate(all_results[:3], 1):\n",
    "        print(f\"\\n🏆 RANK {i}: {result['filename']}\")\n",
    "        print(f\"   Test Accuracy: {result['accuracy']:.4f}\")\n",
    "        print(f\"   AUC Score: {result['auc']:.4f}\")\n",
    "        \n",
    "        # Confusion matrix details\n",
    "        cm = result['confusion_matrix']\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        print(f\"   Confusion Matrix:\")\n",
    "        print(f\"     True Negatives (Correct Cats): {tn}\")\n",
    "        print(f\"     False Positives (Cats as Non-cats): {fp}\")\n",
    "        print(f\"     False Negatives (Non-cats as Cats): {fn}\")\n",
    "        print(f\"     True Positives (Correct Non-cats): {tp}\")\n",
    "        \n",
    "        # Class-specific metrics\n",
    "        cats_precision = result['classification_report']['Cats']['precision']\n",
    "        cats_recall = result['classification_report']['Cats']['recall']\n",
    "        cats_f1 = result['classification_report']['Cats']['f1-score']\n",
    "        \n",
    "        noncats_precision = result['classification_report']['Non-cats']['precision']\n",
    "        noncats_recall = result['classification_report']['Non-cats']['recall']\n",
    "        noncats_f1 = result['classification_report']['Non-cats']['f1-score']\n",
    "        \n",
    "        print(f\"   Class-specific Performance:\")\n",
    "        print(f\"     Cats - Precision: {cats_precision:.4f}, Recall: {cats_recall:.4f}, F1: {cats_f1:.4f}\")\n",
    "        print(f\"     Non-cats - Precision: {noncats_precision:.4f}, Recall: {noncats_recall:.4f}, F1: {noncats_f1:.4f}\")\n",
    "        \n",
    "        # Model parameters if available\n",
    "        if result['model_info']:\n",
    "            info = result['model_info']\n",
    "            print(f\"   Hyperparameters:\")\n",
    "            print(f\"     Learning Rate: {info.get('lr', 'N/A')}\")\n",
    "            print(f\"     Adam Beta1: {info.get('b1', 'N/A')}\")\n",
    "            print(f\"     Adam Beta2: {info.get('b2', 'N/A')}\")\n",
    "            print(f\"     Batch Size: {info.get('bs', 'N/A')}\")\n",
    "            print(f\"     Regularization: {info.get('reg', 'N/A')}\")\n",
    "            print(f\"     Dense Units: {info.get('du', 'N/A')}\")\n",
    "    \n",
    "    # Parameter analysis\n",
    "    trial_results = [r for r in all_results if r['model_info'] is not None]\n",
    "    if trial_results:\n",
    "        print(f\"\\nHYPERPARAMETER ANALYSIS:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Learning rate analysis\n",
    "        lr_performance = {}\n",
    "        for result in trial_results:\n",
    "            lr = result['model_info']['lr']\n",
    "            if lr not in lr_performance:\n",
    "                lr_performance[lr] = []\n",
    "            lr_performance[lr].append(result['accuracy'])\n",
    "        \n",
    "        print(f\"\\nLearning Rate Performance:\")\n",
    "        for lr, accs in sorted(lr_performance.items()):\n",
    "            avg_acc = np.mean(accs)\n",
    "            std_acc = np.std(accs)\n",
    "            print(f\"  LR {lr}: {avg_acc:.4f} ± {std_acc:.4f} (n={len(accs)})\")\n",
    "        \n",
    "        # Beta parameters analysis\n",
    "        beta1_performance = {}\n",
    "        beta2_performance = {}\n",
    "        \n",
    "        for result in trial_results:\n",
    "            b1 = result['model_info']['b1']\n",
    "            b2 = result['model_info']['b2']\n",
    "            acc = result['accuracy']\n",
    "            \n",
    "            if b1 not in beta1_performance:\n",
    "                beta1_performance[b1] = []\n",
    "            beta1_performance[b1].append(acc)\n",
    "            \n",
    "            if b2 not in beta2_performance:\n",
    "                beta2_performance[b2] = []\n",
    "            beta2_performance[b2].append(acc)\n",
    "        \n",
    "        print(f\"\\nBeta1 (Momentum) Performance:\")\n",
    "        for b1, accs in sorted(beta1_performance.items()):\n",
    "            avg_acc = np.mean(accs)\n",
    "            std_acc = np.std(accs)\n",
    "            print(f\"  β₁={b1}: {avg_acc:.4f} ± {std_acc:.4f} (n={len(accs)})\")\n",
    "        \n",
    "        print(f\"\\nBeta2 (Variance) Performance:\")\n",
    "        for b2, accs in sorted(beta2_performance.items()):\n",
    "            avg_acc = np.mean(accs)\n",
    "            std_acc = np.std(accs)\n",
    "            print(f\"  β₂={b2}: {avg_acc:.4f} ± {std_acc:.4f} (n={len(accs)})\")\n",
    "\n",
    "print(\"Visualization and reporting functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 STARTING COMPREHENSIVE MODEL ANALYSIS\n",
      "================================================================================\n",
      "Found 13 model files to analyze:\n",
      "  1. best_tuned_model_with_betas.h5\n",
      "  2. trial_01_lr0.0005_b1-0.9_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "  3. trial_02_lr0.001_b1-0.95_b2-0.999_bs32_reg0.0001_du512.h5\n",
      "  4. trial_03_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "  5. trial_04_lr0.001_b1-0.8_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "  6. trial_05_lr0.0005_b1-0.9_b2-0.99_bs32_reg0.0005_du512.h5\n",
      "  7. trial_06_lr0.0001_b1-0.9_b2-0.9999_bs32_reg0.001_du512.h5\n",
      "  8. trial_07_lr0.0005_b1-0.95_b2-0.9999_bs64_reg0.0005_du1024.h5\n",
      "  9. trial_08_lr0.001_b1-0.8_b2-0.99_bs16_reg0.0001_du512.h5\n",
      "  10. trial_09_lr0.0001_b1-0.95_b2-0.9999_bs64_reg0.001_du256.h5\n",
      "  11. trial_10_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du1024.h5\n",
      "  12. trial_11_lr0.001_b1-0.8_b2-0.99_bs64_reg0.0001_du512.h5\n",
      "  13. trial_12_lr0.0001_b1-0.99_b2-0.9999_bs32_reg0.0005_du512.h5\n",
      "\n",
      "Analyzing models on 1068 test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set distribution:\n",
      "  Cats (class 0): 534 samples\n",
      "  Non-cats (class 1): 534 samples\n",
      "\n",
      "====================================================================================================\n",
      "INDIVIDUAL MODEL ANALYSIS\n",
      "====================================================================================================\n",
      "\n",
      "[1/13] Analyzing: best_tuned_model_with_betas.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linhnguyen/Library/Python/3.10/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9625\n",
      "AUC: 0.9930\n",
      "Precision (Cats): 0.9458\n",
      "Recall (Cats): 0.9813\n",
      "Precision (Non-cats): 0.9805\n",
      "Recall (Non-cats): 0.9438\n",
      "\n",
      "[2/13] Analyzing: trial_01_lr0.0005_b1-0.9_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9213\n",
      "AUC: 0.9764\n",
      "Precision (Cats): 0.9518\n",
      "Recall (Cats): 0.8876\n",
      "Precision (Non-cats): 0.8947\n",
      "Recall (Non-cats): 0.9551\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0005\n",
      "  Beta1: 0.9\n",
      "  Beta2: 0.999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 512\n",
      "\n",
      "[3/13] Analyzing: trial_02_lr0.001_b1-0.95_b2-0.999_bs32_reg0.0001_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8886\n",
      "AUC: 0.9744\n",
      "Precision (Cats): 0.9621\n",
      "Recall (Cats): 0.8090\n",
      "Precision (Non-cats): 0.8352\n",
      "Recall (Non-cats): 0.9682\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.001\n",
      "  Beta1: 0.95\n",
      "  Beta2: 0.999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0001\n",
      "  Dense Units: 512\n",
      "\n",
      "[4/13] Analyzing: trial_03_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9073\n",
      "AUC: 0.9773\n",
      "Precision (Cats): 0.9718\n",
      "Recall (Cats): 0.8390\n",
      "Precision (Non-cats): 0.8583\n",
      "Recall (Non-cats): 0.9757\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0005\n",
      "  Beta1: 0.99\n",
      "  Beta2: 0.999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 512\n",
      "\n",
      "[5/13] Analyzing: trial_04_lr0.001_b1-0.8_b2-0.999_bs32_reg0.0005_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8895\n",
      "AUC: 0.9811\n",
      "Precision (Cats): 0.9860\n",
      "Recall (Cats): 0.7903\n",
      "Precision (Non-cats): 0.8250\n",
      "Recall (Non-cats): 0.9888\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.001\n",
      "  Beta1: 0.8\n",
      "  Beta2: 0.999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 512\n",
      "\n",
      "[6/13] Analyzing: trial_05_lr0.0005_b1-0.9_b2-0.99_bs32_reg0.0005_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9232\n",
      "AUC: 0.9825\n",
      "Precision (Cats): 0.9728\n",
      "Recall (Cats): 0.8708\n",
      "Precision (Non-cats): 0.8831\n",
      "Recall (Non-cats): 0.9757\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0005\n",
      "  Beta1: 0.9\n",
      "  Beta2: 0.99\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 512\n",
      "\n",
      "[7/13] Analyzing: trial_06_lr0.0001_b1-0.9_b2-0.9999_bs32_reg0.001_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7116\n",
      "AUC: 0.8789\n",
      "Precision (Cats): 0.9007\n",
      "Recall (Cats): 0.4757\n",
      "Precision (Non-cats): 0.6438\n",
      "Recall (Non-cats): 0.9476\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0001\n",
      "  Beta1: 0.9\n",
      "  Beta2: 0.9999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.001\n",
      "  Dense Units: 512\n",
      "\n",
      "[8/13] Analyzing: trial_07_lr0.0005_b1-0.95_b2-0.9999_bs64_reg0.0005_du1024.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9204\n",
      "AUC: 0.9778\n",
      "Precision (Cats): 0.9535\n",
      "Recall (Cats): 0.8839\n",
      "Precision (Non-cats): 0.8918\n",
      "Recall (Non-cats): 0.9569\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0005\n",
      "  Beta1: 0.95\n",
      "  Beta2: 0.9999\n",
      "  Batch Size: 64\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 1024\n",
      "\n",
      "[9/13] Analyzing: trial_08_lr0.001_b1-0.8_b2-0.99_bs16_reg0.0001_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9176\n",
      "AUC: 0.9837\n",
      "Precision (Cats): 0.9785\n",
      "Recall (Cats): 0.8539\n",
      "Precision (Non-cats): 0.8704\n",
      "Recall (Non-cats): 0.9813\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.001\n",
      "  Beta1: 0.8\n",
      "  Beta2: 0.99\n",
      "  Batch Size: 16\n",
      "  Regularization: 0.0001\n",
      "  Dense Units: 512\n",
      "\n",
      "[10/13] Analyzing: trial_09_lr0.0001_b1-0.95_b2-0.9999_bs64_reg0.001_du256.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7303\n",
      "AUC: 0.8076\n",
      "Precision (Cats): 0.7236\n",
      "Recall (Cats): 0.7453\n",
      "Precision (Non-cats): 0.7375\n",
      "Recall (Non-cats): 0.7154\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0001\n",
      "  Beta1: 0.95\n",
      "  Beta2: 0.9999\n",
      "  Batch Size: 64\n",
      "  Regularization: 0.001\n",
      "  Dense Units: 256\n",
      "\n",
      "[11/13] Analyzing: trial_10_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du1024.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9232\n",
      "AUC: 0.9809\n",
      "Precision (Cats): 0.9538\n",
      "Recall (Cats): 0.8895\n",
      "Precision (Non-cats): 0.8965\n",
      "Recall (Non-cats): 0.9569\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0005\n",
      "  Beta1: 0.99\n",
      "  Beta2: 0.999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 1024\n",
      "\n",
      "[12/13] Analyzing: trial_11_lr0.001_b1-0.8_b2-0.99_bs64_reg0.0001_du512.h5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9335\n",
      "AUC: 0.9872\n",
      "Precision (Cats): 0.9443\n",
      "Recall (Cats): 0.9213\n",
      "Precision (Non-cats): 0.9232\n",
      "Recall (Non-cats): 0.9457\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.001\n",
      "  Beta1: 0.8\n",
      "  Beta2: 0.99\n",
      "  Batch Size: 64\n",
      "  Regularization: 0.0001\n",
      "  Dense Units: 512\n",
      "\n",
      "[13/13] Analyzing: trial_12_lr0.0001_b1-0.99_b2-0.9999_bs32_reg0.0005_du512.h5\n",
      "--------------------------------------------------------------------------------\n",
      "Accuracy: 0.8146\n",
      "AUC: 0.8854\n",
      "Precision (Cats): 0.8032\n",
      "Recall (Cats): 0.8333\n",
      "Precision (Non-cats): 0.8268\n",
      "Recall (Non-cats): 0.7959\n",
      "\n",
      "Model Parameters:\n",
      "  Learning Rate: 0.0001\n",
      "  Beta1: 0.99\n",
      "  Beta2: 0.9999\n",
      "  Batch Size: 32\n",
      "  Regularization: 0.0005\n",
      "  Dense Units: 512\n",
      "\n",
      "====================================================================================================\n",
      "PERFORMANCE RANKING\n",
      "====================================================================================================\n",
      " 1. best_tuned_model_with_betas.h5                               | Acc: 0.9625 | AUC: 0.9930\n",
      " 2. trial_11_lr0.001_b1-0.8_b2-0.99_bs64_reg0.0001_du512.h5      | Acc: 0.9335 | AUC: 0.9872\n",
      " 3. trial_05_lr0.0005_b1-0.9_b2-0.99_bs32_reg0.0005_du512.h5     | Acc: 0.9232 | AUC: 0.9825\n",
      " 4. trial_10_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du1024.h5  | Acc: 0.9232 | AUC: 0.9809\n",
      " 5. trial_01_lr0.0005_b1-0.9_b2-0.999_bs32_reg0.0005_du512.h5    | Acc: 0.9213 | AUC: 0.9764\n",
      " 6. trial_07_lr0.0005_b1-0.95_b2-0.9999_bs64_reg0.0005_du1024.h5 | Acc: 0.9204 | AUC: 0.9778\n",
      " 7. trial_08_lr0.001_b1-0.8_b2-0.99_bs16_reg0.0001_du512.h5      | Acc: 0.9176 | AUC: 0.9837\n",
      " 8. trial_03_lr0.0005_b1-0.99_b2-0.999_bs32_reg0.0005_du512.h5   | Acc: 0.9073 | AUC: 0.9773\n",
      " 9. trial_04_lr0.001_b1-0.8_b2-0.999_bs32_reg0.0005_du512.h5     | Acc: 0.8895 | AUC: 0.9811\n",
      "10. trial_02_lr0.001_b1-0.95_b2-0.999_bs32_reg0.0001_du512.h5    | Acc: 0.8886 | AUC: 0.9744\n",
      "11. trial_12_lr0.0001_b1-0.99_b2-0.9999_bs32_reg0.0005_du512.h5  | Acc: 0.8146 | AUC: 0.8854\n",
      "12. trial_09_lr0.0001_b1-0.95_b2-0.9999_bs64_reg0.001_du256.h5   | Acc: 0.7303 | AUC: 0.8076\n",
      "13. trial_06_lr0.0001_b1-0.9_b2-0.9999_bs32_reg0.001_du512.h5    | Acc: 0.7116 | AUC: 0.8789\n",
      "\n",
      "📊 Creating comprehensive visualizations...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Generate visualizations\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 Creating comprehensive visualizations...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model_comparison_visualizations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Generate detailed report\u001b[39;00m\n\u001b[1;32m     13\u001b[0m generate_detailed_report(all_results, y_true)\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mcreate_model_comparison_visualizations\u001b[0;34m(all_results, y_true)\u001b[0m\n\u001b[1;32m      4\u001b[0m n_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_results)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a large figure with multiple subplots\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m15\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1. Accuracy comparison bar plot\u001b[39;00m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Execute Comprehensive Model Analysis\n",
    "print(\"🔍 STARTING COMPREHENSIVE MODEL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run the analysis\n",
    "all_results, y_true = analyze_all_saved_models()\n",
    "\n",
    "# Generate visualizations\n",
    "print(f\"\\n📊 Creating comprehensive visualizations...\")\n",
    "fig = create_model_comparison_visualizations(all_results, y_true)\n",
    "\n",
    "# Generate detailed report\n",
    "generate_detailed_report(all_results, y_true)\n",
    "\n",
    "print(f\"\\n✅ Model analysis complete!\")\n",
    "print(f\"📈 {len(all_results)} models analyzed on {len(y_true)} test samples\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
