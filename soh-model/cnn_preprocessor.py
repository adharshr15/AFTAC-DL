import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import os

#path to file generated by other script will take more than one data set
DATA_FILES = [r"./data/synthetic_data_independent_failures_5.csv", "./data/synthetic_data_independent_failures_4.csv"]

SEQUENCE_LENGTH = 24 * 2
STEP_SIZE = 24
FORECAST_HORIZON = 1
SELECT_MODEL = 0

# Data processing
def load_and_combine_data(file_paths):
    """
    Loads multiple CSV files, finds the union of all sensor features, 
    reindexes dataframes to match the full feature set (filling missing sensors with 0.0), 
    and concatenates them into a single DataFrame.
    """
    all_data = []
    all_sensor_features = set()
    
    # Load data and collect all unique sensor feature names
    non_sensor_cols = ['machine_id', "timestamp",'failure_mode', 'is_precursor_period', 'is_final_failure']
    
    for file_path in file_paths:
        try:
            df = pd.read_csv(file_path)
            
            # Identify potential sensor features in the current file
            current_sensor_features = [col for col in df.columns if col not in non_sensor_cols]
            print(current_sensor_features)
            all_sensor_features.update(current_sensor_features)
            
            all_data.append(df)
            print(f"Loaded {file_path} with {len(current_sensor_features)} sensor features.")
        except FileNotFoundError:
            print(f"Warning: File '{file_path}' not found. Skipping.")
    
    if not all_data:
        raise FileNotFoundError("No valid data files were loaded.")

    sensor_feature_list = sorted(list(all_sensor_features))
    final_combined_df = pd.DataFrame()
    full_column_list = non_sensor_cols + sensor_feature_list
    
    for df in all_data:
        
        df_reindexed = df.reindex(columns=full_column_list, fill_value=0.0)
        
        final_combined_df = pd.concat([final_combined_df, df_reindexed], ignore_index=True)

    print(f"\nSuccessfully combined {len(all_data)} files.")
    print(f"Total rows in combined data: {len(final_combined_df)}")
    print(f"Total unique sensor features used: {len(sensor_feature_list)}")
    
    return final_combined_df, sensor_feature_list

def create_sequences(data, seq_length, forecast_horizon, step_size):
    sequences = []
    target = []
    for i in range(0, len(data) - seq_length - forecast_horizon + 1, step_size):
        sequences.append(data[i:i+seq_length])
        target.append(data[i+seq_length: i+seq_length+forecast_horizon])
    return np.array(sequences), np.array(target)

def prepare_data(df, sensor_features):
    """Loads, cleans, labels, and scales the data. Returns sequences and the fitted scaler."""
    sensor_data = df[sensor_features].values
   
    # 2. Standardization
    scaler = StandardScaler()
    sensor_data_scaled = scaler.fit_transform(sensor_data)
    
    # 3. Create Sequences
    X_seq, Y_seq = create_sequences(sensor_data_scaled, SEQUENCE_LENGTH, FORECAST_HORIZON, STEP_SIZE)
    
    print(f"\n--- Data Preparation Complete ---")
    print(f"Total time points in raw data: {len(df)}")
    print(f"Total sequences created: {len(X_seq)}")
    print(f"Sequence shape (num_samples, time steps, features): {X_seq.shape}")
    
    # Return the fitted scaler so we can inverse-transform predictions later
    return X_seq, Y_seq, scaler

def prepare_feature_data(X, Y, feature_idx):
    """Extract data for a specific feature index."""
    X_feature = X[:, :, feature_idx:feature_idx+1]
    Y_feature = Y[:, :, feature_idx].squeeze()
    return X_feature, Y_feature


def main():
    test_split = float(input("Enter test split fraction (e.g., 0.2 for 20%): "))
    val_split = float(input("Enter validation split fraction (e.g., 0.2 for 20%): "))
    
    print("Loading and combining data...")
    data, sensor_features = load_and_combine_data(DATA_FILES)
    X_seq, Y_seq, scaler = prepare_data(data, sensor_features)
    
    # Save scaler mean and scale values to a .txt file
    scaler_params_path = os.path.join("data", "finalized", "scaler_params.txt")
    with open(scaler_params_path, "w") as f:
        f.write("mean:")
        f.write(" ".join(map(str, scaler.mean_)) + "\n")
        f.write("std:")
        f.write(" ".join(map(str, scaler.scale_)) + "\n")
    print(f"Scaler parameters saved to {scaler_params_path}")
    
    # Split data into training, validation, and testing sets for 3D arrays
    # First split: 80% train+val, 20% test
    indices = np.arange(X_seq.shape[0])
    train_val_indices, test_indices = train_test_split(
        indices, test_size=test_split, random_state=42, shuffle=False)

    # Second split: 75% train, 25% val (of the train+val set = 60% train, 20% val overall)
    train_indices, val_indices = train_test_split(
        train_val_indices, test_size=val_split, random_state=42, shuffle=True)

    X_train, X_val, X_test = X_seq[train_indices], X_seq[val_indices], X_seq[test_indices]
    Y_train, Y_val, Y_test = Y_seq[train_indices], Y_seq[val_indices], Y_seq[test_indices]
    
    # Prepare data for the selected feature
    X_train_feature, Y_train_feature = prepare_feature_data(X_train, Y_train, SELECT_MODEL)
    X_val_feature, Y_val_feature = prepare_feature_data(X_val, Y_val, SELECT_MODEL)
    X_test_feature, Y_test_feature = prepare_feature_data(X_test, Y_test, SELECT_MODEL)

    print(f"Training feature {SELECT_MODEL}: {sensor_features[SELECT_MODEL]}")
    print(f"Train: X_train_feature shape: {X_train_feature.shape}, Y_train_feature shape: {Y_train_feature.shape}")
    print(f"Val: X_val_feature shape: {X_val_feature.shape}, Y_val_feature shape: {Y_val_feature.shape}")
    print(f"Test: X_test_feature shape: {X_test_feature.shape}, Y_test_feature shape: {Y_test_feature.shape}")
    
    os.makedirs(os.path.join("data", "finalized"), exist_ok=True)
    
    # Save processed data
    np.save(os.path.join("data", "finalized", f"X_train_feature_{SELECT_MODEL}.npy"), X_train_feature)
    np.save(os.path.join("data", "finalized", f"Y_train_feature_{SELECT_MODEL}.npy"), Y_train_feature)
    np.save(os.path.join("data", "finalized", f"X_val_feature_{SELECT_MODEL}.npy"), X_val_feature)
    np.save(os.path.join("data", "finalized", f"Y_val_feature_{SELECT_MODEL}.npy"), Y_val_feature)
    np.save(os.path.join("data", "finalized", f"X_test_feature_{SELECT_MODEL}.npy"), X_test_feature)
    np.save(os.path.join("data", "finalized", f"Y_test_feature_{SELECT_MODEL}.npy"), Y_test_feature)

main()