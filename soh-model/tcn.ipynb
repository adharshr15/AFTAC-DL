{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4897c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Config\n",
    "\n",
    "#path to file generated by other script will take more than one data set\n",
    "DATA_FILES = [r\"./data/synthetic_data_independent_failures_5.csv\"] \n",
    "\n",
    "SEQUENCE_LENGTH = 24 * 7 * 2 \n",
    "STEP_SIZE = 24 * 2\n",
    "FORECAST_HORIZON = 5\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Set device for training\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47d47218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing \n",
    "def load_and_combine_data(file_paths):\n",
    "    \"\"\"\n",
    "    Loads multiple CSV files, finds the union of all sensor features, \n",
    "    reindexes dataframes to match the full feature set (filling missing sensors with 0.0), \n",
    "    and concatenates them into a single DataFrame.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    all_sensor_features = set()\n",
    "    \n",
    "    # Load data and collect all unique sensor feature names\n",
    "    non_sensor_cols = ['machine_id', \"timestamp\",'failure_mode', 'is_precursor_period', 'is_final_failure']\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Identify potential sensor features in the current file\n",
    "            current_sensor_features = [col for col in df.columns if col not in non_sensor_cols]\n",
    "            print(current_sensor_features)\n",
    "            all_sensor_features.update(current_sensor_features)\n",
    "            \n",
    "            all_data.append(df)\n",
    "            print(f\"Loaded {file_path} with {len(current_sensor_features)} sensor features.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File '{file_path}' not found. Skipping.\")\n",
    "    \n",
    "    if not all_data:\n",
    "        raise FileNotFoundError(\"No valid data files were loaded.\")\n",
    "\n",
    "    \n",
    "    sensor_feature_list = sorted(list(all_sensor_features))\n",
    "    \n",
    "    \n",
    "    final_combined_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    full_column_list = non_sensor_cols + sensor_feature_list\n",
    "    \n",
    "    for df in all_data:\n",
    "        \n",
    "        df_reindexed = df.reindex(columns=full_column_list, fill_value=0.0)\n",
    "        \n",
    "        final_combined_df = pd.concat([final_combined_df, df_reindexed], ignore_index=True)\n",
    "\n",
    "    print(f\"\\nSuccessfully combined {len(all_data)} files.\")\n",
    "    print(f\"Total rows in combined data: {len(final_combined_df)}\")\n",
    "    print(f\"Total unique sensor features used: {len(sensor_feature_list)}\")\n",
    "    \n",
    "    \n",
    "    return final_combined_df, sensor_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55a5847b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_internal_sensor', 'V_sensor']\n",
      "Loaded ./data/synthetic_data_independent_failures_5.csv with 2 sensor features.\n",
      "\n",
      "Successfully combined 1 files.\n",
      "Total rows in combined data: 131400\n",
      "Total unique sensor features used: 2\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data, sensor_features = load_and_combine_data(DATA_FILES)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Fatal Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9451875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, forecast_horizon, step_size):\n",
    "    sequences = []\n",
    "    target = []\n",
    "    for i in range(0, len(data) - seq_length - forecast_horizon + 1, step_size):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        target.append(data[i+seq_length: i+seq_length+forecast_horizon])\n",
    "    return np.array(sequences), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e0eee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, sensor_features):\n",
    "    \"\"\"Loads, cleans, labels, and scales the data.\"\"\"\n",
    "    sensor_data = df[sensor_features].values\n",
    "   \n",
    "    # 2. Standardization\n",
    "    scaler = StandardScaler()\n",
    "    sensor_data_scaled = scaler.fit_transform(sensor_data)\n",
    "    \n",
    "    # 3. Create Sequences\n",
    "    X_seq, Y_seq = create_sequences(sensor_data_scaled, SEQUENCE_LENGTH, FORECAST_HORIZON, STEP_SIZE)\n",
    "    \n",
    "    print(f\"\\n--- Data Preparation Complete ---\")\n",
    "    print(f\"Total time points in raw data: {len(df)}\")\n",
    "    print(f\"Total sequences created: {len(X_seq)}\")\n",
    "    print(f\"Sequence shape (num_samples, time steps, features): {X_seq.shape}\")\n",
    "    \n",
    "    return X_seq, Y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7fa2476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Preparation Complete ---\n",
      "Total time points in raw data: 131400\n",
      "Total sequences created: 2731\n",
      "Sequence shape (num_samples, time steps, features): (2731, 336, 2)\n"
     ]
    }
   ],
   "source": [
    "X_seq, Y_seq = prepare_data(data, sensor_features)\n",
    "\n",
    "# Split data into training and testing sets for 3D arrays\n",
    "indices = np.arange(X_seq.shape[0])\n",
    "train_indices, test_indices = train_test_split(\n",
    "    indices, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "X_train, X_test = X_seq[train_indices], X_seq[test_indices]\n",
    "Y_train, Y_test = Y_seq[train_indices], Y_seq[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b7a8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTORCH Model for fun or more like testing\n",
    "class TimeDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for time-series sequences.\"\"\"\n",
    "    def __init__(self, X_data, y_data, feature_idx):\n",
    "        self.X_data = torch.tensor(X_data[:, :, feature_idx: feature_idx + 1], dtype=torch.float32).permute(0, 2, 1)\n",
    "        self.y_data = torch.tensor(y_data[:, :, feature_idx: feature_idx + 1], dtype=torch.float32).squeeze(-1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e74aa19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imbalanced Data\n",
    "# train_labels_series = pd.Series(Y_train)\n",
    "# class_counts = train_labels_series.value_counts().sort_index()\n",
    "# total_samples = len(Y_train)\n",
    "\n",
    "\n",
    "# class_weights = total_samples / (NUM_CLASSES * class_counts)\n",
    "\n",
    "\n",
    "# class_weights_tensor = torch.tensor(class_weights.values, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# print(\"\\nCalculated Class Weights (Higher = More Important):\")\n",
    "# for i, weight in enumerate(class_weights_tensor):\n",
    "#     print(f\"Class {i} Weight: {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4dc814c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2184, 336, 2), (2184, 5, 2))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aa3ce686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Datasets and DataLoaders\n",
    "train_loader = []\n",
    "test_loader = []\n",
    "\n",
    "for i in range(X_train.shape[2]):\n",
    "    train_dataset = TimeDataset(X_train, Y_train, i)\n",
    "    test_dataset = TimeDataset(X_test, Y_test, i)\n",
    "\n",
    "    train_loader.append(DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=False))\n",
    "    test_loader.append(DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be9bd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 1D CNN for time-series regression built with PyTorch.\n",
    "    Matches the Keras Conv1D + MaxPool + Dense architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_length, forecast_horizon):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        # Layer 1: Conv1D (64 filters, kernel size 3)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=0)\n",
    "        \n",
    "        # Layer 2: Max Pooling\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Calculate flattened size after conv and pooling\n",
    "        # seq_length -> (seq_length - 2) after conv -> (seq_length - 2) // 2 after pool\n",
    "        conv_output_size = (seq_length - 2) // 2\n",
    "        flattened_size = 64 * conv_output_size\n",
    "        \n",
    "        # Layer 3: Fully connected (Dense) layer with 100 units\n",
    "        self.fc1 = nn.Linear(flattened_size, 100)\n",
    "        \n",
    "        # Layer 4: Output layer for regression\n",
    "        self.fc2 = nn.Linear(100, forecast_horizon)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv + ReLU\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        \n",
    "        # Max Pooling\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Dense layer + ReLU\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        \n",
    "        # Output layer (no activation for regression)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1f50eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TRAINING AND EVALUATION ---\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    \"\"\"PyTorch training loop.\"\"\"\n",
    "    model.train()\n",
    "    print(f\"\\n--- Starting PyTorch Model Training ({num_epochs} Epochs) ---\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f3fb4384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "CNNClassifier(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(3,), stride=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=10688, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the PyTorch CNN model\n",
    "model = CNNClassifier(X_train.shape[1], FORECAST_HORIZON).to(DEVICE)\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07ff7c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting PyTorch Model Training (10 Epochs) ---\n",
      "Epoch [1/10], Loss: 0.0379\n",
      "Epoch [2/10], Loss: 0.0214\n",
      "Epoch [3/10], Loss: 0.0232\n",
      "Epoch [4/10], Loss: 0.0245\n",
      "Epoch [5/10], Loss: 0.0169\n",
      "Epoch [6/10], Loss: 0.0239\n",
      "Epoch [7/10], Loss: 0.0155\n",
      "Epoch [8/10], Loss: 0.0171\n",
      "Epoch [9/10], Loss: 0.0161\n",
      "Epoch [10/10], Loss: 0.0156\n"
     ]
    }
   ],
   "source": [
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader[0], criterion, optimizer, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c842255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"PyTorch evaluation loop for regression.\"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            \n",
    "            # Forward pass - get predictions\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Collect predictions and targets (no argmax for regression)\n",
    "            y_true.extend(targets.cpu().numpy())\n",
    "            y_pred.extend(outputs.cpu().numpy())\n",
    "            \n",
    "    return np.array(y_true), np.array(y_pred)\n",
    "\n",
    "def calculate_msre(y_pred, y_true):\n",
    "    avg_error_per_column = np.mean(np.abs(y_true - y_pred), axis=0)\n",
    "    print(f\"Average error per column: {avg_error_per_column}\")\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    print(f\"Total MSE: {mse}\")\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Model Performance ---\n",
      "Average error per column: [0.11688787 0.11731971 0.12023129 0.12289765 0.12906395]\n",
      "Total MSE: 0.02321123518049717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(0.15235233)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"\\n--- Evaluating Model Performance ---\")\n",
    "y_true, y_pred = evaluate_model(model, test_loader[0])\n",
    "\n",
    "calculate_msre(y_pred, y_true)\n",
    "\n",
    "# # Basic classification report\n",
    "# print(\"\\nClassification Report (PyTorch):\")\n",
    "# report = classification_report(y_true, y_pred, \n",
    "#                                 target_names=['Normal (0)', 'Precursor (1)', 'Failure (2)'],\n",
    "#                                 labels=[0, 1, 2])\n",
    "# print(report)\n",
    "\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a9e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression Metrics:\n",
      "Mean Squared Error (MSE): 0.0232\n",
      "Root Mean Squared Error (RMSE): 0.1524\n",
      "Mean Absolute Error (MAE): 0.1213\n",
      "R² Score: 0.9373\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Regression metrics\n",
    "mse = mean_squared_error(y_true.flatten(), y_pred.flatten())\n",
    "mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true.flatten(), y_pred.flatten())\n",
    "\n",
    "print(\"\\nRegression Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78152f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
