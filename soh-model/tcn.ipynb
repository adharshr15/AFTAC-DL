{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4897c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Config\n",
    "\n",
    "#path to file generated by other script will take more than one data set\n",
    "DATA_FILES = [r\"./data/synthetic_data_independent_failures_5.csv\"] \n",
    "\n",
    "SEQUENCE_LENGTH = 24 * 7 * 2 \n",
    "STEP_SIZE = 24 * 2\n",
    "FORECAST_HORIZON = 5\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# Set device for training\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d47218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing \n",
    "def load_and_combine_data(file_paths):\n",
    "    \"\"\"\n",
    "    Loads multiple CSV files, finds the union of all sensor features, \n",
    "    reindexes dataframes to match the full feature set (filling missing sensors with 0.0), \n",
    "    and concatenates them into a single DataFrame.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    all_sensor_features = set()\n",
    "    \n",
    "    # Load data and collect all unique sensor feature names\n",
    "    non_sensor_cols = ['machine_id', \"timestamp\",'failure_mode', 'is_precursor_period', 'is_final_failure']\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Identify potential sensor features in the current file\n",
    "            current_sensor_features = [col for col in df.columns if col not in non_sensor_cols]\n",
    "            print(current_sensor_features)\n",
    "            all_sensor_features.update(current_sensor_features)\n",
    "            \n",
    "            all_data.append(df)\n",
    "            print(f\"Loaded {file_path} with {len(current_sensor_features)} sensor features.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File '{file_path}' not found. Skipping.\")\n",
    "    \n",
    "    if not all_data:\n",
    "        raise FileNotFoundError(\"No valid data files were loaded.\")\n",
    "\n",
    "    \n",
    "    sensor_feature_list = sorted(list(all_sensor_features))\n",
    "    \n",
    "    \n",
    "    final_combined_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    full_column_list = non_sensor_cols + sensor_feature_list\n",
    "    \n",
    "    for df in all_data:\n",
    "        \n",
    "        df_reindexed = df.reindex(columns=full_column_list, fill_value=0.0)\n",
    "        \n",
    "        final_combined_df = pd.concat([final_combined_df, df_reindexed], ignore_index=True)\n",
    "\n",
    "    print(f\"\\nSuccessfully combined {len(all_data)} files.\")\n",
    "    print(f\"Total rows in combined data: {len(final_combined_df)}\")\n",
    "    print(f\"Total unique sensor features used: {len(sensor_feature_list)}\")\n",
    "    \n",
    "    \n",
    "    return final_combined_df, sensor_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9451875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, forecast_horizon, step_size):\n",
    "    sequences = []\n",
    "    target = []\n",
    "    for i in range(len(data) - seq_length - forecast_horizon + 1, step_size):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        target.append(data[i+seq_length: i+seq_length+forecast_horizon])\n",
    "    return np.array(sequences), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0eee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, sensor_features):\n",
    "    \"\"\"Loads, cleans, labels, and scales the data.\"\"\"\n",
    "    sensor_data = df[sensor_features].values\n",
    "   \n",
    "    # 2. Standardization\n",
    "    scaler = StandardScaler()\n",
    "    sensor_data_scaled = scaler.fit_transform(sensor_data)\n",
    "    \n",
    "    # 3. Create Sequences\n",
    "    X_seq, Y_seq = create_sequences(sensor_data_scaled, SEQUENCE_LENGTH, FORECAST_HORIZON, STEP_SIZE)\n",
    "    \n",
    "    print(f\"\\n--- Data Preparation Complete ---\")\n",
    "    print(f\"Total time points in raw data: {len(df)}\")\n",
    "    print(f\"Total sequences created: {len(X_seq)}\")\n",
    "    print(f\"Sequence shape (time steps, features): {X_seq.shape[1:]}\")\n",
    "    print(f\"Label distribution: {pd.Series(Y_seq).value_counts().sort_index()}\")\n",
    "    \n",
    "    return X_seq, Y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7a8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTORCH Model for fun or more like testing\n",
    "class TimeDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for time-series sequences.\"\"\"\n",
    "    def __init__(self, X_data, y_data):\n",
    "\n",
    "        self.X_data = torch.tensor(X_data, dtype=torch.float32).permute(0, 2, 1)\n",
    "        self.y_data = torch.tensor(y_data, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be9bd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    5-layer 1D CNN for time-series classification built with PyTorch.\n",
    "    Input channels adapts based on the number of sensor features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        # Layer 1: Conv1D (Large filter for high-level feature extraction)\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=12, padding='same')\n",
    "        \n",
    "        # Layer 2: Conv1D (Smaller filter for finer patterns)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=6, padding='same')\n",
    "        \n",
    "        # Layer 3: Max Pooling (Downsampling)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=4)\n",
    "        \n",
    "        # Layer 4: Conv1D (Deep feature extraction)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding='same')\n",
    "        \n",
    "        # Layer 5: Global Average Pooling (Summarizes the time-series output)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Final fully connected layer\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        \n",
    "        # Apply Global Average Pooling\n",
    "        x = self.global_pool(x)\n",
    "        \n",
    "        # Flatten the tensor from (batch, features, 1) to (batch, features)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f50eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TRAINING AND EVALUATION ---\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    \"\"\"PyTorch training loop.\"\"\"\n",
    "    model.train()\n",
    "    print(f\"\\n--- Starting PyTorch Model Training ({num_epochs} Epochs) ---\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"PyTorch evaluation loop and metric calculation.\"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            y_true.extend(targets.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a706c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timestamp', 'T_internal_sensor', 'V_sensor']\n",
      "Loaded ./data/synthetic_data_independent_failures_5.csv with 3 sensor features.\n",
      "\n",
      "Successfully combined 1 files.\n",
      "Total rows in combined data: 131400\n",
      "Total unique sensor features used: 3\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data, sensor_features = load_and_combine_data(DATA_FILES)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Fatal Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7890a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq, Y_seq = prepare_data(data, sensor_features)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_seq, Y_seq, test_size=0.2, random_state=42, stratify=Y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e4e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalanced Data\n",
    "train_labels_series = pd.Series(Y_train)\n",
    "class_counts = train_labels_series.value_counts().sort_index()\n",
    "total_samples = len(Y_train)\n",
    "\n",
    "\n",
    "class_weights = total_samples / (NUM_CLASSES * class_counts)\n",
    "\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights.values, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "print(\"\\nCalculated Class Weights (Higher = More Important):\")\n",
    "for i, weight in enumerate(class_weights_tensor):\n",
    "    print(f\"Class {i} Weight: {weight:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Datasets and DataLoaders\n",
    "train_dataset = TimeDataset(X_train, Y_train)\n",
    "test_dataset = TimeDataset(X_test, Y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the PyTorch CNN model\n",
    "input_channels = X_train.shape[2] \n",
    "model = CNNClassifier(input_channels, NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff7c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"\\n--- Evaluating Model Performance ---\")\n",
    "y_true, y_pred = evaluate_model(model, test_loader)\n",
    "\n",
    "# # Basic classification report\n",
    "# print(\"\\nClassification Report (PyTorch):\")\n",
    "# report = classification_report(y_true, y_pred, \n",
    "#                                 target_names=['Normal (0)', 'Precursor (1)', 'Failure (2)'],\n",
    "#                                 labels=[0, 1, 2])\n",
    "# print(report)\n",
    "\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(confusion_matrix(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
